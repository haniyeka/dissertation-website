[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dynamic Selection of Parallel Portfolio of Algorithms for Solving Combinatorial Problems",
    "section": "",
    "text": "When tackling complex combinatorial and AI problems, algorithms can perform very differently on each instance: one might solve it in seconds, while another could fail to resolve within the time limit. This means that algorithms have inherent performance variability across problem instances, resulting in no universally optimal solution. To address this, we can take advantage of the complementary performance of solvers by including them in an algorithm portfolio. This allows us to leverage various portfolio approaches to solve problems more efficiently.\nOne of the most studied portfolio approaches is algorithm selection. In this approach, performance models are trained using machine learning algorithms and features extracted from instances. Then, a single algorithm is chosen through the trained model that is estimated to perform well on a specific problem instance. However, selecting a single algorithm might be risky, as machine learning could choose an incorrect solver. Additionally, the advancement of multicore processing technologies provides an opportunity to optimize combinatorial problem solving. One approach to take advantage of multicore systems is to run a large number of solvers in parallel and halt execution as soon as one solver solves the problem. This approach can introduce overhead in shared-resource settings, as the more solvers running in parallel, the more processors will compete for resources.\nThis work addresses the limitations of the single-algorithm selection and parallel execution approaches by first including a comprehensive empirical analysis of solvers executed in parallel and direct comparisons of parallel execution versus traditional algorithm selection strategies. This comparison reveals that when there are too many solvers running in parallel, algorithm selection is a better choice. This study also introduces a hybrid model that combines traditional algorithm selection with parallel execution, tailored to instance-specific features, and utilizes uncertainty in algorithm performance predictions. This demonstrated that while algorithm selection generally yields higher efficiency, the proposed hybrid approach significantly improves performance by selecting a per-instance number of algorithms to execute in parallel to make use of parallel processing benefits. In addition, we consider alternative approaches similar to the proposed method and show that the main approach still works better than those alternatives in most cases. These comparisons pave the way for future robust and scalable models in automatic parallel portfolio selection.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Abstract</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1/Introduction.html",
    "href": "chapters/chapter1/Introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Combinatorial Problems\nCombinatorial problems are computational tasks that arise in various fields, such as computer science and artificial intelligence (AI), and have a wide range of applications. These tasks mainly involve decision-making in arranging, choosing, or assigning elements and primarily deal with finite sets of objects under specific constraints. There are many NP-complete and NP-hard problems that, when focused on discrete optimization and decision-making within a finite set of objects, fall into the category of combinatorial problems.\nFor example, one well-known problem in this category is the Traveling Salesman Problem (TSP), which is an optimization problem with the objective of finding the shortest path between a finite set of cities, with the constraint of visiting each city at least once before returning to the starting point. TSP is a classic combinatorial problem, and it is categorized as an NP-hard problem, which means that there is no efficient polynomial-time solution for solving the TSP.\nAnother well-studied example of a combinatorial problem is the Boolean Satisfiability Problem (SAT), which is a fundamental problem in computational theory. Given a propositional formula, the objective is to determine if there is an assignment of truth values to the variables in the formula that makes the formula true (Hoos and Stützle 2005). Equation [eq:int1] shows a simple propositional formula, and the goal here is to find an assignment of Boolean values to each variable that satisfies the entire formula. Many real-world problems can be transformed into SAT problems, including software testing, software verification, bioinformatics, and various optimization problems. SAT is proved to be NP-complete, which means that while a given solution can be verified in polynomial time, finding a solution is challenging, and no polynomial-time algorithm is known to solve all instances of SAT efficiently.\n\\[\\label{eq:int1}\n        (A \\vee B) \\wedge (\\neg A \\vee C) \\wedge (B \\vee \\neg C)\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1/Introduction.html#combinatorial-problems",
    "href": "chapters/chapter1/Introduction.html#combinatorial-problems",
    "title": "2  Introduction",
    "section": "",
    "text": "Traveling Salesman Problem (TSP)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1/Introduction.html#overview-of-algorithm-and-portfolio-selection",
    "href": "chapters/chapter1/Introduction.html#overview-of-algorithm-and-portfolio-selection",
    "title": "2  Introduction",
    "section": "2.2 Overview of Algorithm and Portfolio Selection",
    "text": "2.2 Overview of Algorithm and Portfolio Selection\nIn the domain of combinatorial problems, such as SAT and TSP, the complexity of the problems means that no single algorithm can solve every problem instance optimally. This concept is aligned with the No Free Lunch Theorem, which states that there is no one-size-fits-all algorithm that excels in solving all problems (Karp 1972). Research on the development and proposal of innovative algorithms for addressing these problems has a long tradition. These include exact algorithms, approximation algorithms, and heuristics, all of which are widely discussed in the literature (Biere et al. 2021).\nHowever, accessibility of a plethora of solvers adds complexity to the problem-solving process, as it makes it difficult to identify the best algorithm for a specific problem. This is because each solver has its unique strengths, weaknesses, and strategies to solve problems. As a result, algorithm performances are often complementary and their performance depends on the characteristics of the instance (Kotthoff 2014), such as the size of the search space and the number of constraints. In other words, within a portfolio of solvers, one solver might solve a particular instance in less than a second, while another could take much longer or even fail within the available time frame.\nIn addition, relying solely on the single best solver (SBS) to solve all instances, which often outperforms other algorithms on average, is not always the most effective strategy. Although SBS may be optimal for some, it could be a terrible choice for others. This is illustrated by data from the 2018 SAT Competition (see Figure 1.1), where MapleLCMDistChronoBT was the best performing solver on average for all problem instances. Even the worst-performing solver, Yalsat, outperformed the SBS in certain instances. This shows that the solver considered the worst can, at times, surpass the SBS on specific problem instances. Thus, relying on a single solver to address all problems is inefficient, as it wastes both resources and time. In addition, identifying the best solver for each instance is not feasible without a custom selection process.\n\n\n\nAlgorithm complementary\n\n\nFor decades, one of the most prominent ideas in the combinatorial optimization literature has been the concept of designing a portfolio of solvers (Gomes and Selman 2001; Huberman, Lukose, and Hogg 1997). This approach does not require extensive knowledge in specific domains, algorithm design, or computational complexity. By treating solvers as black boxes, we can incorporate existing solvers into a portfolio without modifying or designing new ones. This approach allows us to include multiple solvers in the portfolio and take advantage of their complementary strengths, which enables the entire portfolio to excel at solving all problem instances. The actual best solver for each instance, known as the Virtual Best Solver (VBS), which is hard to identify, always exists in the portfolio.\nThere are different strategies for applying these portfolios to solve problems. Algorithm selection is one portfolio approach that has been studied extensively (Kotthoff 2014; Kerschke et al. 2019). Algorithm selection techniques are particularly important in solving combinatorial problems, as it is always challenging to choose the best solver for each instance. Selection is not a new problem, and in professional settings or daily life, we often select strategies and things that best fit the situation at hand. In Algorithm selection method, the goal is to identify the most suitable algorithm based on the characteristics of the problem. To address the algorithm selection problem, most approaches focus on employing machine learning algorithms to select suitable solvers (Bischl et al. 2016). Specifically, by extracting instance features and using a history of algorithm performance data on instances, performance models are trained to predict the best solver.\nDespite the significant success of algorithm selection techniques, there is still room for improvement. A common strategy used to do algorithm selection is choosing a single algorithm to solve a problem instance; however, this can increase the risk of selection, as machine learning models may not always generalize well. This means that they can choose an incorrect algorithm that may perform poorly, leading to suboptimal solutions. To mitigate this risk, several studies have shown that employing a more robust strategy, such as dynamically selecting an ensemble of algorithms based on the characteristics of the problem at hand, can be effective (Kadioglu et al. 2011; Xu et al. 2008).\nClassic subportfolio selection methods focus on choosing a fixed subportfolio for all instances (Roussel 2012). However, this approach is suboptimal and inefficient, as it overlooks non-dominating algorithms that perform well on specific instances. In contrast, modern subportfolio approaches emphasize the per instance selection of the solvers (Xu et al. 2008; Kadioglu et al. 2011). Here, algorithm selection plays a key role, which allows the system to leverage the strengths of multiple algorithms by dynamically adapting the selection to each problem instance’s specific characteristics. This adaptability reduces the risk associated with selection and ensures that overlooked algorithms in fixed portfolios are utilized when most effective.\nSome subportfolio methods focus on running solvers in parallel, while others use a sequential approach. In sequential execution, a selected solver attempts to solve the instance for a portion of the available time. If the instance remains unsolved, another solver is then employed. This approach can be suboptimal if the initial solver is not the best choice. On the other hand, parallel portfolios run multiple solvers simultaneously, stopping the process as soon as one solver successfully solves the instance. The concept of parallel subportfolios has attracted substantial interest in the literature and has shown great potential.\nMost proposed parallel subportfolio techniques have shown performance improvements primarily through simulations, without executing the algorithms in parallel in real-world scenarios. The few approaches that have collected actual parallel performance data have often focused on a limited number of solvers. Consequently, there is limited understanding of the overhead associated with running solvers in parallel. This gap indicates that, while the concept is promising, more robust real-world evaluations are needed to fully understand its benefits and assess the true scalability of parallel subportfolios.\nAdditionally, running too many solvers in parallel can introduce overhead on shared memory architectures due to potential contention for shared resources. Although some publications have mentioned this issue, it has not been empirically examined. Others have also overlooked this overhead, proposing parallel methods without accounting for it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1/Introduction.html#contributions",
    "href": "chapters/chapter1/Introduction.html#contributions",
    "title": "2  Introduction",
    "section": "2.3 Contributions",
    "text": "2.3 Contributions\n\n2.3.1 Comprehensive Empirical Evaluation of Solvers in Parallel\nThe preliminary focus of this dissertation is on extensive empirical evaluation of algorithms in different types of problem, including SAT, Planning, and MaxSAT. This work assesses their performance on varying numbers of cores to provide insights into how solvers behave when running in parallel. By evaluating the solvers in diverse parallel settings, we are able to quantify the computational overhead introduced when running solvers in parallel on shared memory systems, a critical aspect that has been largely ignored in previous research. The results provide an estimate of how beneficial parallelization is when running black-box solvers in parallel, and examine the trade-offs between solver performance and computational overhead. This insight helps us identify the optimal level of parallelization that maximizes efficiency without causing excessive overhead. These evaluations provided a unique dataset, which is a valuable addition to the algorithm selection community.\n\n\n2.3.2 Comparison Between Parallel Execution and Algorithm Selection\nAnother important contribution of this dissertation is the direct comparison between the parallel execution of algorithm portfolios and the traditional algorithm selection approach, where a single solver is selected based on the characteristics of the problem instance. Based on the results of our empirical evaluations, this study analyzed the performance of these two methods. The goal was to determine whether running multiple solvers in parallel can offer a competitive or superior alternative to selecting the best predicted solver for each instance.\nOur results indicate that algorithm selection outperforms parallel execution in most cases. This is because the overhead associated with running multiple solvers in parallel —especially when there are many parallel runs — can lead to resource contention (competition for CPU, memory, and other resources) and increased coordination costs on multicore architectures, which often offset the benefits of parallelization. This study is the first in the literature to offer a head-to-head comparison between these two strategies. The superiority of algorithm selection, as shown in our results, suggests that a more selective approach can achieve better performance than running a handful of solvers in parallel.\n\n\n2.3.3 Development of a Hybrid Subportfolio Selection Approach\nAnother contribution is the development of a hybrid approach that combines algorithm selection with parallel execution of solvers. Traditional algorithm selection methods choose a single solver based on the predicted performance for a given problem instance, but this can result in poor performance if the prediction is inaccurate. In contrast, our hybrid approach dynamically selects a subportfolio of solvers based on instance-specific features and runs them in parallel. Specifically, the uncertainty of the predictions made by the algorithm selector model is used to make more informed decisions. The size and composition of the subportfolio are dynamically adjusted for each problem instance using this uncertainty measurement. This \"happy middle\" approach balances the strengths of per-instance algorithm selection with the benefits of parallel execution. This method leads to significant performance improvements compared to the existing subportfolio designs in the literature. In addition, the selection and size of the subportfolio are dynamic and tailored to each instance.\n\n\n2.3.4 Fairness and Accuracy of Evaluations\nA unique aspect of this dissertation is its focus on the fairness and precision of evaluations. In shared-memory systems, existing studies often rely on theoretical models or idealized runtime assumptions when running multiple solvers in parallel. This does not reflect the actual performance of the solver in a parallel setting. However, this work uses actual runtime data collected from solver executions in a consistent hardware environment. This work provides a more realistic and fair comparison between the proposed hybrid approach and the baseline methods.\n\n\n2.3.5 Variations of the Hybrid Method\nLastly, acknowledging the superiority of the hybrid method proposed in this work, we considered variations of the method for comparison. Initially, the performance model used was a regression random forest with Jackknife uncertainty estimation (Wager, Hastie, and Efron 2014), implemented in the randomForest package in R, based on (Breiman 2001). Another implementation of the random forest algorithm, known for its speed, is Ranger (Wright and Ziegler 2017), which offers two methods for uncertainty estimation: Jackknife and infinitesimal Jackknife. This work compares the initial model with two models using Ranger. In addition, the proposed method initially introduced a metric to account for the likelihood of algorithms based on the distribution of predictions. This is replaced with the Kullback–Leibler (KL) divergence method, which quantifies how distinct algorithms are based on the distribution curves. Lastly, while our prediction model was initially trained on sequential data, we compared the results using a model trained on both sequential and parallel data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1/Introduction.html#organization-of-chapters",
    "href": "chapters/chapter1/Introduction.html#organization-of-chapters",
    "title": "2  Introduction",
    "section": "2.4 Organization of Chapters",
    "text": "2.4 Organization of Chapters\nThis dissertation is organized into six chapters, each addressed different aspects of the research problem and contributed to the overall development of the proposed approach. The chapters are organized as follows.\nThe first chapter offers an introduction to the research topic, highlighting the motivation for the study and clearly defining the research objectives. It also introduces the key contributions of the dissertation and outlines the literature gaps that this work addresses.\nThe second chapter reviews the existing literature related to the research problem, ranging from early studies in this community to recent, more promising work on subportfolio selection and scheduling. It includes a discussion of previous works in the fields of combinatorial problem solving, algorithm selection, parallel solvers, and both sequential and parallel portfolio designs.\nChapter three focuses on the first two key contributions outlined earlier. Provides a detailed presentation of the results from the empirical evaluation that compares the performance of solvers in both sequential and parallel execution modes, covering problem scenarios such as SAT, Planning, and MaxSAT. Furthermore, the results are compared to the traditional single-algorithm selection method.\nChapter four introduces the novel hybrid approach developed in this dissertation, which combines algorithm selection with parallel execution. It expands data collection to cover more problem types and discusses the formulation of dynamic subportfolios. It also provides details on how uncertainty in predicted performance can assist in selecting solvers based on instance-specific features. The effectiveness of this hybrid approach is demonstrated through a comparative analysis with existing methods and baseline models.\nChapter five provides variations of the proposed approach and compares them with the method introduced in the fourth chapter. Comparisons include different implementations of random forest, replacing the likelihood metric with the KL divergence method, and models trained on sequential and parallel data for a comprehensive analysis.\nThe final chapter summarizes the key findings and contributions of this dissertation. It also outlines potential directions for future research and acknowledges the limitations of this study, including areas for improvement in algorithm selection techniques and parallel portfolio methods.\n\n\n\n\n\n\nBiere, Armin, Marijn Heule, Hans van Maaren, and Toby Walsh, eds. 2021. Handbook of Satisfiability - Second Edition. Vol. 336. Frontiers in Artificial Intelligence and Applications. IOS Press. https://doi.org/10.3233/FAIA336.\n\n\nBischl, Bernd, Pascal Kerschke, Lars Kotthoff, Marius Lindauer, Yuri Malitsky, Alexandre Fréchette, Holger Hoos, et al. 2016. “ASlib: A Benchmark Library for Algorithm Selection.” Artificial Intelligence 237: 41–58.\n\n\nBreiman, Leo. 2001. “Random Forests.” Machine Learning 45: 5–32.\n\n\nGomes, Carla, and Bart Selman. 2001. “Algorithm Portfolios.” Artificial Intelligence 126: 43–62.\n\n\nHoos, Holger H., and Thomas Stützle. 2005. “Propositional Satisfiability and Constraint Satisfaction.” In Stochastic Local Search, 257–312. The Morgan Kaufmann Series in Artificial Intelligence. San Francisco: Morgan Kaufmann. https://doi.org/https://doi.org/10.1016/B978-155860872-6/50023-8.\n\n\nHuberman, Bernardo A., Rajan M. Lukose, and Tad Hogg. 1997. “An economics approach to hard computational problems.” Science 275 (5296): 51–54. https://doi.org/10.1126/science.275.5296.51.\n\n\nKadioglu, Serdar, Yuri Malitsky, Ashish Sabharwal, Horst Samulowitz, and Meinolf Sellmann. 2011. “Algorithm selection and scheduling.” Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 6876 LNCS: 454–69. https://doi.org/10.1007/978-3-642-23786-7_35.\n\n\nKarp, Richard M. 1972. “Reducibility Among Combinatorial Problems.” In Complexity of Computer Computations: Proceedings of a Symposium on the Complexity of Computer Computations, Held March 20–22, 1972, at the IBM Thomas j. Watson Research Center, Yorktown Heights, New York, and Sponsored by the Office of Naval Research, Mathematics Program, IBM World Trade Corporation, and the IBM Research Mathematical Sciences Department, edited by Raymond E. Miller, James W. Thatcher, and Jean D. Bohlinger, 85–103. Boston, MA: Springer US. https://doi.org/10.1007/978-1-4684-2001-2_9.\n\n\nKerschke, Pascal, Holger H. Hoos, Frank Neumann, and Heike Trautmann. 2019. “Automated Algorithm Selection: Survey and Perspectives.” Evolutionary Computation 27 (1): 3–45. https://doi.org/10.1162/evco_a_00242.\n\n\nKotthoff, Lars. 2014. “Algorithm selection for combinatorial search problems: A survey.” AI Magazine 35 (3): 48–69.\n\n\nRoussel, Olivier. 2012. “Description of Ppfolio (2011).” Proc. SAT Challenge, 46.\n\n\nWager, Stefan, Trevor Hastie, and Bradley Efron. 2014. “Confidence intervals for random forests: The jackknife and the infinitesimal jackknife.” The Journal of Machine Learning Research 15 (1): 1625–51.\n\n\nWright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” Journal of Statistical Software 77 (1): 1–17. https://doi.org/10.18637/jss.v077.i01.\n\n\nXu, Lin, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2008. “SATzilla: Portfolio-Based Algorithm Selection for SAT.” J. Artif. Int. Res. 32 (1): 565–606.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2/Background.html",
    "href": "chapters/chapter2/Background.html",
    "title": "3  Background",
    "section": "",
    "text": "3.1 Parallel Solvers\nCombinatorial problems involve problems whose goal is to find the best arrangement or selection from a finite set of items. These problems mainly focus on assignment, sorting, selection, and optimization tasks such as scheduling and routing. These challenges are common in multiple disciplines, such as computer science, operations research, bioinformatics, and artificial intelligence (Biere et al. 2021). Many combinatorial problems fall into the categories of NP-complete or NP-hard, meaning they can be significantly computationally expensive.\nCommon examples of these problems include boolean satisfiability (SAT), integer programming (IP), quantified boolean formulas (QBF), constraint satisfaction problem (CSP), and any other NP-complete problems like the 21 Karp NP-complete problems (Karp 1972). Additionally, NP-hard problems such as the traveling salesman (TSP), answer set programming (ASP), scheduling, and mixed-integer programming (MIP) fall into this category. Over several decades, various algorithms and complex mathematical methods have been developed to solve these problems, which mainly deal with discrete optimization.\nEvery year, competitions are held to compare the proposed algorithms. For example, the SAT community organized annual SAT competitions to evaluate and benchmark algorithms on various tracks and domains (Tomáš Balyo, Heule, and Järvisalo 2017). Similarly, annual QBF solver evaluations (QBFEVAL), MiniZinc Challenge, ASP Competition, and International Planning Competition exist, focusing on quantified Boolean formulas, constraint programming, ASP and planning solvers, respectively (Pulina and Seidl 2019; Stuckey et al. 2014; GEBSER, MARATEA, and RICCA 2019; Taitler et al. 2024).\nMany solvers have been proposed and continue to evolve, leading to significant improvements in both efficiency and effectiveness. In addition to exact solvers (Woeginger 2002), researchers have also explored approximation algorithms (Williamson and Shmoys 2011) and heuristics (Pearl 1984) as alternatives to achieve solutions close to the optimum, particularly in cases where exact solutions are computationally infeasible. These methods often leverage problem-specific knowledge to guide the search process, and they often sacrifice optimality for speed and produce suboptimal solutions.\nMoreover, the inherent difficulty of these problems means that no polynomial-time solutions are known, and there is no one-size-fits-all solution to solve them. The \"No Free Lunch\" theorem formalizes the idea that, when considering every possible problem, no algorithm outperforms others on average; they all have equivalent performance (Wolpert and Macready 1997). It is the problem-specific performance that makes some algorithms appear to be better suited for certain domains (Wolpert and Macready 1997). The performance of any algorithm is limited by the specific characteristics of the problem instance it encounters. The results of (Nudelman et al., n.d.) contribute to a deeper understanding of the relationship between the characteristics of the instance (that is, the number of clauses and variables in the SAT), which is known to correlate with the difficulty of the problem.\nIn the realm of algorithm development, both sequential and parallel solvers have been extensively studied. Theoretically, many problem-solving approaches were originally sequential. For instance, in the SAT domain, Davis-Putnam-Loveland-Logemann (DPLL) algorithm (M. Davis, Logemann, and Loveland 1962) was initially designed to be sequential. Sequential solvers generally operate without awareness of available resources and do not dynamically adapt to them. The results of (Hölldobler, Manthey, and Saptawijaya 2010) have revealed that the performance and efficiency of a sequential solver depend heavily on hardware resources and low-level processing. This underscores the need to utilize parallel resources in problem-solving to improve the performance.\nParallel solvers then emerged as multicore architectures became increasingly accessible. Initially, the first parallel SAT solvers utilized single-core CPUs, with multiple units communicating over a network and employing the leader-follower model (Hölldobler et al. 2011). This approach is showcased by PMSat, a parallel version of the sequential MiniSAT solver (Gil, Flores, and Silveira 2009), as well as the PSATO solvers (ZHANG, BONACINA, and HSIANG 1996) and PaSAT (Sinz, Blochinger, and Küchlin 2001), which introduced search-space partitioning between processing units communicating over MPI in distributed systems (Hyvärinen, Junttila, and Niemelä 2010). Additionally, advancements in algorithm design made the development of parallel solvers possible. For example, in the SAT domain, the transition from DPLL solvers to Conflict-Driven Clause Learning (CDCL) solvers (Biere et al. 2009), divide-and-conquer techniques (Le Frioux 2019), and the implementation of restart strategies (Hyvärinen, Junttila, and Niemelä 2008) facilitated the parallelization of existing algorithm designs. These design let the solvers share the learned clauses or split search spaces between processing units.\nFollowing the emergence of shared memory architectures, parallel solvers started to exploit multicore CPUs with shared memory among cores (Singer and Vagner 2006). Modern parallel solvers inherited the methods developed from earlier distributed parallelization approaches. For example, the parallel SAT solver, Plingeling (Biere 2012), inherits its algorithm design from the sequential Lingeling solver and incorporates a clause-sharing technique among cores for SAT solving. However, in earlier versions of Plingeling, despite using shared-memory systems, clauses were copied by each core. As noted in (Biere 2013; Aigner et al. 2013), the memory used by each processing unit was physically separated, leading to an n-fold increase in memory usage.\nOther solvers, such as the one proposed in (Hyvarinen and Wintersteiger 2012; Manthey 2011), used a more finer granularity in their parallelization approach where multiple cores exchange clauses to solve the problem; however, these methods were not scalable to a high number of cores and resulted in limited improvement. With regard to modern graphical processing unit (GPU) and field-programmable gate array (FPGA) computing, several solutions have been proposed (Redekopp and Dandalis 2000; Zhong et al. 1998; J. D. Davis et al. 2008; Gulati et al. 2009; Osama, Wijs, and Biere 2023, 2021; Collevati, Dovier, and Formisano 2022; Meyer et al. 2010). However, some have not demonstrated promising results and while some show potential, discussions of these solvers are outside the focus of this thesis.\nDespite all the developments in parallel computing and parallel algorithm designs, the transition from sequential to parallel solvers remains highly challenging, if not impossible, in some cases. Most solvers are originally designed for sequential execution; therefore, adapting them to parallel environments often requires a thorough understanding of the problem domain, algorithm design, and many core systems.\nA number of works have shown that this problem can be overcome by improving performance without directly developing new solvers. Instead, existing solvers can be used and combined in a portfolio, selecting the appropriate solver(s) (Kotthoff 2014; Xu et al. 2008; Kadıoglu, Malitsky, and Sellmann, n.d.). In cases where multiple solvers are chosen, they can be run in a schedule, in parallel, or in combination with each other to achieve superior performance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2/Background.html#parallel-solvers",
    "href": "chapters/chapter2/Background.html#parallel-solvers",
    "title": "3  Background",
    "section": "",
    "text": "Algorithm Selection using ML models",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2/Background.html#algorithm-selection",
    "href": "chapters/chapter2/Background.html#algorithm-selection",
    "title": "3  Background",
    "section": "3.2 Algorithm Selection",
    "text": "3.2 Algorithm Selection\nAlgorithm selection is a meta-algorithmic technique that was first introduced by John Rice (Rice 1976) in 1976. The algorithm selection problem is strategically choosing the most suitable algorithm from a set of candidates to solve a specific problem instance using the features and characteristics of the problem instances. This problem is closely related to general optimization theory, as both involve finding the best solution among a set of options (Rice 1976). Algorithm selection is an important task in optimizing the performance of computationally expensive tasks, particularly those that can be translated into combinatorial problems (Kotthoff 2014), NP-complete and NP-hard problems (Musliu and Schwengerer 2013; Sitaru and Raschip 2022).\nDuring the last two decades, the algorithm selection problem has attracted significant attention from both researchers and practitioners as it is relevant in various domains, including high-performance computing (Ciorba et al. 2023), sorting (Somshubra Majumdar 2016), probabilistic inference (Guo and Hsu 2003; Kocamaz 2013), software validation (Richter et al. 2020), software verification (Wang et al. 2021; Leeson and Dwyer 2024), software package dependency management (Nichols et al. 2023) and data mining (Ewald, Uhrmacher, and Saha 2010; Batista dos Santos and Merschmann 2020). Algorithm selection is also applicable in the fields of meta-learning and automated machine learning (AutoML), where the goal is to build a meta-model that recommends the best machine learning algorithm, configurations and hyper-parameters for a given training task (Brazdil et al. 2022; Vanschoren 2019; Hutter, Kotthoff, and Vanschoren 2019). In algorithm selection problem, the search space is a discrete set of algorithms, while in problems such as the hyperparameter optimization (HPO) and the combination algorithm selection and hyperparameter optimization (CASH) problem (Thornton et al. 2013), the search spaces are typically continuous or heterogeneous configuration spaces (Brazdil et al. 2022).\nAlgorithm selection is sometimes known as portfolio-based algorithm selection in the literature (Xu, Hoos, and Leyton-Brown 2010; Xu et al. 2011), and is used in combination with automatic algorithm configuration. Algorithm Configuration is a specialized subset of algorithm selection (Hutter, López-Ibáñez, et al. 2014) in which algorithms with varying configuration parameters build a portfolio. The configuration parameters of these algorithms are dynamically adjusted based on the problem instance. This allows algorithms to adapt to the specifics of a problem, potentially improving their performance. This approach involves using different configurations of an algorithm to design a portfolio of solvers and choose the appropriate algorithm to solve problems. Focusing on the literature on algorithm configuration techniques is beyond the scope of this chapter.\nA traditional approach to address algorithm selection is the \"winner-take-all\" or per-set algorithm selection strategy (Kerschke et al. 2019), where different algorithms are evaluated based on their performance across a problem distribution, and the one with the lowest average runtime is chosen. However, this per-distribution selection often leads to the neglect of algorithms that may not perform well on average, but could excel in specific instances (Leyton-Brown et al. 2003). Per-instance algorithm selection, however, has proven effective in numerous problem solving scenarios (Bischl et al. 2016).\nInstance-based selection of an effective algorithm is inherently complex, and evaluation of numerous algorithms against various performance criteria is needed, which can lead to significant computational demands. In addition, the abundance of algorithms designed to address different types of problem has underscored the importance of developing effective strategies for selecting the best algorithm for a given context. The field of algorithm selection has evolved significantly to address these challenges and there is a wide choice of algorithm selection techniques available in the literature.\nOne key component of per-instance algorithm selection is feature selection. In order to improve the design and selection of algorithms, measuring the difficulty of instances and identifying features of the instance that contribute to its complexity is a fundamental task (Smith-Miles and Lopes 2012; Cárdenas-Montes 2016). Different studies have concluded that problem instance features contribute distinctively to the performance of different solvers. (Leyton-Brown, Nudelman, and Shoham 2002) used statistical regression models to understand the empirical hardness of NP-hard problems, which is believed to help tailor algorithms based on the hardness of instances. (Nudelman et al., n.d.) also contributed to understanding the impact of the features of SAT instances on the difficulty of the problem. In short, the literature pertaining to the study of instance features strongly suggests that solvers need to be either designed to address a specific instance or carefully selected to solve the particular problem.\nFor various types of problem, numerous studies have focused on collecting and identifying features to aid in training performant algorithm selectors. In the domain of SAT, works such as (Nudelman et al., n.d.; Xu et al. 2008; Hutter, Xu, et al. 2014) have made significant contributions by extracting features from the CNF formulas of SAT instances. More recently, (Shavit and Hoos 2024) revised the SATzilla feature extractor, showing notable performance improvements with the updated SAT features. In the field of AI planning, studies such as (Howe et al. 2000; Fawcett et al. 2014; Roberts et al. 2008; Cenamor, De La Rosa, and Fernández 2013) have contributed by extracting features from the PDDL format of AI planning instances. Although most studies focus on instance features, some have also explored extracting algorithm-specific features and incorporating them into performance model training (Pulatov et al. 2022). Algorithm selection using machine learning and statistical analysis is made possible through the extraction and utilization of these features.\nFeature selection is only appropriate if its calculation time is considerably less than the time needed to run all the algorithms. (Carchrae and Beck 2005) explored low-knowledge techniques that focus only on observed performance improvement, demonstrating that effective algorithm selection does not always require deep domain knowledge and complex feature extractions and predictive models. In addition, in scenarios where traditional meta-features are hard to obtain, such as image segmentation tasks, verification-based approaches could be used (Lukac and Kameyama 2023).\nThere is extensive literature on per-instance algorithm selection approaches, developed to resolve various NP-complete and NP-hard problems, including SAT, TSP, and other related challenges (Xu et al. 2008, 2012; Kerschke et al. 2018, 2019; Kotthoff 2014). Some methods are offline, where solvers are selected before solving the problems (e.g., (Xu et al. 2008)), while others are online, where solvers are continuously selected during the solving process (e.g., (Arbelaez, Hamadi, and Sebag 2009)).\nA large number of studies in the broader literature have examined machine learning-based approaches to tackle the algorithm selection problem, ranging from simple classifiers to complex ensemble approaches. These studies focus on using machine learning algorithms to learn the performance mapping from problem instances to algorithms by extracting features from instances (Kotthoff 2014). To name a few, (Guo and Hsu 2005) has focused on training two classifiers to predict the appropriate exact and approximation algorithms to solve the most probable explanation (MPE) problems. Rule-based systems also play a role in the literature using rules or decision trees to assign algorithms to problem instances based on extracted features (Ali and Smith 2006). (Lieder et al. 2014) considers algorithm selection as a special case of metareasoning and proposes a model for human cognitive strategy selection to solve the sorting problem.\nIn the ASLib paper, different regression and classification methods are also compared, with the results showing that the RandomForest regression is superior to other methods (Bischl et al. 2016). Before the implementation of the ASLib data format and library, algorithm selection studies were scattered and many were unaware of each other, so comparing different approaches was challenging due to the absence of a standardized format. ASLib was proposed to standardize model training and to collect benchmarking data in a structured manner which facilitated comparisons (Bischl et al. 2016). In response to the need for a fair comparison of different algorithm selection methods, the ICON Challenge on Algorithm Selection was held in 2015 and 2017 (M. Lindauer, van Rijn, and Kotthoff 2019). In 2015, the Zilla system (Xu et al. 2008) was the winner among eight different methods. In the 2017 challenge, ASAP.v2 (Gonard, Schoenauer, and Sebag 2019) was the overall winner of eight other methods. The comparison in this competition was conducted using the ASLib format and library.\"\nOther efforts exist which have not specifically focused on improving algorithm selector choices, but rather on making the decision-making process more intuitive or reducing the cost of performance model training. Some efforts have aimed to improve the transparency of algorithm selection through explainability methods. For example, (Visentin et al. 2024) employed iterative feature selection and SHAP analysis to explain selection decisions. Additionally, the cost of training algorithm selection models can be high due to the need to run candidate algorithms on a representative set of training instances. (Kuş et al. 2024) reduces training costs by selecting a subset of training instances using active learning, with timeout predictors and progressively increasing timeouts to further minimize costs. (Brighton and Mellish 2002) has also focused on instance selection which means focusing on choosing a representative set of instances for training, which can help identify the most suitable algorithm and improve algorithm selector efficiency.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2/Background.html#algorithm-portfolios-and-scheduling",
    "href": "chapters/chapter2/Background.html#algorithm-portfolios-and-scheduling",
    "title": "3  Background",
    "section": "3.3 Algorithm Portfolios and Scheduling",
    "text": "3.3 Algorithm Portfolios and Scheduling\nIn response to the reality that no single algorithm can effectively address all types of problems, the concept of utilizing a portfolio emerged (C. Gomes and Selman 2001). Drawing from the concept of risk diversification in finance, as discussed in (Huberman, Lukose, and Hogg 1997), it is generally wiser to distribute investments across multiple options rather than concentrating on a single one.\nSimilarly, to optimize problem solving, an algorithm portfolio includes a diverse set of algorithms with complementary performance characteristics, ensuring that at least one will be effective in solving any given instance. These algorithms often have different probability distributions for runtimes. We can leverage their strengths by combining them into algorithm portfolio approaches to take advantage of differences in probability distributions to enhance overall efficiency as proposed by (C. Gomes and Selman 2001; C. P. Gomes and Selman 1997).\nSubsequently, this portfolio can be leveraged through various methods to use the strengths of multiple algorithms and ultimately reduces risk (C. Gomes and Selman 2001; Huberman, Lukose, and Hogg 1997; C. P. Gomes and Selman 1997; Malitsky et al. 2014). Different portfolio approaches have been proposed to effectively harness the capabilities of multiple complementary algorithms. An approach already discussed in this chapter is algorithm selection, where the objective is to select a single suitable algorithm per instance. Another approach, plain portfolio selection, includes some algorithms that can be executed in parallel on a multicore machine (Malitsky et al. 2012; M. Lindauer, Bergdoll, and Hutter 2016; H. H. Hoos et al. 2015; Roussel 2012; Wotzlaw et al. 2012) or sequentially on a single processor by splitting the available time between solvers (Kadioglu et al. 2011; T. M. Lindauer 2014; M. Lindauer, Bergdoll, and Hutter 2016; Malitsky et al. 2013; Xu et al. 2008; O’Mahony et al. 2008; Amadini, Gabbrielli, and Mauro 2018; H. Hoos, Lindauer, and Schaub 2014; Cenamor, De La Rosa, and Fernández 2016). In addition, numerous alternative approaches have been developed to enable algorithms in the portfolio to cooperate with each other, such as clause sharing, clause exchange, and search space splitting. Ultimately, these approaches outperform any individual algorithm.\nIn the plain sequential portfolio domain, over time, a vast body of literature has developed successful sequential portfolio solvers. In particular, 3S (Kadioglu et al. 2011) and CSHC (Malitsky et al. 2013) both secured gold medals in the 2011 and 2013 SAT competitions, respectively. SATZilla (Xu et al. 2008) won the SAT Challenge 2012, CPHydra (O’Mahony et al. 2008) was the winner of Intetnational Constraint Solver Competition 2008, and Sunny-CP won MiniZinc Challenge in 2015 (Amadini, Gabbrielli, and Mauro 2018). In addition, Claspfolio (H. Hoos, Lindauer, and Schaub 2014) earned the gold medal in both the ASP 2009 and 2011 competitions, and IBaCOP2 (Cenamor, De La Rosa, and Fernández 2016) won the Sequential Satisficing Track in the 2014 International Planning Competition.\n3S (Kadioglu et al. 2011) combines solver selection and scheduling. It trains a performance model using K-Nearest Neighbors (KNN) and improves the KNN approach with Distance-Based Weighting and Clustering-Based Adaptive Neighborhood Size. Additionally, they enhanced the model by integrating solver scheduling and testing various approaches such as static scheduling, dynamic scheduling, fixed-time scheduling, and semistatic scheduling. Semi-static scheduling performed best, where 90% of the time was allocated to the solver suggested by the KNN model, and 10% to a static solver schedule, which chose the top solvers averaged over all instances and allocated time statically.\nCSHC (Malitsky et al. 2013) selects a portfolio of solvers using a classification method known as cost-sensitive hierarchical clustering and then sequentially allocates time to each solver. In short, the cost-sensitive hierarchical clustering method selects algorithms by grouping problem instances based on their features and the cost of misclassification. It starts with all instances in one cluster and splits them to maximize agreement on the best algorithm within each cluster. Clusters with too few instances are discarded, and merging is considered if it improves performance.\nIn SATZilla (Xu et al. 2008), seven solvers are selected by manually analyzing competition results to identify algorithms that perform best on specific subsets of instances. An empirical hardness model, trained on instance features, is then used to select the most appropriate solver for each instance. Additionally, SATZilla employs presolvers, typically two, that run before the main solver, quickly resolving instances that are easy to solve. For harder instances which did not get resolved in the presolver phase, one solver is chosen from the seven based on the model’s predictions, and the remaining time is allocated to this solver based on its expected performance.\nCPHydra (O’Mahony et al. 2008) is another sequential algorithm portfolio approach designed to solve constraint satisfaction problems using case-based reasoning (CBR). CBR is a lazy machine learning method in which no model is explicitly trained; instead, the selection is based on past cases and experiences. CPHydra uses syntactic features, such as the average and maximum domain size, and solver-specific features, including recorded search statistics, number of explored nodes, and runtime, which describe the problem instance. Subsequently, the KNN algorithm is employed to query the top k (with k set to 10) similar cases to the current problem instance to choose the dominated solvers. The available CPU time limit is then divided among multiple solvers, in contrast to approaches such as SATzilla, which rely on a single solver. This division is formulated as a small knapsack problem with the aim of maximizing the probability of finding a solution. The scheduling is computed to maximize the number of similar cases that can be solved using the assigned schedule.\nClaspfolio (Gebser et al. 2011) considers multiple configurations of the clasp ASP solver and uses support vector regression (SVR) to select suitable solvers. Claspfolio is composed of four key parts: an ASP grounder, gringo; a lightweight clasp solver called claspre (which extracts features from the problem instance); a scoring mechanism that evaluates clasp configurations based on the extracted features; and finally, the configuration with the highest score is selected to solve the instance. There is no explicit scheduling in the original Claspfolio approach, but it is considered a sequential portfolio approach because it selects one solver configuration and runs it sequentially. (H. Hoos, Lindauer, and Schaub 2014) later introduced Claspfolio2, which includes a solver schedule based on a pre-solving method. This approach employs a timeout-minimal pre-solving schedule using ASPEED (H. H. Hoos et al. 2015), where each solver is assigned a specific time budget to solve a given instance. Although ASPEED is capable of handling multicore scheduling, Claspfolio2 primarily focuses on sequential scheduling, maintaining the original approach’s emphasis on running solvers one after another.\nIBaCOP (Cenamor, De La Rosa, and Fernández 2016) is a portfolio approach in the planning domain. It designs a portfolio by creating a per-instance configurable portfolio rather than using a fixed configuration. The design process involves selecting a set of planners (five in the experiments) to narrow down those likely to perform well for a given task. This selection is done using Pareto efficiency, choosing planners that offer a balance between different performance criteria without being dominated by others. The selected planners are then allocated a portion of time on a single processor. IBaCOP (Cenamor et al. 2014) was later developed based on the previous version, but more planners were included in the method, and a performance prediction model was trained instead of using Pareto efficiency for choosing the sub-portfolio, making the selection dynamic.\nSNNAP (Collautti et al. 2013) is a KNN-based approach for solving SAT problems that combines supervised and unsupervised learning to create a solver portfolio. It uses random forests to predict the runtime of solvers on specific instances, and based on these predictions, it selects similar instances from the training set using KNN, choosing the top-performing solvers predicted to perform well on the given instance. The value of k is defined by the user, with the top three solvers being selected. SNNAP has been shown to outperform ISAC (Kadioglu et al. 2010), a method for the configuration of algorithm based on instance that ultimately selects a single algorithm, in experiments. The available time is divided between the selected solvers, and they are run sequentially.\nSunny (Amadini, Gabbrielli, and Mauro 2014) is a portfolio designed to solve constraint satisfaction problems (CSP). It uses KNN to select a subset of training instances that are closest to the problem at hand. It then computes a schedule of solvers that solve the maximum number of instances within a specific time limit, allocating a portion of time to each solver to create a schedule. It also includes a backup solver to handle exceptional cases. Sunny-CP (Amadini, Gabbrielli, and Mauro 2018) is based on Sunny and extends its capabilities to handle both CSPs and constraint optimization problems (COPs). Sunny-CP includes an improved feature extraction tool compared to Sunny. Both Sunny and Sunny-CP are sequential portfolio designs.\nAutoFolio (M. Lindauer et al. 2015) is another effort that integrates various algorithm and portfolio selection approaches. It is built on the claspfolio2 framework and employs a highly parametric framework for algorithm selection. AutoFolio is a general-purpose system, incorporating techniques from systems like 3S, ISAC, SATzilla, SNAPP as well as multiclass classification, pairwise classification, and various regression methods. Unlike many other systems, AutoFolio is not specific to any particular problem domain, aiming instead to harness the strengths of each algorithm selection approach it includes. However, it does not focus on parallel portfolios.\nA recent sequential portfolio design is MedleySolver (Pimpalkhare et al. 2021), which is designed to solve satisfiability modulo theories (SMT) problems. In this method, multiple solvers are selected based on the multi-armed bandit (MAB) framework and run sequentially. MAB models a situation where an agent must choose between multiple options, balancing the trade-off between exploring new solvers and exploiting the best-known solvers for the highest reward. MedleySolver essentially provides an ordering for a sequence of SMT solvers and splits available time between solvers.\nIn the domain of parallel portfolio solvers, the Plain Parallel Portfolio (PPP) approach involves running multiple solvers on same instance in parallel, leveraging multi-core shared memory architectures (Aigner et al. 2013). In this approach, sequential solvers are treated as \"black boxes\", they do not share any information with each other, and as soon as any solver resolves the instance, the entire process stops (Aigner et al. 2013; M. Lindauer, Hoos, and Hutter 2015). Some PPP methods are static, where the sub-portfolios are selected to run on all problems and are not instance-based. Notable examples of static PPP solvers in the SAT domain include ManySAT (Hamadi, Jabbour, and Sais 2009), ppfolio (Roussel 2012), pfolioUZK (Wotzlaw et al. 2012), with ppfolio excelling in the 2011 SAT Competition. The PPP approach typically selects the top solvers that consistently outperform others on average. This approach has also been used in methods such as SATZilla, where a presolving phase, as mentioned above, is employed (Xu et al. 2008). However, this strategy often overlooks solvers that, while less dominant overall, may significantly outperform the top solvers in specific instances.\nTo address these limitations, more innovative approaches, such as P3S (Malitsky et al. 2012), have been introduced. P3S is a parallel version of 3S (Kadioglu et al. 2011) and participated in the SAT Challenge 2012. It uses the same method as 3S but operates in parallel. ASPEED (H. H. Hoos et al. 2015), a static ASP technique, defines a fixed schedule for multiple solvers to run in parallel, solving this scheduling problem using answer set programming. Sunny-cp2 (Amadini, Gabbrielli, and Mauro 2015) is another approach, a parallel version of Sunny-cp, designed for the parallel execution of constraint satisfaction and constraint optimization solvers. We will discuss these approaches in more detail in Chapter 4, where we introduce another approach in this field that surpasses the options currently available in the literature.\nArvandHerd (Valenzano et al. 2012) is another example of parallel portfolio design, but in planning problems. This method won tracks in the 2011 Planning Competition. The portfolio includes multiple configurations of two different planners. Typically, on three cores, two cores are dedicated to running multiple configurations of the Arvand solver, while one core is allocated to LAMA-2008 solver configurations. The process starts with one configuration, and if that configuration exhausts the memory, the planner restarts with another configuration.\nAlthough there have been many studies on plain portfolio designs, no plain portfolio solvers have been accepted into the SAT competition since 2020 (Froleyks et al. 2021). Prior to 2020, and since the SAT Competition 2016, plain portfolio solvers had the opportunity to enter the No Limit track where submitting the source code was not required. Before 2016, they were accepted as regular solvers, which is why we only have gold medalists and winning plain portfolio approaches related to competitions before 2016. Cooperative portfolio solvers, on the other hand, have been accepted in all tracks, especially in the parallel track of the SAT competition. In this portfolio approach, solvers can communicate, share learned clauses, or split search spaces to improve efficiency and performance.\nFor example, PRS-parallel, a parallel portfolio solver capable of running in distributed environments, supports clause sharing and won the parallel track of the 2023 SAT Competition (Tomas Balyo et al. 2023). PaKis, the parallel portfolio solver that won the parallel track of the 2021 SAT Competition, relies on different configurations of the Kissat solver but does not support information sharing between solvers (Tomas Balyo et al. 2021). There is a vast body of literature on cooperative portfolios, which is beyond the scope of this chapter.\n\n\n\n\n\n\nAigner, Martin, Armin Biere, Christoph M. Kirsch, Aina Niemetz, and Mathias Preiner. 2013. “Analysis of Portfolio-Style Parallel SAT Solving on Current Multi-Core Architectures.” In POS-13. Fourth Pragmatics of SAT Workshop, a Workshop of the SAT 2013 Conference, July 7, 2013, Helsinki, Finland, edited by Daniel Le Berre, 29:28–40. EPiC Series in Computing. EasyChair. https://doi.org/10.29007/73N4.\n\n\nAli, Shawkat, and Kate A Smith. 2006. “On Learning Algorithm Selection for Classification.” Applied Soft Computing 6 (2): 119–38.\n\n\nAmadini, Roberto, Maurizio Gabbrielli, and Jacopo Mauro. 2014. “SUNNY: A Lazy Portfolio Approach for Constraint Solving.” Theory Pract. Log. Program. 14 (4-5): 509–24. https://doi.org/10.1017/S1471068414000179.\n\n\n———. 2015. “A Multicore Tool for Constraint Solving.” In Proceedings of the 24th International Conference on Artificial Intelligence, 232–38.\n\n\n———. 2018. “SUNNY-CP and the MiniZinc Challenge.” Theory and Practice of Logic Programming 18 (1): 81–96. https://doi.org/10.1017/S1471068417000205.\n\n\nArbelaez, Alejandro, Youssef Hamadi, and Michele Sebag. 2009. “Online heuristic selection in constraint programming.”\n\n\nBalyo, Tomas, Nils Froleyks, Marijn Heule, Markus Iser, Matti Järvisalo, and Martin Suda, eds. 2021. Proceedings of SAT Competition 2021: Solver and Benchmark Descriptions. Anthology or special issue. Department of Computer Science Report Series b. Department of Computer Science Report Series B. http://hdl.handle.net/10138/333647.\n\n\nBalyo, Tomas, Marijn Heule, Markus Iser, Matti Järvisalo, and Martin Suda, eds. 2023. Proceedings of SAT Competition 2023: Solver, Benchmark and Proof Checker Descriptions. Anthology or special issue. Department of Computer Science, Helsinki Institute for Information Technology, Constraint Reasoning; Optimization research group / Matti Järvisalo.\n\n\nBalyo, Tomáš, Marijn J H Heule, and Matti Järvisalo. 2017. “SAT Competition 2017 Solver and Benchmark Descriptions.” Proceedings of SAT COMPETITION 2017, 14–15.\n\n\nBatista dos Santos, Vânia, and Luiz Henrique de Campos Merschmann. 2020. “Metalearning Applied to Multi-Label Text Classification.” In Proceedings of the XVI Brazilian Symposium on Information Systems. SBSI ’20. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3411564.3411646.\n\n\nBiere, Armin. 2012. “Lingeling and friends entering the SAT challenge 2012.” Department of Computer Science Series of Publications B, January, 33–34.\n\n\n———. 2013. “Lingeling, Plingeling and Treengeling Entering the SAT Competition 2013.” In. https://api.semanticscholar.org/CorpusID:972178.\n\n\nBiere, Armin, Marijn Heule, Hans van Maaren, and Toby Walsh. 2009. “Conflict-Driven Clause Learning Sat Solvers.” Handbook of Satisfiability, Frontiers in Artificial Intelligence and Applications, 131–53.\n\n\n———, eds. 2021. Handbook of Satisfiability - Second Edition. Vol. 336. Frontiers in Artificial Intelligence and Applications. IOS Press. https://doi.org/10.3233/FAIA336.\n\n\nBischl, Bernd, Pascal Kerschke, Lars Kotthoff, Marius Lindauer, Yuri Malitsky, Alexandre Fréchette, Holger Hoos, et al. 2016. “ASlib: A Benchmark Library for Algorithm Selection.” Artificial Intelligence 237: 41–58.\n\n\nBrazdil, P., J. N. van Rijn, C. Soares, and J. Vanschoren. 2022. Metalearning: Applications to Automated Machine Learning and Data Mining. Cognitive Technologies. Springer International Publishing. https://books.google.com/books?id=zDcOzgEACAAJ.\n\n\nBrighton, Henry, and Chris Mellish. 2002. “Advances in Instance Selection for Instance-Based Learning Algorithms.” Data Mining and Knowledge Discovery 6: 153–72.\n\n\nCarchrae, Tom, and J. Christopher Beck. 2005. “Applying Machine Learning to Low-Knowledge Control of Optimization Algorithms.” Computational Intelligence 21. https://api.semanticscholar.org/CorpusID:7953876.\n\n\nCárdenas-Montes, Miguel. 2016. “Evaluating the Difficulty of Instances of the Travelling Salesman Problem in the Nearby of the Optimal Solution Based on Random Walk Exploration.” In Hybrid Artificial Intelligent Systems, edited by Francisco Martínez-Álvarez, Alicia Troncoso, Héctor Quintián, and Emilio Corchado, 299–310. Cham: Springer International Publishing.\n\n\nCenamor, Isabel, Tomás De La Rosa, and Fernando Fernández. 2013. “Learning Predictive Models to Configure Planning Portfolios.” In Proceedings of the 4th Workshop on Planning and Learning (ICAPS-PAL 2013), 14–22. Citeseer.\n\n\n———. 2016. “The IBaCoP planning system: instance-based configured portfolios.” J. Artif. Int. Res. 56 (1): 657–91.\n\n\nCenamor, Isabel, Tomás de la Rosa, Fernando Fernández, et al. 2014. “IBACOP and IBACOP2 planner.” IPC 2014 Planner Abstracts, 35–38.\n\n\nCiorba, Florina M., Ali Mohammed, Jonas H. Müller Korndörfer, and Ahmed Eleliemy. 2023. “Automated Scheduling Algorithm Selection in OpenMP.” In 2023 22nd International Symposium on Parallel and Distributed Computing (ISPDC), 106–9. https://doi.org/10.1109/ISPDC59212.2023.00025.\n\n\nCollautti, Marco, Yuri Malitsky, Deepak Mehta, and Barry O’Sullivan. 2013. “SNNAP: Solver-based nearest neighbor for algorithm portfolios.” In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III 13, 435–50. Springer.\n\n\nCollevati, M., Agostino Dovier, and A. Formisano. 2022. “GPU Parallelism for SAT Solving Heuristics.” CEUR-WS.\n\n\nDavis, John D., Zhangxi Tan, Fang Yu, and Lintao Zhang. 2008. “A Practical Reconfigurable Hardware Accelerator for Boolean Satisfiability Solvers.” In 2008 45th ACM/IEEE Design Automation Conference, 780–85. https://doi.org/10.1145/1391469.1391669.\n\n\nDavis, Martin, George Logemann, and Donald Loveland. 1962. “A Machine Program for Theorem-Proving.” Commun. ACM 5 (7): 394–97. https://doi.org/10.1145/368273.368557.\n\n\nEwald, Roland, Adelinde M. Uhrmacher, and Kaustav Saha. 2010. “Data Mining for Simulation Algorithm Selection.” In. ICST. https://doi.org/10.4108/ICST.SIMUTOOLS2009.5659.\n\n\nFawcett, Chris, Mauro Vallati, Frank Hutter, Jörg Hoffmann, Holger Hoos, and Kevin Leyton-Brown. 2014. “Improved Features for Runtime Prediction of Domain-Independent Planners.” Proceedings of the International Conference on Automated Planning and Scheduling 24 (1): 355–59. https://doi.org/10.1609/icaps.v24i1.13680.\n\n\nFroleyks, Nils, Marijn Heule, Markus Iser, Matti Järvisalo, and Martin Suda. 2021. “SAT Competition 2020.” Artificial Intelligence 301: 103572. https://doi.org/https://doi.org/10.1016/j.artint.2021.103572.\n\n\nGebser, Martin, Roland Kaminski, Benjamin Kaufmann, Torsten Schaub, Marius Thomas Schneider, and Stefan Ziller. 2011. “A Portfolio Solver for Answer Set Programming: Preliminary Report.” In Logic Programming and Nonmonotonic Reasoning, edited by James P. Delgrande and Wolfgang Faber, 352–57. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nGEBSER, MARTIN, MARCO MARATEA, and FRANCESCO RICCA. 2019. “The Seventh Answer Set Programming Competition: Design and Results.” Theory and Practice of Logic Programming 20 (2): 176–204. https://doi.org/10.1017/s1471068419000061.\n\n\nGil, Luı́s, Paulo F. Flores, and Luı́s Miguel Silveira. 2009. “PMSat: a parallel version of MiniSAT.” J. Satisf. Boolean Model. Comput. 6 (1-3): 71–98. https://doi.org/10.3233/SAT190063.\n\n\nGomes, Carla Pedro, and Bart Selman. 1997. “Algorithm Portfolio Design: Theory Vs. Practice.” ArXiv abs/1302.1541. https://api.semanticscholar.org/CorpusID:8512615.\n\n\nGomes, Carla, and Bart Selman. 2001. “Algorithm Portfolios.” Artificial Intelligence 126: 43–62.\n\n\nGonard, François, Marc Schoenauer, and Michèle Sebag. 2019. “Algorithm Selector and Prescheduler in the ICON Challenge.” In Bioinspired Heuristics for Optimization, 203–19. Springer International Publishing.\n\n\nGulati, Kanupriya, Suganth Paul, Sunil P. Khatri, Srinivas Patil, and Abhijit Jas. 2009. “FPGA-based hardware acceleration for Boolean satisfiability.” ACM Trans. Des. Autom. Electron. Syst. 14 (2). https://doi.org/10.1145/1497561.1497576.\n\n\nGuo, Haipeng, and William H. Hsu. 2003. “Algorithm Selection for Sorting and Probabilistic Inference: A Machine Learning-Based Approach.” PhD thesis, USA: Kansas State University.\n\n\n———. 2005. “A Learning-Based Algorithm Selection Meta-Reasoner for the Real-Time MPE Problem.” In AI 2004: Advances in Artificial Intelligence, edited by Geoffrey I. Webb and Xinghuo Yu, 307–18. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-540-30549-1_28.\n\n\nHamadi, Youssef, Said Jabbour, and Lakhdar Sais. 2009. “ManySAT: a Parallel SAT Solver.” Journal on Satisfiability, Boolean Modeling and Computation. https://doi.org/10.3233/sat190070.\n\n\nHölldobler, Steffen, Norbert Manthey, and Ari Saptawijaya. 2010. “Improving Resource-Unaware SAT Solvers.” In Logic for Programming, Artificial Intelligence, and Reasoning, edited by Christian G. Fermüller and Andrei Voronkov, 519–34. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nHölldobler, Steffen, Norbert Manthey, Van Hau Nguyen, Julian Stecklina, and Peter Steinke. 2011. “A short overview on modern parallel SAT-solvers.” ICACSIS 2011 - 2011 International Conference on Advanced Computer Science and Information Systems, Proceedings, no. January: 201–6.\n\n\nHoos, Holger H., Roland Kaminski, Marius Thomas Lindauer, and Torsten Schaub. 2015. “aspeed: Solver scheduling via answer set programming.” TPLP 15 (1): 117–42.\n\n\nHoos, Holger, Marius Lindauer, and Torsten Schaub. 2014. “Claspfolio 2: Advances in Algorithm Selection for Answer Set Programming.” https://arxiv.org/abs/1405.1520.\n\n\nHowe, Adele E, Eric Dahlman, Christopher Hansen, Michael Scheetz, and Anneliese Von Mayrhauser. 2000. “Exploiting Competitive Planner Performance.” In Recent Advances in AI Planning: 5th European Conference on Planning, ECP’99, Durham, UK, September 8-10, 1999. Proceedings 5, 62–72. Springer.\n\n\nHuberman, Bernardo A., Rajan M. Lukose, and Tad Hogg. 1997. “An economics approach to hard computational problems.” Science 275 (5296): 51–54. https://doi.org/10.1126/science.275.5296.51.\n\n\nHutter, Frank, Lars Kotthoff, and Joaquin Vanschoren, eds. 2019. Automated Machine Learning - Methods, Systems, Challenges. Springer.\n\n\nHutter, Frank, Manuel López-Ibáñez, Chris Fawcett, Marius Lindauer, Holger H. Hoos, Kevin Leyton-Brown, and Thomas Stützle. 2014. “AClib: A Benchmark Library for Algorithm Configuration.” In Learning and Intelligent Optimization, edited by Panos M. Pardalos, Mauricio G. C. Resende, Chrysafis Vogiatzis, and Jose L. Walteros, 36–40. Cham: Springer International Publishing.\n\n\nHutter, Frank, Lin Xu, Holger H. Hoos, and Kevin Leyton-Brown. 2014. “Algorithm Runtime Prediction: Methods & Evaluation.” Artificial Intelligence 206: 79–111. https://doi.org/https://doi.org/10.1016/j.artint.2013.10.003.\n\n\nHyvarinen, Antti E. J., and Christoph M. Wintersteiger. 2012. “Approaches for Multi-Core Propagation in Clause Learning Satisfiability Solvers.” MSR-TR-2012-47. https://www.microsoft.com/en-us/research/publication/approaches-for-multi-core-propagation-in-clause-learning-satisfiability-solvers/.\n\n\nHyvärinen, Antti E. J., Tommi Junttila, and Ilkka Niemelä. 2008. “Strategies for Solving SAT in Grids by Randomized Search.” In Intelligent Computer Mathematics, edited by Serge Autexier, John Campbell, Julio Rubio, Volker Sorge, Masakazu Suzuki, and Freek Wiedijk, 125–40. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\n———. 2010. “Partitioning SAT Instances for Distributed Solving.” In Logic for Programming, Artificial Intelligence, and Reasoning, edited by Christian G. Fermüller and Andrei Voronkov, 372–86. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nKadioglu, Serdar, Yuri Malitsky, Ashish Sabharwal, Horst Samulowitz, and Meinolf Sellmann. 2011. “Algorithm selection and scheduling.” Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 6876 LNCS: 454–69. https://doi.org/10.1007/978-3-642-23786-7_35.\n\n\nKadioglu, Serdar, Yuri Malitsky, Meinolf Sellmann, and Kevin Tierney. 2010. “ISAC–Instance-Specific Algorithm Configuration.” In ECAI 2010, 751–56. IOS Press.\n\n\nKadıoglu, Serdar, Yuri Malitsky, and Meinolf Sellmann. n.d. “How to Win Gold at a SAT Competition Without Writing a SAT Solver.”\n\n\nKarp, Richard M. 1972. “Reducibility Among Combinatorial Problems.” In Complexity of Computer Computations: Proceedings of a Symposium on the Complexity of Computer Computations, Held March 20–22, 1972, at the IBM Thomas j. Watson Research Center, Yorktown Heights, New York, and Sponsored by the Office of Naval Research, Mathematics Program, IBM World Trade Corporation, and the IBM Research Mathematical Sciences Department, edited by Raymond E. Miller, James W. Thatcher, and Jean D. Bohlinger, 85–103. Boston, MA: Springer US. https://doi.org/10.1007/978-1-4684-2001-2_9.\n\n\nKerschke, Pascal, Holger H. Hoos, Frank Neumann, and Heike Trautmann. 2019. “Automated Algorithm Selection: Survey and Perspectives.” Evolutionary Computation 27 (1): 3–45. https://doi.org/10.1162/evco_a_00242.\n\n\nKerschke, Pascal, Lars Kotthoff, Jakob Bossek, Holger H. Hoos, and Heike Trautmann. 2018. “Leveraging TSP Solver Complementarity through Machine Learning.” Evolutionary Computation 26 (4): 597–620. https://doi.org/10.1162/evco_a_00215.\n\n\nKocamaz, Uğur Erkin. 2013. “Increasing the Efficiency of Quicksort Using a Neural Network Based Algorithm Selection Model.” Information Sciences 229: 94–105. https://doi.org/https://doi.org/10.1016/j.ins.2012.11.014.\n\n\nKotthoff, Lars. 2014. “Algorithm selection for combinatorial search problems: A survey.” AI Magazine 35 (3): 48–69.\n\n\nKuş, Erdem, Özgür Akgün, Nguyen Dang, and Ian Miguel. 2024. “Frugal Algorithm Selection.” Cornell University. https://doi.org/ 10.48550/arxiv.2405.11059 .\n\n\nLe Frioux, Ludovic. 2019. “Towards more efficient parallel SAT solving.” Theses, Sorbonne Université. https://theses.hal.science/tel-03030122.\n\n\nLeeson, Will, and Matthew B. Dwyer. 2024. “Algorithm Selection for Software Verification Using Graph Neural Networks.” ACM Trans. Softw. Eng. Methodol. 33 (3). https://doi.org/10.1145/3637225.\n\n\nLeyton-Brown, Kevin, Eugene Nudelman, Galen Andrew, Jim McFadden, and Yoav Shoham. 2003. “A portfolio approach to algorithm selection.” In Proceedings of the 18th International Joint Conference on Artificial Intelligence, 1542–43. IJCAI’03. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.\n\n\nLeyton-Brown, Kevin, Eugene Nudelman, and Yoav Shoham. 2002. “Learning the Empirical Hardness of Optimization Problems: The Case of Combinatorial Auctions.” In Principles and Practice of Constraint Programming - CP 2002, edited by Pascal Van Hentenryck, 556–72. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nLieder, Falk, Dillon Plunkett, Jessica B Hamrick, Stuart J Russell, Nicholas Hay, and Tom Griffiths. 2014. “Algorithm Selection by Rational Metareasoning as a Model of Human Strategy Selection.” In Advances in Neural Information Processing Systems, edited by Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger. Vol. 27. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2014/file/7fb8ceb3bd59c7956b1df66729296a4c-Paper.pdf.\n\n\nLindauer, Marius, Rolf-David Bergdoll, and Frank Hutter. 2016. “An Empirical Study of Per-Instance Algorithm Scheduling.” In Proceedings of the Tenth International Conference on Learning and Intelligent Optimization, LION’16, in: Lecture Notes in Computer Science, 253–59. Springer; Springer.\n\n\nLindauer, Marius, Holger H Hoos, Frank Hutter, and Torsten Schaub. 2015. “Autofolio: An Automatically Configured Algorithm Selector.” Journal of Artificial Intelligence Research 53: 745–78.\n\n\nLindauer, Marius, Holger Hoos, and Frank Hutter. 2015. “From sequential algorithm selection to parallel portfolio selection.” In International Conference on Learning and Intelligent Optimization, 1–16. Springer.\n\n\nLindauer, Marius, Jan N. van Rijn, and Lars Kotthoff. 2019. “The algorithm selection competitions 2015 and 2017.” Artificial Intelligence 272: 86–100. https://doi.org/https://doi.org/10.1016/j.artint.2018.10.004.\n\n\nLindauer, T Marius. 2014. “Algorithm selection, scheduling and configuration of Boolean constraint solvers.” PhD thesis, Universität Potsdam.\n\n\nLukac, Martin, and Michitaka Kameyama. 2023. “Verification Based Algorithm Selection.” In 2023 International Conference on Information and Digital Technologies (IDT), 25–30. https://doi.org/10.1109/IDT59031.2023.10194439.\n\n\nMalitsky, Yuri, Barry O’Sullivan, Alessandro Previti, and Joao Marques-Silva. 2014. “A Portfolio Approach to Enumerating Minimal Correction Subsets for Satisfiability Problems.” In, 368–76. Springer, Cham. https://doi.org/ 10.1007/978-3-319-07046-9_26 .\n\n\nMalitsky, Yuri, Ashish Sabharwal, Horst Samulowitz, and Meinolf Sellmann. 2012. “Parallel SAT Solver Selection and Scheduling.” In Proceedings of the 18th International Conference on Principles and Practice of Constraint Programming - Volume 7514, 512–26. Springer-Verlag.\n\n\n———. 2013. “Algorithm portfolios based on cost-sensitive hierarchical clustering.” In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, 608–14. IJCAI ’13. Beijing, China: AAAI Press.\n\n\nManthey, Norbert. 2011. “Parallel SAT solving-using more cores.” Pragmatics of SAT (POS’11).\n\n\nMeyer, Quirin, Fabian Schönfeld, Marc Stamminger, and Rolf Wanka. 2010. “3-SAT on CUDA: Towards a massively parallel SAT solver.” In 2010 International Conference on High Performance Computing & Simulation, 306–13. https://doi.org/10.1109/HPCS.2010.5547116.\n\n\nMusliu, Nysret, and Martin Schwengerer. 2013. “Algorithm Selection for the Graph Coloring Problem.” In Learning and Intelligent Optimization, edited by Giuseppe Nicosia and Panos Pardalos, 389–403. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nNichols, Daniel, Harshitha Menon, Todd Gamblin, and Abhinav Bhatele. 2023. “A Probabilistic Approach To Selecting Build Configurations in Package Managers,” November. https://doi.org/10.2172/2223030.\n\n\nNudelman, Eugene, Kevin Leyton-Brown, Holger H. Hoos, Alex Devkar, and Yoav Shoham. n.d. “Understanding Random SAT: Beyond the Clauses-to-Variables Ratio.” In Principles and Practice of Constraint Programming – CP 2004, edited by Mark Wallace, 438–52. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nO’Mahony, Eoin, Emmanuel Hebrard, Alan Holland, Conor Nugent, and Barry O’Sullivan. 2008. “Using Case-Based Reasoning in an Algorithm Portfolio for Constraint Solving.” In Irish Conference on Artificial Intelligence and Cognitive Science, 210–16. Proceedings of the 19th Irish Conference on Artificial Intelligence; Cognitive Science.\n\n\nOsama, Muhammad, Anton Wijs, and Armin Biere. 2021. “SAT Solving with GPU Accelerated Inprocessing.” In Tools and Algorithms for the Construction and Analysis of Systems, edited by Jan Friso Groote and Kim Guldstrand Larsen, 133–51. Cham: Springer International Publishing.\n\n\n———. 2023. “Certified SAT solving with GPU accelerated inprocessing.” Form. Methods Syst. Des. 62 (1–3): 79–118. https://doi.org/10.1007/s10703-023-00432-z.\n\n\nPearl, Judea. 1984. Heuristics: Intelligent Search Strategies for Computer Problem Solving. USA: Addison-Wesley Longman Publishing Co., Inc.\n\n\nPimpalkhare, Nikhil, Federico Mora, Elizabeth Polgreen, and Sanjit A. Seshia. 2021. “MedleySolver: Online SMT Algorithm Selection.” In Theory and Applications of Satisfiability Testing – SAT 2021, edited by Chu-Min Li and Felip Manyà, 453–70. Cham: Springer International Publishing.\n\n\nPulatov, Damir, Marie Anastacio, Lars Kotthoff, and Holger Hoos. 2022. “Opening the Black Box: Automated Software Analysis for Algorithm Selection.” In Proceedings of the First International Conference on Automated Machine Learning, 188:6/1–18. PMLR. https://proceedings.mlr.press/v188/pulatov22a.html.\n\n\nPulina, Luca, and Martina Seidl. 2019. “The 2016 and 2017 QBF Solvers Evaluations (QBFEVAL’16 and QBFEVAL’17).” Artificial Intelligence 274: 224–48. https://doi.org/https://doi.org/10.1016/j.artint.2019.04.002.\n\n\nRedekopp, M., and A. Dandalis. 2000. “A Parallel Pipelined SAT Solver for FPGA’s.” In Field-Programmable Logic and Applications: The Roadmap to Reconfigurable Computing, edited by Reiner W. Hartenstein and Herbert Grünbacher, 462–68. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nRice, John R. 1976. “The Algorithm Selection Problem.” Advances in Computers. https://doi.org/10.1016/S0065-2458(08)60520-3.\n\n\nRichter, Cedric, Eyke Hüllermeier, Marie-Christine Jakobs, and Heike Wehrheim. 2020. “Algorithm Selection for Software Validation Based on Graph Kernels.” Automated Software Engg. 27 (1–2): 153–86. https://doi.org/10.1007/s10515-020-00270-x.\n\n\nRoberts, Mark, Adele E Howe, Brandon Wilson, and Marie desJardins. 2008. “What Makes Planners Predictable?” In ICAPS, 288–95.\n\n\nRoussel, Olivier. 2012. “Description of Ppfolio (2011).” Proc. SAT Challenge, 46.\n\n\nShavit, Hadar, and Holger H. Hoos. 2024. “Revisiting SATZilla Features in 2024.” In 27th International Conference on Theory and Applications of Satisfiability Testing (SAT 2024), edited by Supratik Chakraborty and Jie-Hong Roland Jiang, 305:27:1–26. Leibniz International Proceedings in Informatics (LIPIcs). Dagstuhl, Germany: Schloss Dagstuhl – Leibniz-Zentrum für Informatik. https://doi.org/10.4230/LIPIcs.SAT.2024.27.\n\n\nSinger, Daniel, and Alain Vagner. 2006. “Parallel Resolution of the Satisfiability Problem (SAT) with OpenMP and MPI.” In Parallel Processing and Applied Mathematics, edited by Roman Wyrzykowski, Jack Dongarra, Norbert Meyer, and Jerzy Waśniewski, 380–88. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nSinz, Carsten, Wolfgang Blochinger, and Wolfgang Küchlin. 2001. “PaSAT — Parallel SAT-Checking with Lemma Exchange: Implementation and Applications.” Electronic Notes in Discrete Mathematics 9: 205–16. https://doi.org/https://doi.org/10.1016/S1571-0653(04)00323-3.\n\n\nSitaru, Ioana, and Madalina Raschip. 2022. “Algorithm Selection for Combinatorial Packing Problems.” In 2022 IEEE Congress on Evolutionary Computation (CEC), 1–8. Padua, Italy: IEEE Press. https://doi.org/10.1109/CEC55065.2022.9870417.\n\n\nSmith-Miles, Kate, and Leo Lopes. 2012. “Measuring Instance Difficulty for Combinatorial Optimization Problems.” Computers & Operations Research 39 (5): 875–89. https://doi.org/https://doi.org/10.1016/j.cor.2011.07.006.\n\n\nSomshubra Majumdar, Kunal Kukreja, Ishaan Jain. 2016. “AdaSort: Adaptive Sorting using Machine Learning.” International Journal of Computer Applications 145 (12): 12–17. https://doi.org/ 10.5120/ijca2016910726 .\n\n\nStuckey, Peter J., Thibaut Feydy, Andreas Schutt, Guido Tack, and Julien Fischer. 2014. “The MiniZinc Challenge 2008–2013.” AI Magazine 35 (2): 55–60. https://doi.org/10.1609/aimag.v35i2.2539.\n\n\nTaitler, Ayal, Ron Alford, Joan Espasa, Gregor Behnke, Daniel Fišer, Michael Gimelfarb, Florian Pommerening, et al. 2024. “The 2023 International Planning Competition.” AI Magazine, 1–17.\n\n\nThornton, Chris, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2013. “Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms.” In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 847–55. KDD ’13. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/2487575.2487629.\n\n\nValenzano, Richard, Hootan Nakhost, Martin Müller, Jonathan Schaeffer, and Nathan Sturtevant. 2012. “ArvandHerd: Parallel Planning with a Portfolio.” In Proceedings of the 20th European Conference on Artificial Intelligence, 786–91. ECAI’12. NLD: IOS Press.\n\n\nVanschoren, Joaquin. 2019. “Meta-Learning.” In Automated Machine Learning: Methods, Systems, Challenges, edited by Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren, 35–61. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-05318-5_2.\n\n\nVisentin, Andrea, Aodh Ó Gallchóir, Jens Kärcher, and Herbert Meyr. 2024. “Explainable Algorithm Selection for the Capacitated Lot Sizing Problem.” In Integration of Constraint Programming, Artificial Intelligence, and Operations Research, edited by Bistra Dilkina, 243–52. Cham: Springer Nature Switzerland.\n\n\nWang, Qiang, Jiawei Jiang, Yongxin Zhao, Weipeng Cao, Chunjiang Wang, and Shengdong Li. 2021. “Algorithm selection for software verification based on adversarial LSTM.” In 2021 7th IEEE Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS), 87–92. https://doi.org/10.1109/BigDataSecurityHPSCIDS52275.2021.00026.\n\n\nWilliamson, David P., and David B. Shmoys. 2011. The Design of Approximation Algorithms. Cambridge University Press.\n\n\nWoeginger, Gerhard J. 2002. “Exact Algorithms for NP-hard Problems: A Survey.” In, 185–207. Springer, Berlin, Heidelberg. https://doi.org/ 10.1007/3-540-36478-1_17 .\n\n\nWolpert, D. H., and W. G. Macready. 1997. “No Free Lunch Theorems for Optimization.” IEEE Transactions on Evolutionary Computation 1 (1): 67–82. https://doi.org/10.1109/4235.585893.\n\n\nWotzlaw, Andreas, Alexander van der Grinten, Ewald Speckenmeyer, and Stefan Porschen. 2012. “pfolioUZK: Solver description.” Balint Et Al.(Balint Et Al., 2012a), 45.\n\n\nXu, Lin, Holger H. Hoos, and Kevin Leyton-Brown. 2010. “Hydra: Automatically Configuring Algorithms for Portfolio-Based Selection.” In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, 210–16. AAAI’10 1. AAAI Press.\n\n\nXu, Lin, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2008. “SATzilla: Portfolio-Based Algorithm Selection for SAT.” J. Artif. Int. Res. 32 (1): 565–606.\n\n\nXu, Lin, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. 2011. “Hydra-MIP: Automated algorithm configuration and selection for mixed integer programming.” In Proceedings of the 18th RCRA Workshop, 16–30.\n\n\nXu, Lin, Frank Hutter, Holger Hoos, and Kevin Leyton-Brown. 2012. “Evaluating Component Solver Contributions to Portfolio-Based Algorithm Selectors.” In Theory and Applications of Satisfiability Testing – SAT 2012, edited by Alessandro Cimatti and Roberto Sebastiani, 228–41. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nZHANG, HANTAO, MARIA PAOLA BONACINA, and JIEH HSIANG. 1996. “PSATO: a Distributed Propositional Prover and its Application to Quasigroup Problems.” Journal of Symbolic Computation 21 (4): 543–60. https://doi.org/https://doi.org/10.1006/jsco.1996.0030.\n\n\nZhong, Peixin, M. Martonosi, P. Ashar, and S. Malik. 1998. “Accelerating Boolean satisfiability with configurable hardware.” In Proceedings. IEEE Symposium on FPGAs for Custom Computing Machines (Cat. No.98TB100251), 186–95. https://doi.org/10.1109/FPGA.1998.707896.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3/IsAlgorithmSelectionWorthItComparingSelectingSingleAlgorithmsandParallelExecution.html",
    "href": "chapters/chapter3/IsAlgorithmSelectionWorthItComparingSelectingSingleAlgorithmsandParallelExecution.html",
    "title": "4  Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution",
    "section": "",
    "text": "4.1 Abstract\nThe material of this chapter is based on the following publication:\nH. Kashgarani and L. Kotthoff, “Is algorithm selection worth it? comparing selecting single algorithms and parallel execution,” in AAAI Workshop on Meta-Learning and MetaDL Challenge, vol. 140 of Proceedings of Machine Learning Research, pp. 58–64, PMLR, 2021.\nThis chapter provides an empirical evaluation of SAT18-EXP solver performance (from the SAT Competition 2018) when running in parallel with other solvers at different levels of parallelism. Using the collected data, we trained two performance models based on the solver’s sequential performance data and instance features. We then performed algorithm selection using these selectors to choose the best-predicted solver for each instance, comparing these results to running multiple solvers in parallel. The findings showed that algorithm selection is superior when many solvers run in parallel. The results in this chapter offer preliminary insights for this dissertation, highlighting the importance of selecting an instance-based subportfolio of solvers since, with fewer solvers, the overhead is minimal. The instance based subportfolio approach will be further developed in Chapter 4 and explored with variations in Chapter 5 and 6.\nFor many practical problems, there is more than one algorithm or approach to solve them. Such algorithms often have complementary performance – where one fails, another performs well, and vice versa. Per-instance algorithm selection leverages this by employing portfolios of complementary algorithms to solve sets of difficult problems, choosing the most appropriate algorithm for each problem instance. However, this requires complex models to effect this selection and introduces overhead to compute the data needed for those models. On the other hand, even basic hardware is more than capable of running several algorithms in parallel. We investigate the tradeoff between selecting a single algorithm and running multiple in parallel and incurring a slowdown because of contention for shared resources. Our results indicate that algorithm selection is worth it, especially for large portfolios.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3/IsAlgorithmSelectionWorthItComparingSelectingSingleAlgorithmsandParallelExecution.html#introduction",
    "href": "chapters/chapter3/IsAlgorithmSelectionWorthItComparingSelectingSingleAlgorithmsandParallelExecution.html#introduction",
    "title": "4  Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution",
    "section": "4.2 Introduction",
    "text": "4.2 Introduction\nThe performance of algorithms can vary significantly on different problem instances and there is no single algorithm that performs well in all cases. We can take advantage of such performance differences and create algorithm portfolios to combine the complementary strengths of different algorithms (C. Gomes and Selman 2001). From this portfolio, we can choose the algorithm with the best performance for each problem instance – this is known as the algorithm selection problem (Rice 1976). This is usually done by using machine learning methods and features extracted from the instances (Kotthoff 2014). Like all machine learning models, such approaches to algorithm selection make mistakes and in some cases choose an algorithm that does not have optimal performance. We can avoid this by exploiting modern multi-core architectures and simply running all algorithms in the portfolio in parallel, see e.g. (Amadini, Gabbrielli, and Mauro 2015a). While in theory optimal in terms of achieved performance, in practice contention for shared resources such as memory and caches reduces overall performance.\nWe present, to the best of our knowledge, the first investigation into the practical implications of running a large number of algorithms in parallel. We show the trade-off between algorithm selection that chooses a single algorithm and exploiting parallel resources and demonstrate that simply running all algorithms in a portfolio in parallel is not a panacea.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3/IsAlgorithmSelectionWorthItComparingSelectingSingleAlgorithmsandParallelExecution.html#background",
    "href": "chapters/chapter3/IsAlgorithmSelectionWorthItComparingSelectingSingleAlgorithmsandParallelExecution.html#background",
    "title": "4  Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution",
    "section": "4.3 Background",
    "text": "4.3 Background\nAlgorithm selection and other portfolio-based approaches have been applied in many areas of AI to improve performance. The first paper to introduce portfolios for solving hard AI problem considered a relatively simple parallel approach that executes all algorithms in the portfolio at the same time and stops them all as soon as the solution has been found by one (Huberman, Lukose, and Hogg 1997). (C. Gomes and Selman 2001; Hamadi, Jabbour, and Sais 2009) evaluate this strategy for stochastic algorithms and demonstrate that the variance of the time required to solve a problem decreases as the number of parallel runs increases.\nThis led to further approaches that take advantage of parallel processing by having several algorithms work independently or in cooperation on a given problem instance. (Yun and Epstein 2012) construct algorithm portfolios for constraint satisfaction problems that are executed in parallel and show performance improvements for up to 16 processors, and (Amadini, Gabbrielli, and Mauro 2015a) propose parallel portfolios with a dynamic schedule for up to 8 cores. Similarly, (Bordeaux, Hamadi, and Samulowitz 2009) show that by splitting the search space into sub-spaces, constraint solving portfolio approaches can take advantage of as many as 128 processors to achieve performance improvements.\nFor the Boolean satisfiability problem (SAT), simple static hand-crafted parallel portfolios have been studied by (Roussel 2012a) and (Wotzlaw et al. 2012) combined with a computed resource allocation for each solver. They employ a fixed selection of SAT solvers with good performance independently in parallel for a given number of cores. (Gagliolo and Schmidhuber 2006) introduce the dynamic algorithm portfolios that run a portfolio of algorithms with different shares of parallel processors along with an online time allocation learning approach. This includes a lifelong-learning approach in which the priority of algorithms is continually updated based on new runtime information. (Petrik and Zilberstein 2006) also propose a method for enhancing the performance of deterministic algorithms by running multiple algorithms in parallel for the same problem instance. (Kadioglu et al. 2011; Malitsky et al. 2012a) propose a more sophisticated approach. They select algorithms through an improved k-nearest-neighbor approach and use both dynamic and static scheduling for multiple algorithms from the portfolio to improve the chance that a particular problem instance will be solved within a time limit.\nSimilarly, (M. Lindauer, Hoos, and Hutter 2015) investigate parallel portfolio selection, and (H. H. Hoos et al. 2015) propose an approach to optimally schedule algorithms from a portfolio using answer set programming, while (Gonard, Schoenauer, and Sebag 2019) take the simpler approach of running a small portfolio of algorithms in parallel for a short amount of time and using algorithm selection to tackle any problem instances that remain unsolved after that. To the best of our knowledge, all previous research has only simulated parallel execution without measuring the actual performance. We investigate the practical ramifications of running more than one algorithm in parallel.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3/IsAlgorithmSelectionWorthItComparingSelectingSingleAlgorithmsandParallelExecution.html#experimental-setup",
    "href": "chapters/chapter3/IsAlgorithmSelectionWorthItComparingSelectingSingleAlgorithmsandParallelExecution.html#experimental-setup",
    "title": "4  Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution",
    "section": "4.4 Experimental Setup",
    "text": "4.4 Experimental Setup\nWe run algorithms sequentially and with varying degrees of parallelism. We build and evaluate algorithm selection models for sequential execution to be able to compare selecting a single algorithm to run multiple in parallel. We measure performance in terms of penalized average runtime with factor 10 (PAR10) and misclassification penalty (MCP). The PAR10 score is the observed performance unless an algorithm timed out on a particular instance, when the timeout multiplied by the penalization factor is assumed as the runtime. The misclassification penalty is the difference between the performance of the algorithm that was run and the optimal algorithm on the same instance, i.e. it is always zero for the optimal algorithm.\nWe compare to the virtual best solver (VBS), which is the optimal algorithm from the portfolio for each problem instance to solve (cumulative misclassification penalty zero), and the single best solver (SBS), which is the algorithm from the portfolio with the best average performance across the entire set of problem instances to solve. The performance of the overhead-free parallel portfolio corresponds to the VBS.\nWe consider algorithms and problem instances from SAT, a popular application area for algorithm selection. We selected all 400 instances from the main track of the SAT Competition 2018 (Heule, Järvisalo, and Suda 2019) and computed their features using the SATzilla feature computation code (Xu et al. 2008). We exclude 19 instances for which we were unable to extract features within two hours of computational time, for a total of 381 problem instances.\nOur solvers also come from the main track of the 2018 SAT competition; we consider all 39 submitted solvers for a total of 14,859 algorithm runs. We use the same time limit as in the SAT competition; 5000 CPU seconds for solving a single instance. However, we allowed 128 GB of RAM; more than five times what was allowed in the competition. During the parallel runs, the total amount of memory is shared among all running algorithms. We run the algorithms sequentially, 10 in parallel, 20 in parallel, 30 in parallel, and 32 in parallel to fully saturate a machine with 32 cores.\nWe leverage the algorithm selection benchmark library ASlib (Bischl, Kerschke, et al. 2016) and the LLAMA algorithm selection toolkit (Kotthoff 2013) for our algorithm selection experiments. We build regression models that predict the performance of each algorithm in the portfolio individually and select the algorithm with the best-predicted performance, and pairwise regression models that predict the performance difference for each pair of algorithms and select the algorithm with the aggregated best performance difference. We removed constant-valued (and therefore irrelevant) instance features and imputed missing feature values as the mean over all non-missing values of the feature.\nFor both regression and pairwise regression approaches, we use random forests as the base machine learning models. We tune their hyperparameters following (Bischl, Kerschke, et al. 2016); we consider values of 10 to 200 for the ntree hyperparameter and 1 to 30 for mtry. We optimize the hyperparameters using random search with 250 iterations and perform a nested cross-validation with 10 external and three internal folds to ensure unbiased performance measurements. All other hyperparameters were left at their default values.\n\n\n\n\n# parallel runs\n# timeouts\n# out of memory errors\n\n\n\n\n1\n6982 (47%)\n0 (0%)\n\n\n10\n10281 (69.19%)\n6 (0.04%)\n\n\n20\n11853 (79.77%)\n20 (0.13%)\n\n\n30\n12590 (84.73%)\n27 (0.18%)\n\n\n32\n12715 (85.57%)\n5 (0.03%)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3/IsAlgorithmSelectionWorthItComparingSelectingSingleAlgorithmsandParallelExecution.html#results",
    "href": "chapters/chapter3/IsAlgorithmSelectionWorthItComparingSelectingSingleAlgorithmsandParallelExecution.html#results",
    "title": "4  Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution",
    "section": "4.5 Results",
    "text": "4.5 Results\n\n\n\n\nmetric\n# parallel runs\nVBS\nSBS\nregression\npairwise regression\n\n\n\n\nPAR10\n1\n9256.089\n17585.66\n13004.31\n12588.44\n\n\n\n10\n13062.16\n27251.34\n19888.25\n20410.18\n\n\n\n20\n17099.23\n33630.54\n25233.38\n24970.07\n\n\n\n30\n19970.29\n36498.11\n28628.15\n27317.29\n\n\n\n32\n21674.1\n37285.5\n29937.23\n28888.24\n\n\nMCP\n1\n0\n1006.738\n441.0526\n379.5133\n\n\n\n10\n0\n1433.268\n684.2932\n733.7837\n\n\n\n20\n0\n1649.426\n811.264\n784.1721\n\n\n\n30\n0\n1645.936\n862.5388\n732.7824\n\n\n\n32\n0\n1556.284\n822.1427\n718.0386\n\n\n\n\n\n\n \n\n\nPerformance in terms of PAR10 score and misclassification penalty for different numbers of algorithms run in parallel. The VBS is the performance of the parallel portfolio; SBS is shown for comparison. The regression and pairwise regression bars show the performance of the respective algorithm selection models. We omit the plot for VBS performance in terms of MCP score as it is always zero by definition.\n\n\nWe first evaluate the effect the number of parallel runs has on what fraction of all algorithm runs is successful. Table [tab:errors] shows the number and percentage of unsuccessful runs at each level of parallelism. With only one algorithm running at a time, 47% of runs fail with a timeout. This increases as more and more algorithms are run in parallel. Similarly, the number of runs that fail because they run out of memory increases, as more and more runs share the same amount of physical memory. This does not significantly affect the results though, as even in the worst-case much less than 1% of the total number of runs is affected. Parallel runs have a much more significant effect on the number of timeouts though – from 47% runs that exceeded the available time when only a single algorithm is running at a time, we see an increase to 85.57% of total runs when 32 algorithms are run in parallel. Altogether, 85.6% of runs either time out or run out of memory when 32 algorithms are running in parallel; a significant increase over running only a single algorithm.\nTable [tab:values] and Figure 1.1 show the performance we observed for all parallelism levels and approaches we consider. The PAR10 score for the VBS increases significantly as we increase the number of algorithms run in parallel; \\(\\approx\\)41% from one to 10 parallel runs. Similarly, the score for the single best solver increases by \\(\\approx\\)55% for the same interval. The PAR10 score is more than twice as high for 32 parallel runs compared to a single run for both VBS and SBS – contention for shared resources has a significant impact on the time it takes to solve a set of instances. A large contributor to the increase in PAR10 score is the increased number of unsuccessful runs because of timeouts or memory outs.\nWe observe a similar decrease in performance as for the VBS and SBS for the algorithm selection approaches as the level of parallelism increases – in fact, we observe even steeper performance losses in the beginning, with \\(\\approx\\)53% performance decrease from one algorithm to 10 for regression models and \\(\\approx\\)62% for pairwise regression models in terms of PAR10. However, we observe a performance increase for both approaches (lower MCP scores) when going from 30 algorithms run in parallel to 32, and a performance increase for pairwise regression model when going from 20 algorithms run in parallel to 30. It is unclear what exactly causes this performance increase; it is likely that the machine learning task that underlies the selection process becomes easier as more algorithms lose competitiveness because of timeouts and memory limits.\n\n\n\nPercentage increase in terms of PAR10 score for running different numbers of algorithms in parallel compared to algorithm selector performance for choosing a single algorithm. For example, an increase of 100% means that running the algorithms in parallel doubles the PAR10 score over selecting a single algorithm.\n\n\nOur results show that algorithm selection for choosing a single algorithm to run can beat parallel execution in practice for a large number of solvers. Figure 1.2 shows that the performance of both the regression and pairwise regression algorithm selection approaches are better than the VBS for any level of parallelism beyond running a single algorithm. Both in terms of PAR10 and MCP, algorithm selection is always better than the single best solver. When using all 32 cores we have available, the VBS becomes more than 66% worse than the regression algorithm selection approach and more than 72% worse than the pairwise regression algorithm selection approach. Even when running only 10 algorithms at the same time (and assuming that we know which 10 of the 39 total algorithms to run to maximize performance), the VBS is more than 0.4% and 3% worse than regression and pairwise regression approaches, respectively.\nWhile the overhead-free parallel portfolio promises optimal performance, in theory, we clearly see that in practice this is not the case – contention for shared resources and physical limits of the machine that is used to run the algorithms has a significant detrimental effect on performance. Even though algorithm selection models are not perfect, they outperform actual parallel portfolios in terms of observed performance even for a relatively small number of algorithms run in parallel.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3/IsAlgorithmSelectionWorthItComparingSelectingSingleAlgorithmsandParallelExecution.html#conclusions-and-future-work",
    "href": "chapters/chapter3/IsAlgorithmSelectionWorthItComparingSelectingSingleAlgorithmsandParallelExecution.html#conclusions-and-future-work",
    "title": "4  Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution",
    "section": "4.6 Conclusions and Future Work",
    "text": "4.6 Conclusions and Future Work\nWe investigated the actual observed performance of parallel portfolios, in contrast to their theoretical performance that is usually used in the literature. We found that running even a relatively small number of algorithms in parallel on the same machine can have a significant negative impact on overall performance. Algorithm selection on the other hand chooses only a single algorithm and is able to achieve better overall performance, even though its predictions are not perfect and it does not always choose the algorithm with the best performance for solving a given problem instance.\nAn obvious avenue for future work is a hybrid approach to what we present here, where instead of a single algorithm several are chosen to run in parallel. Existing literature proposes a multitude of methods for doing so; however, none of these approaches have been evaluated as in the investigation we present here – by actually running more than one algorithm in parallel and observing the performance rather than simulating this based on the performance observed when only a single algorithm is run at a time. In addition, there is scope for developing new approaches for dynamic resource allocation for algorithm selection.\n\n\n\n\n\n\nAigner, Martin, Armin Biere, Christoph M. Kirsch, Aina Niemetz, and Mathias Preiner. 2013a. “Analysis of Portfolio-Style Parallel SAT Solving on Current Multi-Core Architectures.” In POS-13. Fourth Pragmatics of SAT Workshop, a Workshop of the SAT 2013 Conference, July 7, 2013, Helsinki, Finland, edited by Daniel Le Berre, 29:28–40. EPiC Series in Computing. EasyChair. https://doi.org/10.29007/73N4.\n\n\nAigner, Martin, Armin Biere, Christoph M Kirsch, Aina Niemetz, and Mathias Preiner. 2013b. “Analysis of Portfolio-Style Parallel SAT Solving on Current Multi-Core Architectures.” POS@ SAT 29: 28–40.\n\n\nAli, Shawkat, and Kate A Smith. 2006. “On Learning Algorithm Selection for Classification.” Applied Soft Computing 6 (2): 119–38.\n\n\nAmadini, Roberto, Maurizio Gabbrielli, and Jacopo Mauro. 2014. “SUNNY: A Lazy Portfolio Approach for Constraint Solving.” Theory Pract. Log. Program. 14 (4-5): 509–24. https://doi.org/10.1017/S1471068414000179.\n\n\n———. 2015a. “A Multicore Tool for Constraint Solving.” In Proceedings of the 24th International Conference on Artificial Intelligence, 232–38.\n\n\n———. 2015b. “Why CP Portfolio Solvers Are (under)Utilized? Issues and Challenges.” In Logic-Based Program Synthesis and Transformation, edited by Moreno Falaschi, 349–64. Cham: Springer International Publishing.\n\n\n———. 2018. “SUNNY-CP and the MiniZinc Challenge.” Theory and Practice of Logic Programming 18 (1): 81–96. https://doi.org/10.1017/S1471068417000205.\n\n\nArbelaez, Alejandro, Youssef Hamadi, and Michele Sebag. 2009. “Online heuristic selection in constraint programming.”\n\n\nBacchus, Fahiem, Matti Jarvisalo, and Ruben Martins. 2019. MaxSAT Evaluation 2019.\n\n\nBalyo, Tomas, Nils Froleyks, Marijn Heule, Markus Iser, Matti Järvisalo, and Martin Suda, eds. 2021. Proceedings of SAT Competition 2021: Solver and Benchmark Descriptions. Anthology or special issue. Department of Computer Science Report Series b. Department of Computer Science Report Series B. http://hdl.handle.net/10138/333647.\n\n\nBalyo, Tomas, Marijn Heule, Markus Iser, Matti Järvisalo, and Martin Suda, eds. 2023. Proceedings of SAT Competition 2023: Solver, Benchmark and Proof Checker Descriptions. Anthology or special issue. Department of Computer Science, Helsinki Institute for Information Technology, Constraint Reasoning; Optimization research group / Matti Järvisalo.\n\n\nBalyo, Tomáš, Marijn J H Heule, and Matti Järvisalo. 2017. “SAT Competition 2017 Solver and Benchmark Descriptions.” Proceedings of SAT COMPETITION 2017, 14–15.\n\n\nBatista dos Santos, Vânia, and Luiz Henrique de Campos Merschmann. 2020. “Metalearning Applied to Multi-Label Text Classification.” In Proceedings of the XVI Brazilian Symposium on Information Systems. SBSI ’20. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3411564.3411646.\n\n\nBiere, Armin. 2012. “Lingeling and friends entering the SAT challenge 2012.” Department of Computer Science Series of Publications B, January, 33–34.\n\n\n———. 2013. “Lingeling, Plingeling and Treengeling Entering the SAT Competition 2013.” In. https://api.semanticscholar.org/CorpusID:972178.\n\n\nBiere, Armin, Marijn Heule, Hans van Maaren, and Toby Walsh. 2009. “Conflict-Driven Clause Learning Sat Solvers.” Handbook of Satisfiability, Frontiers in Artificial Intelligence and Applications, 131–53.\n\n\n———, eds. 2021. Handbook of Satisfiability - Second Edition. Vol. 336. Frontiers in Artificial Intelligence and Applications. IOS Press. https://doi.org/10.3233/FAIA336.\n\n\nBischl, Bernd, Pascal Kerschke, Lars Kotthoff, Marius Lindauer, Yuri Malitsky, Alexandre Fréchette, Holger Hoos, et al. 2016. “ASlib: A Benchmark Library for Algorithm Selection.” Artificial Intelligence 237: 41–58.\n\n\nBischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. Jones. 2016. “mlr: Machine Learning in R.” Journal of Machine Learning Research 17 (170): 1–5. https://jmlr.org/papers/v17/15-066.html.\n\n\nBischl, Bernd, Raphael Sonabend, Lars Kotthoff, and Michel Lang, eds. 2024. Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com.\n\n\nBishop, Christopher M, and Nasser M Nasrabadi. 2006. Pattern Recognition and Machine Learning. Vol. 4. 4. Springer.\n\n\nBordeaux, Lucas, Youssef Hamadi, and Horst Samulowitz. 2009. “Experiments with Massively Parallel Constraint Solving.” In Proceedings of the 21st International Jont Conference on Artifical Intelligence, 443–48. IJCAI’09. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.\n\n\nBrazdil, P., J. N. van Rijn, C. Soares, and J. Vanschoren. 2022. Metalearning: Applications to Automated Machine Learning and Data Mining. Cognitive Technologies. Springer International Publishing. https://books.google.com/books?id=zDcOzgEACAAJ.\n\n\nBreiman, Leo. 2001. “Random Forests.” Machine Learning 45: 5–32.\n\n\nBrighton, Henry, and Chris Mellish. 2002. “Advances in Instance Selection for Instance-Based Learning Algorithms.” Data Mining and Knowledge Discovery 6: 153–72.\n\n\nCarchrae, Tom, and J. Christopher Beck. 2005. “Applying Machine Learning to Low-Knowledge Control of Optimization Algorithms.” Computational Intelligence 21. https://api.semanticscholar.org/CorpusID:7953876.\n\n\nCárdenas-Montes, Miguel. 2016. “Evaluating the Difficulty of Instances of the Travelling Salesman Problem in the Nearby of the Optimal Solution Based on Random Walk Exploration.” In Hybrid Artificial Intelligent Systems, edited by Francisco Martínez-Álvarez, Alicia Troncoso, Héctor Quintián, and Emilio Corchado, 299–310. Cham: Springer International Publishing.\n\n\nCenamor, Isabel, Tomás De La Rosa, and Fernando Fernández. 2013. “Learning Predictive Models to Configure Planning Portfolios.” In Proceedings of the 4th Workshop on Planning and Learning (ICAPS-PAL 2013), 14–22. Citeseer.\n\n\n———. 2016. “The IBaCoP planning system: instance-based configured portfolios.” J. Artif. Int. Res. 56 (1): 657–91.\n\n\nCenamor, Isabel, Tomás de la Rosa, Fernando Fernández, et al. 2014. “IBACOP and IBACOP2 planner.” IPC 2014 Planner Abstracts, 35–38.\n\n\nCiorba, Florina M., Ali Mohammed, Jonas H. Müller Korndörfer, and Ahmed Eleliemy. 2023. “Automated Scheduling Algorithm Selection in OpenMP.” In 2023 22nd International Symposium on Parallel and Distributed Computing (ISPDC), 106–9. https://doi.org/10.1109/ISPDC59212.2023.00025.\n\n\nCollautti, Marco, Yuri Malitsky, Deepak Mehta, and Barry O’Sullivan. 2013. “SNNAP: Solver-based nearest neighbor for algorithm portfolios.” In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III 13, 435–50. Springer.\n\n\nCollevati, M., Agostino Dovier, and A. Formisano. 2022. “GPU Parallelism for SAT Solving Heuristics.” CEUR-WS.\n\n\nDavis, John D., Zhangxi Tan, Fang Yu, and Lintao Zhang. 2008. “A Practical Reconfigurable Hardware Accelerator for Boolean Satisfiability Solvers.” In 2008 45th ACM/IEEE Design Automation Conference, 780–85. https://doi.org/10.1145/1391469.1391669.\n\n\nDavis, Martin, George Logemann, and Donald Loveland. 1962. “A Machine Program for Theorem-Proving.” Commun. ACM 5 (7): 394–97. https://doi.org/10.1145/368273.368557.\n\n\nEwald, Roland, Adelinde M. Uhrmacher, and Kaustav Saha. 2010. “Data Mining for Simulation Algorithm Selection.” In. ICST. https://doi.org/10.4108/ICST.SIMUTOOLS2009.5659.\n\n\nFawcett, Chris, Mauro Vallati, Frank Hutter, Jörg Hoffmann, Holger Hoos, and Kevin Leyton-Brown. 2014. “Improved Features for Runtime Prediction of Domain-Independent Planners.” Proceedings of the International Conference on Automated Planning and Scheduling 24 (1): 355–59. https://doi.org/10.1609/icaps.v24i1.13680.\n\n\nFroleyks, Nils, Marijn Heule, Markus Iser, Matti Järvisalo, and Martin Suda. 2021. “SAT Competition 2020.” Artificial Intelligence 301: 103572. https://doi.org/https://doi.org/10.1016/j.artint.2021.103572.\n\n\nGagliolo, Matteo, and Jürgen Schmidhuber. 2006. “Dynamic algorithm portfolios.” Annals of Mathematics and Artificial Intelligence 47: 3–4.\n\n\nGebser, Martin, Roland Kaminski, Benjamin Kaufmann, Torsten Schaub, Marius Thomas Schneider, and Stefan Ziller. 2011. “A Portfolio Solver for Answer Set Programming: Preliminary Report.” In Logic Programming and Nonmonotonic Reasoning, edited by James P. Delgrande and Wolfgang Faber, 352–57. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nGEBSER, MARTIN, MARCO MARATEA, and FRANCESCO RICCA. 2019. “The Seventh Answer Set Programming Competition: Design and Results.” Theory and Practice of Logic Programming 20 (2): 176–204. https://doi.org/10.1017/s1471068419000061.\n\n\nGerevini, Alfonso, and Derek Long. 2005. “Plan constraints and preferences in PDDL3.” Technical Report 2005-08-07, Department of Electronics for Automation ….\n\n\nGil, Luı́s, Paulo F. Flores, and Luı́s Miguel Silveira. 2009. “PMSat: a parallel version of MiniSAT.” J. Satisf. Boolean Model. Comput. 6 (1-3): 71–98. https://doi.org/10.3233/SAT190063.\n\n\nGomes, Carla Pedro, and Bart Selman. 1997. “Algorithm Portfolio Design: Theory Vs. Practice.” ArXiv abs/1302.1541. https://api.semanticscholar.org/CorpusID:8512615.\n\n\nGomes, Carla, and Bart Selman. 2001. “Algorithm Portfolios.” Artificial Intelligence 126: 43–62.\n\n\nGonard, François, Marc Schoenauer, and Michèle Sebag. 2019. “Algorithm Selector and Prescheduler in the ICON Challenge.” In Bioinspired Heuristics for Optimization, 203–19. Springer International Publishing.\n\n\nGulati, Kanupriya, Suganth Paul, Sunil P. Khatri, Srinivas Patil, and Abhijit Jas. 2009. “FPGA-based hardware acceleration for Boolean satisfiability.” ACM Trans. Des. Autom. Electron. Syst. 14 (2). https://doi.org/10.1145/1497561.1497576.\n\n\nGuo, Haipeng, and William H. Hsu. 2003. “Algorithm Selection for Sorting and Probabilistic Inference: A Machine Learning-Based Approach.” PhD thesis, USA: Kansas State University.\n\n\n———. 2005. “A Learning-Based Algorithm Selection Meta-Reasoner for the Real-Time MPE Problem.” In AI 2004: Advances in Artificial Intelligence, edited by Geoffrey I. Webb and Xinghuo Yu, 307–18. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-540-30549-1_28.\n\n\nHamadi, Youssef, Said Jabbour, and Lakhdar Sais. 2009. “ManySAT: a Parallel SAT Solver.” Journal on Satisfiability, Boolean Modeling and Computation. https://doi.org/10.3233/sat190070.\n\n\nHeule, Marijn J. H., Matti Järvisalo, and Martin Suda. 2019. “SAT Competition 2018.” Journal on Satisfiability, Boolean Modeling and Computation. https://doi.org/10.3233/sat190120.\n\n\nHölldobler, Steffen, Norbert Manthey, and Ari Saptawijaya. 2010. “Improving Resource-Unaware SAT Solvers.” In Logic for Programming, Artificial Intelligence, and Reasoning, edited by Christian G. Fermüller and Andrei Voronkov, 519–34. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nHölldobler, Steffen, Norbert Manthey, Van Hau Nguyen, Julian Stecklina, and Peter Steinke. 2011. “A short overview on modern parallel SAT-solvers.” ICACSIS 2011 - 2011 International Conference on Advanced Computer Science and Information Systems, Proceedings, no. January: 201–6.\n\n\nHoos, Holger H., Roland Kaminski, Marius Thomas Lindauer, and Torsten Schaub. 2015. “aspeed: Solver scheduling via answer set programming.” TPLP 15 (1): 117–42.\n\n\nHoos, Holger H., and Thomas Stützle. 2005. “Propositional Satisfiability and Constraint Satisfaction.” In Stochastic Local Search, 257–312. The Morgan Kaufmann Series in Artificial Intelligence. San Francisco: Morgan Kaufmann. https://doi.org/https://doi.org/10.1016/B978-155860872-6/50023-8.\n\n\nHoos, Holger, Marius Lindauer, and Torsten Schaub. 2014. “Claspfolio 2: Advances in Algorithm Selection for Answer Set Programming.” https://arxiv.org/abs/1405.1520.\n\n\nHowe, Adele E, Eric Dahlman, Christopher Hansen, Michael Scheetz, and Anneliese Von Mayrhauser. 2000. “Exploiting Competitive Planner Performance.” In Recent Advances in AI Planning: 5th European Conference on Planning, ECP’99, Durham, UK, September 8-10, 1999. Proceedings 5, 62–72. Springer.\n\n\nHu, Mengqi, Teresa Wu, and Jeffery D. Weir. 2012. “An Intelligent Augmentation of Particle Swarm Optimization with Multiple Adaptive Methods.” Information Sciences 213: 68–83. https://doi.org/https://doi.org/10.1016/j.ins.2012.05.020.\n\n\nHuberman, Bernardo A., Rajan M. Lukose, and Tad Hogg. 1997. “An economics approach to hard computational problems.” Science 275 (5296): 51–54. https://doi.org/10.1126/science.275.5296.51.\n\n\nHutter, Frank, Lars Kotthoff, and Joaquin Vanschoren, eds. 2019. Automated Machine Learning - Methods, Systems, Challenges. Springer.\n\n\nHutter, Frank, Manuel López-Ibáñez, Chris Fawcett, Marius Lindauer, Holger H. Hoos, Kevin Leyton-Brown, and Thomas Stützle. 2014. “AClib: A Benchmark Library for Algorithm Configuration.” In Learning and Intelligent Optimization, edited by Panos M. Pardalos, Mauricio G. C. Resende, Chrysafis Vogiatzis, and Jose L. Walteros, 36–40. Cham: Springer International Publishing.\n\n\nHutter, Frank, Lin Xu, Holger H. Hoos, and Kevin Leyton-Brown. 2014. “Algorithm Runtime Prediction: Methods & Evaluation.” Artificial Intelligence 206: 79–111. https://doi.org/https://doi.org/10.1016/j.artint.2013.10.003.\n\n\nHyvarinen, Antti E. J., and Christoph M. Wintersteiger. 2012. “Approaches for Multi-Core Propagation in Clause Learning Satisfiability Solvers.” MSR-TR-2012-47. https://www.microsoft.com/en-us/research/publication/approaches-for-multi-core-propagation-in-clause-learning-satisfiability-solvers/.\n\n\nHyvärinen, Antti E. J., Tommi Junttila, and Ilkka Niemelä. 2008. “Strategies for Solving SAT in Grids by Randomized Search.” In Intelligent Computer Mathematics, edited by Serge Autexier, John Campbell, Julio Rubio, Volker Sorge, Masakazu Suzuki, and Freek Wiedijk, 125–40. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\n———. 2010. “Partitioning SAT Instances for Distributed Solving.” In Logic for Programming, Artificial Intelligence, and Reasoning, edited by Christian G. Fermüller and Andrei Voronkov, 372–86. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nHyvärinen, Antti E. J., and Norbert Manthey. 2012. “Designing Scalable Parallel SAT Solvers.” In Theory and Applications of Satisfiability Testing – SAT 2012, edited by Alessandro Cimatti and Roberto Sebastiani, 214–27. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nKadioglu, Serdar, Yuri Malitsky, Ashish Sabharwal, Horst Samulowitz, and Meinolf Sellmann. 2011. “Algorithm selection and scheduling.” Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 6876 LNCS: 454–69. https://doi.org/10.1007/978-3-642-23786-7_35.\n\n\nKadioglu, Serdar, Yuri Malitsky, Meinolf Sellmann, and Kevin Tierney. 2010. “ISAC–Instance-Specific Algorithm Configuration.” In ECAI 2010, 751–56. IOS Press.\n\n\nKadıoglu, Serdar, Yuri Malitsky, and Meinolf Sellmann. n.d. “How to Win Gold at a SAT Competition Without Writing a SAT Solver.”\n\n\nKarp, Richard M. 1972. “Reducibility Among Combinatorial Problems.” In Complexity of Computer Computations: Proceedings of a Symposium on the Complexity of Computer Computations, Held March 20–22, 1972, at the IBM Thomas j. Watson Research Center, Yorktown Heights, New York, and Sponsored by the Office of Naval Research, Mathematics Program, IBM World Trade Corporation, and the IBM Research Mathematical Sciences Department, edited by Raymond E. Miller, James W. Thatcher, and Jean D. Bohlinger, 85–103. Boston, MA: Springer US. https://doi.org/10.1007/978-1-4684-2001-2_9.\n\n\nKashgarani, Haniye, and Lars Kotthoff. 2021. “Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution.” In AAAI Workshop on Meta-Learning and MetaDL Challenge, 140:58–64. Proceedings of Machine Learning Research. PMLR. https://proceedings.mlr.press/v140/kashgarani21a.html.\n\n\n———. 2023. “Automatic Parallel Portfolio Selection.” In ECAI 2023, 1215–22. IOS Press.\n\n\nKerschke, Pascal, Holger H. Hoos, Frank Neumann, and Heike Trautmann. 2019. “Automated Algorithm Selection: Survey and Perspectives.” Evolutionary Computation 27 (1): 3–45. https://doi.org/10.1162/evco_a_00242.\n\n\nKerschke, Pascal, Lars Kotthoff, Jakob Bossek, Holger H. Hoos, and Heike Trautmann. 2018. “Leveraging TSP Solver Complementarity through Machine Learning.” Evolutionary Computation 26 (4): 597–620. https://doi.org/10.1162/evco_a_00215.\n\n\nKocamaz, Uğur Erkin. 2013. “Increasing the Efficiency of Quicksort Using a Neural Network Based Algorithm Selection Model.” Information Sciences 229: 94–105. https://doi.org/https://doi.org/10.1016/j.ins.2012.11.014.\n\n\nKotthoff, Lars. 2013. “LLAMA: Leveraging Learning to Automatically Manage Algorithms.” CoRR abs/1306.1031. http://arxiv.org/abs/1306.1031.\n\n\n———. 2014. “Algorithm selection for combinatorial search problems: A survey.” AI Magazine 35 (3): 48–69.\n\n\nKullback, S., and R. A. Leibler. 1951. “On Information and Sufficiency.” The Annals of Mathematical Statistics 22 (1): 79–86. http://www.jstor.org/stable/2236703.\n\n\nKuş, Erdem, Özgür Akgün, Nguyen Dang, and Ian Miguel. 2024. “Frugal Algorithm Selection.” Cornell University. https://doi.org/ 10.48550/arxiv.2405.11059 .\n\n\nLe Frioux, Ludovic. 2019. “Towards more efficient parallel SAT solving.” Theses, Sorbonne Université. https://theses.hal.science/tel-03030122.\n\n\nLeeson, Will, and Matthew B. Dwyer. 2024. “Algorithm Selection for Software Verification Using Graph Neural Networks.” ACM Trans. Softw. Eng. Methodol. 33 (3). https://doi.org/10.1145/3637225.\n\n\nLeyton-Brown, Kevin, Eugene Nudelman, Galen Andrew, Jim McFadden, and Yoav Shoham. 2003. “A portfolio approach to algorithm selection.” In Proceedings of the 18th International Joint Conference on Artificial Intelligence, 1542–43. IJCAI’03. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.\n\n\nLeyton-Brown, Kevin, Eugene Nudelman, and Yoav Shoham. 2002. “Learning the Empirical Hardness of Optimization Problems: The Case of Combinatorial Auctions.” In Principles and Practice of Constraint Programming - CP 2002, edited by Pascal Van Hentenryck, 556–72. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nLieder, Falk, Dillon Plunkett, Jessica B Hamrick, Stuart J Russell, Nicholas Hay, and Tom Griffiths. 2014. “Algorithm Selection by Rational Metareasoning as a Model of Human Strategy Selection.” In Advances in Neural Information Processing Systems, edited by Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger. Vol. 27. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2014/file/7fb8ceb3bd59c7956b1df66729296a4c-Paper.pdf.\n\n\nLindauer, Marius, Rolf-David Bergdoll, and Frank Hutter. 2016. “An Empirical Study of Per-Instance Algorithm Scheduling.” In Proceedings of the Tenth International Conference on Learning and Intelligent Optimization, LION’16, in: Lecture Notes in Computer Science, 253–59. Springer; Springer.\n\n\nLindauer, Marius, Holger H Hoos, Frank Hutter, and Torsten Schaub. 2015. “Autofolio: An Automatically Configured Algorithm Selector.” Journal of Artificial Intelligence Research 53: 745–78.\n\n\nLindauer, Marius, Holger Hoos, and Frank Hutter. 2015. “From sequential algorithm selection to parallel portfolio selection.” In International Conference on Learning and Intelligent Optimization, 1–16. Springer.\n\n\nLindauer, Marius, Holger Hoos, Kevin Leyton-Brown, and Torsten Schaub. 2017. “Automatic Construction of Parallel Portfolios via Algorithm Configuration.” Artificial Intelligence 244: 272–90. https://doi.org/https://doi.org/10.1016/j.artint.2016.05.004.\n\n\nLindauer, Marius, Jan N. van Rijn, and Lars Kotthoff. 2017. “Open Algorithm Selection Challenge 2017: Setup and Scenarios.” In Proceedings of the Open Algorithm Selection Challenge, 79:1–7. PMLR. https://proceedings.mlr.press/v79/lindauer17a.html.\n\n\nLindauer, Marius, Jan N. van Rijn, and Lars Kotthoff. 2019. “The algorithm selection competitions 2015 and 2017.” Artificial Intelligence 272: 86–100. https://doi.org/https://doi.org/10.1016/j.artint.2018.10.004.\n\n\nLindauer, T Marius. 2014. “Algorithm selection, scheduling and configuration of Boolean constraint solvers.” PhD thesis, Universität Potsdam.\n\n\nLissovoi, Andrei, Pietro S. Oliveto, and John Alasdair Warwicker. 2019. “On the Time Complexity of Algorithm Selection Hyper-Heuristics for Multimodal Optimisation.” Proceedings of the AAAI Conference on Artificial Intelligence 33 (01): 2322–29. https://doi.org/10.1609/aaai.v33i01.33012322.\n\n\nLiu, Wenwen, Shiu Yin Yuen, and Chi Sung. 2022. “A Generic Method to Compose an Algorithm Portfolio with a Problem Set of Unknown Distribution.” Memetic Computing 14 (September): 1–18. https://doi.org/10.1007/s12293-022-00367-8.\n\n\nLukac, Martin, and Michitaka Kameyama. 2023. “Verification Based Algorithm Selection.” In 2023 International Conference on Information and Digital Technologies (IDT), 25–30. https://doi.org/10.1109/IDT59031.2023.10194439.\n\n\nMalitsky, Yuri, Barry O’Sullivan, Alessandro Previti, and Joao Marques-Silva. 2014. “A Portfolio Approach to Enumerating Minimal Correction Subsets for Satisfiability Problems.” In, 368–76. Springer, Cham. https://doi.org/ 10.1007/978-3-319-07046-9_26 .\n\n\nMalitsky, Yuri, Ashish Sabharwal, Horst Samulowitz, and Meinolf Sellmann. 2012b. “Parallel SAT Solver Selection and Scheduling.” In Proceedings of the 18th International Conference on Principles and Practice of Constraint Programming - Volume 7514, 512–26. Springer-Verlag.\n\n\n———. 2012a. “Parallel SAT solver selection and scheduling.” Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 7514 LNCS: 512–26. https://doi.org/10.1007/978-3-642-33558-7_38.\n\n\n———. 2013. “Algorithm portfolios based on cost-sensitive hierarchical clustering.” In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, 608–14. IJCAI ’13. Beijing, China: AAAI Press.\n\n\nManthey, Norbert. 2011. “Parallel SAT solving-using more cores.” Pragmatics of SAT (POS’11).\n\n\nMaratea, Marco, Luca Pulina, and Francesco Ricca. 2014. “A Multi-Engine Approach to Answer-Set Programming.” Theory and Practice of Logic Programming 14 (6): 841–68.\n\n\nMartins, Ruben, Matti Jarvisalo, and Fahiem Bacchus. 2016. Proceedings of SAT Competition 2016 : Solver and Benchmark Descriptions. University of Helsinki.\n\n\nMaturana, Jorge, Álvaro Fialho, Frédéric Saubion, Marc Schoenauer, Frédéric Lardeux, and Michèle Sebag. 2012. “Adaptive Operator Selection and Management in Evolutionary Algorithms.” In Autonomous Search, 161–89. Springer. https://doi.org/10.1007/978-3-642-21434-9_7.\n\n\nMeyer, Quirin, Fabian Schönfeld, Marc Stamminger, and Rolf Wanka. 2010. “3-SAT on CUDA: Towards a massively parallel SAT solver.” In 2010 International Conference on High Performance Computing & Simulation, 306–13. https://doi.org/10.1109/HPCS.2010.5547116.\n\n\nMusliu, Nysret, and Martin Schwengerer. 2013. “Algorithm Selection for the Graph Coloring Problem.” In Learning and Intelligent Optimization, edited by Giuseppe Nicosia and Panos Pardalos, 389–403. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nNichols, Daniel, Harshitha Menon, Todd Gamblin, and Abhinav Bhatele. 2023. “A Probabilistic Approach To Selecting Build Configurations in Package Managers,” November. https://doi.org/10.2172/2223030.\n\n\nNudelman, Eugene, Kevin Leyton-Brown, Holger H. Hoos, Alex Devkar, and Yoav Shoham. n.d. “Understanding Random SAT: Beyond the Clauses-to-Variables Ratio.” In Principles and Practice of Constraint Programming – CP 2004, edited by Mark Wallace, 438–52. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nO’Mahony, Eoin, Emmanuel Hebrard, Alan Holland, Conor Nugent, and Barry O’Sullivan. 2008. “Using Case-Based Reasoning in an Algorithm Portfolio for Constraint Solving.” In Irish Conference on Artificial Intelligence and Cognitive Science, 210–16. Proceedings of the 19th Irish Conference on Artificial Intelligence; Cognitive Science.\n\n\nOsama, Muhammad, Anton Wijs, and Armin Biere. 2021. “SAT Solving with GPU Accelerated Inprocessing.” In Tools and Algorithms for the Construction and Analysis of Systems, edited by Jan Friso Groote and Kim Guldstrand Larsen, 133–51. Cham: Springer International Publishing.\n\n\n———. 2023. “Certified SAT solving with GPU accelerated inprocessing.” Form. Methods Syst. Des. 62 (1–3): 79–118. https://doi.org/10.1007/s10703-023-00432-z.\n\n\nPearl, Judea. 1984. Heuristics: Intelligent Search Strategies for Computer Problem Solving. USA: Addison-Wesley Longman Publishing Co., Inc.\n\n\nPetrik, Marek, and Shlomo Zilberstein. 2006. “Learning parallel portfolios of algorithms.” Annals of Mathematics and Artificial Intelligence 48 (1-2): 85–106.\n\n\nPimpalkhare, Nikhil, Federico Mora, Elizabeth Polgreen, and Sanjit A. Seshia. 2021. “MedleySolver: Online SMT Algorithm Selection.” In Theory and Applications of Satisfiability Testing – SAT 2021, edited by Chu-Min Li and Felip Manyà, 453–70. Cham: Springer International Publishing.\n\n\nPulatov, Damir, Marie Anastacio, Lars Kotthoff, and Holger Hoos. 2022. “Opening the Black Box: Automated Software Analysis for Algorithm Selection.” In Proceedings of the First International Conference on Automated Machine Learning, 188:6/1–18. PMLR. https://proceedings.mlr.press/v188/pulatov22a.html.\n\n\nPulina, Luca, and Martina Seidl. 2019. “The 2016 and 2017 QBF Solvers Evaluations (QBFEVAL’16 and QBFEVAL’17).” Artificial Intelligence 274: 224–48. https://doi.org/https://doi.org/10.1016/j.artint.2019.04.002.\n\n\nRedekopp, M., and A. Dandalis. 2000. “A Parallel Pipelined SAT Solver for FPGA’s.” In Field-Programmable Logic and Applications: The Roadmap to Reconfigurable Computing, edited by Reiner W. Hartenstein and Herbert Grünbacher, 462–68. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nRice, John R. 1976. “The Algorithm Selection Problem.” Advances in Computers. https://doi.org/10.1016/S0065-2458(08)60520-3.\n\n\nRichter, Cedric, Eyke Hüllermeier, Marie-Christine Jakobs, and Heike Wehrheim. 2020. “Algorithm Selection for Software Validation Based on Graph Kernels.” Automated Software Engg. 27 (1–2): 153–86. https://doi.org/10.1007/s10515-020-00270-x.\n\n\nRoberts, Mark, Adele E Howe, Brandon Wilson, and Marie desJardins. 2008. “What Makes Planners Predictable?” In ICAPS, 288–95.\n\n\nRoussel, Olivier. 2012b. “Description of Ppfolio (2011).” Proc. SAT Challenge, 46.\n\n\n———. 2012a. “Description of Ppfolio (2011).” Proc. SAT Challenge, 46.\n\n\nShavit, Hadar, and Holger H. Hoos. 2024. “Revisiting SATZilla Features in 2024.” In 27th International Conference on Theory and Applications of Satisfiability Testing (SAT 2024), edited by Supratik Chakraborty and Jie-Hong Roland Jiang, 305:27:1–26. Leibniz International Proceedings in Informatics (LIPIcs). Dagstuhl, Germany: Schloss Dagstuhl – Leibniz-Zentrum für Informatik. https://doi.org/10.4230/LIPIcs.SAT.2024.27.\n\n\nSinger, Daniel, and Alain Vagner. 2006. “Parallel Resolution of the Satisfiability Problem (SAT) with OpenMP and MPI.” In Parallel Processing and Applied Mathematics, edited by Roman Wyrzykowski, Jack Dongarra, Norbert Meyer, and Jerzy Waśniewski, 380–88. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nSinz, Carsten, Wolfgang Blochinger, and Wolfgang Küchlin. 2001. “PaSAT — Parallel SAT-Checking with Lemma Exchange: Implementation and Applications.” Electronic Notes in Discrete Mathematics 9: 205–16. https://doi.org/https://doi.org/10.1016/S1571-0653(04)00323-3.\n\n\nSitaru, Ioana, and Madalina Raschip. 2022. “Algorithm Selection for Combinatorial Packing Problems.” In 2022 IEEE Congress on Evolutionary Computation (CEC), 1–8. Padua, Italy: IEEE Press. https://doi.org/10.1109/CEC55065.2022.9870417.\n\n\nSmith-Miles, Kate, and Leo Lopes. 2012. “Measuring Instance Difficulty for Combinatorial Optimization Problems.” Computers & Operations Research 39 (5): 875–89. https://doi.org/https://doi.org/10.1016/j.cor.2011.07.006.\n\n\nSomshubra Majumdar, Kunal Kukreja, Ishaan Jain. 2016. “AdaSort: Adaptive Sorting using Machine Learning.” International Journal of Computer Applications 145 (12): 12–17. https://doi.org/ 10.5120/ijca2016910726 .\n\n\nStuckey, Peter J., Thibaut Feydy, Andreas Schutt, Guido Tack, and Julien Fischer. 2014. “The MiniZinc Challenge 2008–2013.” AI Magazine 35 (2): 55–60. https://doi.org/10.1609/aimag.v35i2.2539.\n\n\nTaitler, Ayal, Ron Alford, Joan Espasa, Gregor Behnke, Daniel Fišer, Michael Gimelfarb, Florian Pommerening, et al. 2024. “The 2023 International Planning Competition.” AI Magazine, 1–17.\n\n\nTange, O. 2011. “GNU Parallel - The Command-Line Power Tool.” The USENIX Magazine 36 (1): 42–47. https://doi.org/http://dx.doi.org/10.5281/zenodo.16303.\n\n\nThornton, Chris, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2013. “Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms.” In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 847–55. KDD ’13. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/2487575.2487629.\n\n\nTorağay, Oğuz, and Shaheen Pouya. 2023. “A Monte Carlo simulation approach to the gap-time relationship in solving scheduling problem.” Journal of Turkish Operations Management 7 (1): 1579–90. https://doi.org/10.56554/jtom.1286288.\n\n\nTornede, Alexander, Lukas Gehring, Tanja Tornede, Marcel Wever, and Eyke Hüllermeier. 2023. “Algorithm selection on a meta level.” Machine Learning 112 (4): 1253–86.\n\n\nValenzano, Richard, Hootan Nakhost, Martin Müller, Jonathan Schaeffer, and Nathan Sturtevant. 2012. “ArvandHerd: Parallel Planning with a Portfolio.” In Proceedings of the 20th European Conference on Artificial Intelligence, 786–91. ECAI’12. NLD: IOS Press.\n\n\nVanschoren, Joaquin. 2019. “Meta-Learning.” In Automated Machine Learning: Methods, Systems, Challenges, edited by Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren, 35–61. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-05318-5_2.\n\n\nVisentin, Andrea, Aodh Ó Gallchóir, Jens Kärcher, and Herbert Meyr. 2024. “Explainable Algorithm Selection for the Capacitated Lot Sizing Problem.” In Integration of Constraint Programming, Artificial Intelligence, and Operations Research, edited by Bistra Dilkina, 243–52. Cham: Springer Nature Switzerland.\n\n\nWager, Stefan, Trevor Hastie, and Bradley Efron. 2014. “Confidence intervals for random forests: The jackknife and the infinitesimal jackknife.” The Journal of Machine Learning Research 15 (1): 1625–51.\n\n\nWang, Qiang, Jiawei Jiang, Yongxin Zhao, Weipeng Cao, Chunjiang Wang, and Shengdong Li. 2021. “Algorithm selection for software verification based on adversarial LSTM.” In 2021 7th IEEE Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS), 87–92. https://doi.org/10.1109/BigDataSecurityHPSCIDS52275.2021.00026.\n\n\nWilliamson, David P., and David B. Shmoys. 2011. The Design of Approximation Algorithms. Cambridge University Press.\n\n\nWoeginger, Gerhard J. 2002. “Exact Algorithms for NP-hard Problems: A Survey.” In, 185–207. Springer, Berlin, Heidelberg. https://doi.org/ 10.1007/3-540-36478-1_17 .\n\n\nWolpert, D. H., and W. G. Macready. 1997. “No Free Lunch Theorems for Optimization.” IEEE Transactions on Evolutionary Computation 1 (1): 67–82. https://doi.org/10.1109/4235.585893.\n\n\nWotzlaw, Andreas, Alexander van der Grinten, Ewald Speckenmeyer, and Stefan Porschen. 2012. “pfolioUZK: Solver description.” Balint Et Al.(Balint Et Al., 2012a), 45.\n\n\nWright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” Journal of Statistical Software 77 (1): 1–17. https://doi.org/10.18637/jss.v077.i01.\n\n\nXu, Lin, Holger H. Hoos, and Kevin Leyton-Brown. 2010. “Hydra: Automatically Configuring Algorithms for Portfolio-Based Selection.” In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, 210–16. AAAI’10 1. AAAI Press.\n\n\nXu, Lin, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2008. “SATzilla: Portfolio-Based Algorithm Selection for SAT.” J. Artif. Int. Res. 32 (1): 565–606.\n\n\nXu, Lin, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. 2011. “Hydra-MIP: Automated algorithm configuration and selection for mixed integer programming.” In Proceedings of the 18th RCRA Workshop, 16–30.\n\n\nXu, Lin, Frank Hutter, Holger Hoos, and Kevin Leyton-Brown. 2012. “Evaluating Component Solver Contributions to Portfolio-Based Algorithm Selectors.” In Theory and Applications of Satisfiability Testing – SAT 2012, edited by Alessandro Cimatti and Roberto Sebastiani, 228–41. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\nYuen, Shiu Yin, Chi Kin Chow, and Xin Zhang. 2013. “Which algorithm should I choose at any point of the search: An evolutionary portfolio approach.” In GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference, 567–74. https://doi.org/10.1145/2463372.2463435.\n\n\nYuen, Shiu Yin, Chi Kin Chow, Xin Zhang, and Yang Lou. 2016. “Which algorithm should I choose: An evolutionary algorithm portfolio approach.” Applied Soft Computing 40: 654–73. https://doi.org/https://doi.org/10.1016/j.asoc.2015.12.021.\n\n\nYuen, Shiu Yin, Yang Lou, and Xin Zhang. 2019. “Selecting Evolutionary Algorithms for Black Box Design Optimization Problems.” Soft Computing 23 (15): 6511–31.\n\n\nYun, Xi, and Susan L. Epstein. 2012. “Learning Algorithm Portfolios for Parallel Execution.” In Revised Selected Papers of the 6th International Conference on Learning and Intelligent Optimization - Volume 7219, 323–38. LION 6. Paris, France: Springer-Verlag.\n\n\nZHANG, HANTAO, MARIA PAOLA BONACINA, and JIEH HSIANG. 1996. “PSATO: a Distributed Propositional Prover and its Application to Quasigroup Problems.” Journal of Symbolic Computation 21 (4): 543–60. https://doi.org/https://doi.org/10.1006/jsco.1996.0030.\n\n\nZhang, Lintao, C. F. Madigan, M. H. Moskewicz, and S. Malik. 2001. “Efficient conflict driven learning in a Boolean satisfiability solver.” In IEEE/ACM International Conference on Computer Aided Design. ICCAD 2001. IEEE/ACM Digest of Technical Papers (Cat. No.01CH37281), 279–85. https://doi.org/10.1109/ICCAD.2001.968634.\n\n\nZhong, Peixin, M. Martonosi, P. Ashar, and S. Malik. 1998. “Accelerating Boolean satisfiability with configurable hardware.” In Proceedings. IEEE Symposium on FPGAs for Custom Computing Machines (Cat. No.98TB100251), 186–95. https://doi.org/10.1109/FPGA.1998.707896.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4/AutomaticParallelPortfolioSelection.html",
    "href": "chapters/chapter4/AutomaticParallelPortfolioSelection.html",
    "title": "5  Automatic Parallel Portfolio Selection",
    "section": "",
    "text": "5.1 Abstract\nA portion of this chapter has been published as: H. Kashgarani, L. Kotthoff, “Automatic Parallel Portfolio Selection,” in ECAI 2023, pp. 1215-1222, IOS Press, 2023.\nThis chapter introduces a hybrid formulation for dynamic algorithm portfolio selection, which is instance-based and aims to mitigate the risk of selecting a single algorithm or running too many solvers in parallel. The published ECAI paper investigates the results for a random forest regression model trained using the MLR package. Here, in addition to presenting those results, we also expand upon them to explore the transition of regression random forests from the MLR package to its updated version, MLR3, in R.\nAlgorithms to solve hard combinatorial problems often exhibit complementary performance, i.e. where one algorithm fails, another shines. Algorithm portfolios and algorithm selection take advantage of this by running all algorithms in parallel or choosing the best one to run on a problem instance. In this chapter, we show that neither of these approaches gives the best possible performance and propose the happy medium of running a subset of all algorithms in parallel. We propose a method to choose this subset automatically for each problem instance, and demonstrate empirical improvements of up to 23% in terms of runtime, 83% in terms of misclassification penalty, and 32% in terms of penalized averaged runtime on scenarios from the ASlib benchmark library. Unlike all other algorithm selection and scheduling approaches in the literature, our performance measures are based on the actual performance for algorithms running in parallel rather than assuming overhead-free parallelization based on sequential performance. Our approach is easy to apply in practice and does not require to solve hard problems to obtain a schedule, unlike other techniques in the literature, while still delivering superior performance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automatic Parallel Portfolio Selection</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4/AutomaticParallelPortfolioSelection.html#introduction",
    "href": "chapters/chapter4/AutomaticParallelPortfolioSelection.html#introduction",
    "title": "5  Automatic Parallel Portfolio Selection",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction\nFor many types of hard combinatorial problems, different algorithms that exhibit complementary performance are available. In these cases, a portfolio of algorithms often achieves better performance than a single one (Huberman, Lukose, and Hogg 1997; Gomes and Selman 2001). The algorithms can be run in parallel, or a single one selected for each problem instance to solve. The so-called Algorithm Selection Problem (Rice 1976) is often solved using machine learning models which, given characteristics of the problem instance to solve, decide which algorithm should be chosen (Kotthoff 2014; Kerschke et al. 2019). The machine learning models built for per-instance algorithm selection are not perfect, like most models. In some cases, they lead to choosing an algorithm that does not provide the best overall performance, resulting in wasted resources.\nRunning all algorithms in parallel avoids this issue, but again wastes resources. Even if the user is only interested in optimizing the elapsed time, i.e. it does not matter how many things are run in parallel, results are sub-optimal as parallel executions compete for shared resources such as caches. With more solvers running in parallel, more runs time out, which results in a large overhead. Even for a relatively small number of parallel runs, this overhead becomes prohibitive, resulting in overall performance worse than using imperfect machine learning models to choose a single algorithm (Kashgarani and Kotthoff 2021).\nIn this chapter, we propose a middle path – select the most promising subset of algorithms to be run in parallel on a single non-distributed computing machine. This mitigates the impact of both imperfect machine learning models and overhead from parallel runs. We formalize the problem of choosing a subset of algorithms from a portfolio, unifying approaches from the literature. We propose a solution to this problem based on the predictions of algorithm performance models and their uncertainties and compare empirically to other approaches from the literature. We trained three algorithm selection performance models and applied the proposed formulation and could demonstrate improvements of up to 83% in terms of misclassification penalty, establishing a new state of the art in per-instance algorithm selection with multiple algorithms. We assume that the algorithms to run are not parallelized themselves, i.e. each algorithm consumes the same computational resources, and we run on a single machine. We do not consider the case of running algorithms in a distributed setting on multiple machines.\n\n5.2.1 Algorithm Selection\nThe performance of algorithms designed to solve NP-complete problems, such as Boolean Satisfiability and the Traveling Salesman Problem, can vary significantly depending on the specific problem being addressed. There is no one algorithm that performs optimally in all circumstances. However, we can take advantage of these performance disparities by creating algorithm portfolios that incorporate the complementing strengths of several algorithms (Gomes and Selman 2001; Huberman, Lukose, and Hogg 1997).\nThe algorithm portfolios proposed in (Gomes and Selman 2001; Huberman, Lukose, and Hogg 1997) run multiple algorithms in parallel, however, they do not measure the actual execution time when running in parallel but simulate parallel execution based on sequential performance. (Kashgarani and Kotthoff 2021) found that the performance of the portfolio can deteriorate substantially when algorithms are executed in parallel, in particular for more than 10 algorithms. (Lindauer et al. 2017) has also determined that running various configurations of an algorithm in parallel can introduce overhead, and this factor should be considered when designing portfolios. Alternatively, we can choose a subset of the best algorithms from the portfolio for a given problem instance to avoid the overhead of running a large number of solvers in parallel. In the case where we choose only a single algorithm, this is known as the algorithm selection problem (Rice 1976). Typically, this is accomplished through the use of machine learning techniques and features derived from the instances (Kotthoff 2014; Kerschke et al. 2019) and algorithms (Pulatov et al. 2022). However, choosing only a single algorithm to run often achieves suboptimal performance because of incorrect choices. This can be addressed through better algorithm selection models; in this chapter, we explore the alternative of choosing more than one algorithm to run in parallel on a single node.\nAlgorithm selection has been applied successfully in many problem domains. Some of the most prominent systems are SATzilla, Hydra, and Autofolio  (Xu et al. 2008; Lindauer et al. 2015; Xu, Hoos, and Leyton-Brown 2010). While these systems focus on SAT, algorithm selection also has been used in the constraint programming and mixed integer programming domains, where it has been shown to achieve good performance (O’Mahony et al. 2008; Xu et al. 2011). AutoFolio has been applied in additional areas, e.g. ASP, MAXSAT, and QBF. (Kerschke et al. 2018) apply algorithm selection for the TSP, and ME-ASP (Maratea, Pulina, and Ricca 2014) apply algorithm selection for answer set programming to create a multi-engine solver. Algorithm selection has also been used to choose between evolutionary algorithms (Hu, Wu, and Weir 2012; Yuen, Lou, and Zhang 2019; Maturana et al. 2012; Yuen, Chow, and Zhang 2013). The ASlib benchmarking library (Bischl, Kerschke, et al. 2016) collects benchmarks from many different problem domains and is the de facto standard library for evaluating algorithm selection approaches.\nNotation. We follow (Lindauer, Rijn, and Kotthoff 2017) in the notation we use in this chapter. Given a portfolio of algorithms (solvers) \\(S\\), a set of instances \\(I\\), and a performance metric $ m: S I \\(, we aim to find a mapping\\) s: I S $ from instances \\(I\\) to algorithms \\(S\\) such that the performance across all instances is optimized. This performance metric can be for example the time needed to solve the instance and we assume w.l.o.g. that the performance metric should be minimized. In practice, we estimate the value of the performance metric based on the predictions of machine learning models for new problem instances; we denote this estimate \\(\\hat{m}\\). We want to select the solver with the best-predicted performance for each instance:\n\\[\\label{eq:1}\n    min \\frac{1}{|I|} \\sum\\limits_{i\\in I} \\hat{m}(s(i),i)\\]\n\n\n5.2.2 Portfolio Scheduling\nDifferent approaches have been proposed for sub-portfolio selection. Some approaches choose a number of suitable solvers for sequential execution and assign time slices that sum to the total available time to each algorithm. Others have implemented parallel execution of the selected solvers, while a few have combined these two methods, utilizing parallelization across computing processors and splitting the available time of each processor across different algorithms.\n\n5.2.2.1 Time slice allocation on single processor\nTypically, sub-portfolio selection strategies are built for sequential solver runs, e.g. Sunny (Amadini, Gabbrielli, and Mauro 2014) creates a sub-portfolio of solvers using k-nearest neighbor (kNN) models and builds a sequential schedule for the selected solvers by allocating time slices based on the predicted performance. CPHydra (O’Mahony et al. 2008) also employs case-based reasoning and allocates time slices for the selected CSP solvers to run sequentially. 3S (Kadioglu et al. 2011) dynamically selects and sequentially schedules solvers for a given SAT instance using integer programming. ASPEED (Hoos et al. 2015) creates static sequential schedules through answer set programming that optimizes a static sequential time budget allocation for solvers. Depending on the number of algorithms, solving the scheduling problem can take substantial time. Building on the methodologies of 3S (Kadioglu et al. 2011) and ASPEED (Hoos et al. 2015), ISA (Instance-Specific ASPEED) (Lindauer, Bergdoll, and Hutter 2016) uses kNN to identify the closest training problem instances to a given problem instance to solve. It then employs ASPEED to determine a schedule that minimizes the number of timeouts across these instances. (Lindauer, Bergdoll, and Hutter 2016) also introduced TSunny which is a modified version of Sunny that limits the number of solvers to run, thus increasing the chance of success by allocating larger time slices to each algorithm.\n\n\n5.2.2.2 Time slice allocation on multiple processors\nOther portfolio techniques have focused on scheduling solvers to run in parallel and allocating time slots on different processors. P3S (Malitsky et al. 2012) is a parallel version of 3S and uses the kNN algorithm for selecting solvers and scheduling them using integer programming with a specific runtime allocation strategy, where it runs a static set of solvers for the first 10% of the available runtime and solvers selected for the instance for the remaining 90%. ASPEED (Hoos et al. 2015) can also define a fixed schedule for running solvers on multiple processors, which is chosen based on the average solver performance across a set of instances. Flexfolio (Lindauer, Bergdoll, and Hutter 2016) incorporates a reimplementation of the P3S approach utilizing the same 10-90 strategy. However, rather than employing integer programming to address the scheduling problem, Flexfolio makes use of ASPEED and solves it through answer set programming. Sunny-cp (Amadini, Gabbrielli, and Mauro 2015) can simultaneously execute multiple CSP and COP solvers. First, a portion of the total available time is allocated to a pre-solving phase that follows a fixed schedule. The remaining time is then distributed amongst the other selected solvers dynamically, based on the predictions of kNN performance models. As there are often more solvers than processors, all processors except one are assigned to the corresponding number of top-ranked solvers, while the time on the final processors is split among the remaining solvers.\n\n\n5.2.2.3 Running algorithms in parallel\nOne of the first parallel SAT solvers is ppfolio (Roussel 2012). It selects solver portfolios to solve sets of problem instances optimally, but does this only for entire sets of instances, not on a per-instance basis as we do here. The success of ppfolio has inspired many other researchers to create sub-portfolios of solvers to run in parallel. For example, (Lindauer, Hoos, and Hutter 2015) extended existing algorithm selectors like 3S, SATzilla, and ME-ASP to greedily choose the top \\(n\\) solvers to run in parallel by producing a ranking of candidate algorithms; however, the number of solvers has to be specified by the user and the actual runtime of parallel runs is not considered – the same runtime as for sequential execution is assumed. Running algorithms in parallel on the same machine is slower than running sequentially in practice due to the overhead incurred because of shared caches and shared memory. This has been shown in experiments (Kashgarani and Kotthoff 2021), simulations (Yun and Epstein 2012; Torağay and Pouya 2023), and analyses (Aigner et al. 2013) for the parallel executions of solvers – in practice, ignoring the overhead that parallel execution introduces reduces overall performance.\nIn this chapter, we consider the problem of selecting the optimal subset of algorithms to run in parallel on a single machine. This is computationally much easier to solve on a per-instance basis than more complex scheduling approaches, e.g. the ones used by ASPEED and 3S, which means that our method is easier to deploy and introduces less overhead. As long as the best solver is part of the selected portfolio, we will achieve optimal performance or close to it, whereas approaches that allocate time slices may choose the best solver, but fail to achieve optimal performance if too little time is allocated to it. We leverage more information from algorithm selection models than most approaches in the literature, in particular the uncertainty of performance prediction. This allows us to trade off the number of algorithms to choose with the chance of success in a principled way. Our approach is designed to optimize the usage of parallel computational resources when solving combinatorial problems while taking into account the overhead that arises from parallel runs. To the best of our knowledge, there are no other approaches that solve parallel algorithm selection this way.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automatic Parallel Portfolio Selection</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4/AutomaticParallelPortfolioSelection.html#parallel-portfolio-selection",
    "href": "chapters/chapter4/AutomaticParallelPortfolioSelection.html#parallel-portfolio-selection",
    "title": "5  Automatic Parallel Portfolio Selection",
    "section": "5.3 Parallel Portfolio Selection",
    "text": "5.3 Parallel Portfolio Selection\nWe aim to choose a sub-portfolio of solvers $ P_i S $ for a given instance \\(i \\in I\\) that includes the algorithms with the best performance on \\(i\\) ($ A S $ and $ A P_i $) to run in parallel, based on the predicted performance of each solver. Given the predicted performance metric \\(\\hat{m}\\), we can define a total order of the algorithms in the portfolio \\(S\\) for a given instance \\(i\\). This total order is induced by the ranking of the algorithms based on their predicted performance for instance \\(i\\). Formally, the total order can be defined as: \\[\\label{eq:3}\nA &lt; B \\quad \\text{if} \\quad \\hat{m}(A,i) &lt; \\hat{m}(B,i); A,B \\in S\\]\nGiven the total order, the rank of each algorithm \\(A\\) on each instance \\(i\\) can be defined as the number of algorithms that are predicted to be strictly better than \\(A\\) for the instance and denoted \\(r_{A,i}\\). Ties are broken arbitrarily. A portfolio of a specified size \\(n\\) is then defined as the top \\(n\\) algorithms according to rank for that particular instance. The portfolio of size \\(n\\) for instance \\(i\\) can be expressed mathematically as: \\[\\label{eq:4}\nP_i = \\{A \\in S \\: | \\: r_{A,i} \\leq n\\}\\]\nIn a slight abuse of notation, we will denote the rank of an algorithm as a subscript, i.e. \\(r_{A,i}\\) is the rank of algorithm \\(A\\) on instance \\(i\\) and \\(A_{1,i}\\) is the algorithm of rank \\(1\\) (the best performing algorithm) on instance \\(i\\).\nThis allows to choose a subset of algorithms with the best predicted performance for a given instance, which can then be executed in parallel. However, determining the portfolio size \\(n\\) for a given problem instance is the key challenge for parallel portfolios. As discussed above, choosing only a single algorithm or all algorithms is unlikely to give optimal performance in practice. The larger the number of algorithms we include, the larger the chance that the best algorithm is in the chosen portfolio, but also the larger the overhead from running many algorithms in parallel.\nHere, we want to include the algorithms that, according to their predicted performance on a new problem instance, have the highest chances of achieving optimal performance, while also taking into account the computational overhead of running multiple solvers in parallel. We leverage the uncertainty of the predictions of the performance models to gauge the likelihood that a given algorithm would be competitive. To the best of our knowledge, there are no algorithm selection approaches that do this.\nInstead of considering only a point prediction, we consider the predicted distribution of performance metric values, characterized by its mean and standard deviation. Formally, we denote the standard deviation of the prediction \\(\\hat{m}(A, i)\\) as \\(\\sigma_{A, i}\\) for each solver \\(A\\) and instance \\(i\\). We assume that the predictions of our performance models follow a normal distribution, i.e. the predicted value is the mean of that distribution and allows to characterize it completely together with the standard deviation. We assess the likelihood of two algorithms performing equally well by computing the overlap between their distributions. If two algorithms are predicted to perform very similarly, then the overlap between the distributions will be very large.\nWe are in particular interested in the predicted performance distribution of the best-predicted algorithm \\(A_{1,i}\\) (no algorithms are predicted to perform better than it), and how the predictions for the other algorithms compare to it. Formally, for the best predicted solver \\(A_{1,i}\\) on instance \\(i\\) the distribution of predictions is $ (A_{1,i}, i) ({A{1,i},i}, ^2_{A_{1,i},i}) $ with probability density function $ f_{A_{1,i},i}$ and cumulative distribution function \\(F_{A_{1,i},i}\\). The performance distributions for other algorithms are defined similarly.\nFor the distributions of the predicted performance of two algorithms \\(A_x\\) and \\(A_y\\) on instance \\(i\\), the point of intersection \\(c\\) can be computed as \\(f_{A_x,i}(c) =  f_{A_y,i}(c)\\). That is, the predicted probability of achieving this particular performance is equal for both distributions (illustrated in Figure 1.1). For \\(\\mu_{A_x,i} &lt; \\mu_{A_y,i}\\), \\(c\\) is defined as (we omit the index \\(i\\) for the sake of brevity here):\n\\[\\label{eq:5}\n{c = \\frac{\\mu _{A_y} \\sigma _{A_x}^2-\\sigma _{A_y} \\left(\\mu _{A_x} \\sigma _{A_y}+\\sigma _{A_x} \\sqrt{\\left(\\mu _{A_x}-\\mu _{A_y}\\right){}^2+2 \\left(\\sigma _{A_x}^2-\\sigma _{A_y}^2\\right) \\log \\left(\\frac{\\sigma _{A_x}}{\\sigma _{A_y}}\\right)}\\right)}{\\sigma _{A_x}^2-\\sigma _{A_y}^2}}\\]\nGiven \\(c\\), the overlap between the distributions is defined as the joint probability of \\(A_x\\) performing worse than \\(c\\) and \\(A_y\\) performing better than \\(c\\):\n\\[\\label{eq:6}\n    p(\\hat{m}({A_{x,i}}, i) \\geq c) \\cdot p(\\hat{m}({A_{y,i}}, i) \\leq c) = 1 - F_{A_{x,i},i}(c) +  F_{A_{y,i},i}(c)\\]\nWe define \\(p_{\\cap} \\in [0,1]\\) as a threshold for the computed joint probability to include a given algorithm:\n\\[\\label{eq:7}\nP_i = \\{A \\:| \\: \\left(p(\\hat{m}({A_{1,i},i)} \\geq c) \\: \\cdot \\: p(\\hat{m}({A_{x,i}}, i) \\leq c)\\right) \\geq \\:p_{\\cap}\\:\\}\\]\n\\(p_{\\cap}\\) is 1 for the best predicted algorithm, and 0 for algorithms whose distribution does not have any overlap with that of the best predicted algorithm, i.e. the probability of performing at least as good as the best predicted algorithm is 0.\nWe can control the size of the parallel portfolio by adjusting the value of \\(p_{\\cap}\\). If \\(p_{\\cap}\\) is set to 1, only the best predicted algorithm and ones that are predicted to perform exactly like it are included. On the other hand, if \\(p_{\\cap}\\) is set to 0, all algorithms will be included. This allows us to tune our approach to a given algorithm selection scenario and choose the algorithms to run in parallel very flexibly, also accommodating potentially inaccurate performance predictions.\n\n\n\nOverlapping area of two normal distributions. The point (c) is the performance both distributions are equally likely to achieve. The shaded area denotes the probability of overlap between the two distributions; in our case, the probability that the candidate solver will perform as least as well as the best predicted solver.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automatic Parallel Portfolio Selection</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4/AutomaticParallelPortfolioSelection.html#experimental-setup",
    "href": "chapters/chapter4/AutomaticParallelPortfolioSelection.html#experimental-setup",
    "title": "5  Automatic Parallel Portfolio Selection",
    "section": "5.4 Experimental Setup",
    "text": "5.4 Experimental Setup\n\n5.4.1 Data Collection\nWe used three scenarios from the ASlib benchmark repository  (Bischl, Kerschke, et al. 2016): MAXSAT19-UCMS, SAT11-INDU, and SAT18-EXP. Additionally, we created two new scenarios: SAT16-MAIN, which utilizes solvers and instances from the SAT Competition 2016, and IPC2018, which incorporates solvers and instances from the International Planning Competition 2018. As ASlib only offers algorithm performance data for single runs, we conducted our own measurements for parallel runs on individual machines. We also measured the performance for single runs again and repeated the instance feature extraction steps to ensure that all experiments were performed on the same hardware. For MAXSAT19-UCMS, SAT11-INDU1, SAT16-MAIN, and SAT18-EXP, we used SATZilla’s feature computation code (Xu et al. 2008), and extracted 54 different features. For IPC2018 we used the feature extraction code by (Fawcett et al. 2014) which extracts 305 features for planning problems in PDDL format. We excluded 26 instances of SAT Competition 2016 from the SAT16-MAIN scenario because we were unable to extract features within two hours of computational time. We also omitted two solvers, glocusePLE and Scavel_SAT, from SAT16-MAIN because of frequent out-of-memory errors on multiple instances. From IPC2018, we omitted three solvers, MSP, maplan-1, and maplan-2, because they require an unavailable version of CPLEX. Table 1.1 gives an overview of the scenarios, algorithms, instances, and features we use in our evaluation.\n1 For SAT11-INDU, the ASlib benchmark repository contains 115 extracted features, including those from SATZilla. However, we were unable to find the feature extraction for this scenario and used the same 54 instance features extracted by SATZilla.\n\nNumber of Algorithms, Instances, and Features Across All Scenarios.\n\n\nScenario\nAlgorithms\nInstances\nInstance Features\n\n\n\n\n\nIPC2018\n15\n240\n305\n\n\n\nMAXSAT19-UCMS\n7\n572\n54\n\n\n\nSAT11-INDU\n14\n300\n54\n\n\n\nSAT16-MAIN\n25\n274\n54\n\n\n\nSAT18-EXP\n37\n353\n54\n\n\n\n\n\nWe ran all solvers on all instances on compute nodes with 32 processors and 40 MB cache size (Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz), 128 GB memory, and Red Hat Linux version 8.6. We use the same time limits as in the ASlib scenarios; 5000 CPU seconds for SAT18-EXP and SAT11-INDU, and 3600 CPU seconds for MAXSAT19-UCMS. For the new scenarios, we use the same time limits as the respective competitions; 5000 CPU seconds for SAT16-MAIN and 1800 CPU seconds for IPC2018. We ran each algorithm individually with 2-10 parallel runs. For all experiments, we ensured that only the given number of parallel runs were executed on a single machine. As our previous work showed that performance becomes worse than algorithm selection of a single solver for more than 10 parallel runs (Kashgarani and Kotthoff 2021), we did not evaluate more than 10 parallel runs.\n\n\n5.4.2 Training and Tuning\nIn the paper  (Kashgarani and Kotthoff 2023), we first built random forest regression models to predict the performance of an algorithm on an instance using LLAMA (Kotthoff 2013) and MLR (Bischl, Lang, et al. 2016). To expand on the results of the paper, in addition to the initial performance model, we trained two additional algorithm selection performance models using the random forest implementation in the MLR3 package. We compared these with the previous randomForest model trained with the MLR package in R.\nMLR is an R package that unifies the available implementations of machine learning algorithms in R (Bischl, Lang, et al. 2016). In MLR, the Random Forests learner uses the randomForest package in R (Breiman 2001) as a dependency. The MLR package extends this algorithm by providing one method to estimate the uncertainties of the predictions, which is the Jackknife (Wager, Hastie, and Efron 2014) technique. MLR3 (Bischl et al. 2024), on the other hand, is the latest version of the MLR release, offering enhanced features. Some learners differ between MLR and MLR3. In MLR, training random forests uses the implementation of the randomForest R package, while MLR3 replaces this with the Ranger R package. According to its documentation, Ranger is designed to be a fast implementation of random forests, particularly for high-dimensional data (Wright and Ziegler 2017). In Ranger’s paper (Wright and Ziegler 2017) it outperforms other implementations, including randomForest, in terms of runtime and memory usage, especially as the number of trees, features, and sample sizes increases, and Ranger scaled almost linearly with the number of samples, while randomForest scaled superlinearly (Wright and Ziegler 2017).\nOur MLR regression random forest models predict the runtime for each solver as the mean of the underlying distribution, and estimate the standard deviation using the Jackknife method (Wager, Hastie, and Efron 2014; Bischl, Lang, et al. 2016), which calculates the standard deviation of the mean predictions over all observations used to train the random forest. The random forest is trained on \\(n-1\\) observations and makes a prediction for the remaining observation. This process is repeated for all observations. The mean prediction for each tree is determined by averaging its predictions for the left-out observations. The Jackknife method assumes that the distribution of the predictions is normal, and their standard deviation is the uncertainty of the overall prediction.\nRanger allows two different methods to estimate prediction uncertainties (Wager, Hastie, and Efron 2014): the Jackknife (also known as the jackknife-after-bootstrap) and the infinitesimal Jackknife (also known as the infinitesimal-jackknife-for-bagging). We built random forest regression models using the Ranger package twice: one model with the Jackknife method (RJ) to estimate prediction uncertainty and the other with the Infinitesimal Jackknife method (RI).\nThe Jackknife and infinitesimal Jackknife methods both estimate the standard deviation, but differ in approach and efficiency according to (Wager, Hastie, and Efron 2014). The jackknife removes one observation at a time to assess the impact, while the infinitesimal jackknife downweights each observation by an infinitesimal amount. So, the infinitesimal Jackknife method downweights each observation by a very small amount. This approach often leads to more stable predictions and is more computationally efficient, as it requires fewer bootstrap replicates for similar accuracy (Wager, Hastie, and Efron 2014).\nRandom forests usually result in the best algorithm selection performance and performance predictions (Bischl, Kerschke, et al. 2016; Hutter et al. 2014). Our setup mirrors that of (Bischl, Kerschke, et al. 2016): we removed constant-valued instance features and imputed missing feature values with the mean of all non-missing values for that feature. The hyperparameters of the random forest models were tuned using random search with 250 iterations, with \\(ntree\\) ranging from 10 to 200 and \\(mtry\\) from 1 to 30 in a nested cross-validation with three inner folds and 10 outer folds (Bischl, Kerschke, et al. 2016).\nSince the available version of LLAMA (Kotthoff 2013) was only adapted to MLR, we ported the LLAMA package to MLR3 to conduct experiments and compare different implementations2. This update will be available to the community in the near future as CRAN R package.\n2 https://github.com/uwyo-mallet/llama-mlr3To determine the optimal value of \\(p_{\\cap}\\) in Equation [eq:6] for each scenario, we perform a grid search in the \\([0, 1)\\) interval with a resolution of \\(0.01\\) for a total of 100 values. Additionally, we determine the overall optimal value of \\(p_{\\cap}\\) across all five scenarios.\nWe evaluate the proposed approach using penalized average runtime with a factor of 10 (PAR10), misclassification penalty (MCP), runtime. The PAR10 score is equal to the actual runtime when the algorithm succeeds in solving the instance within the timeout, otherwise, it is the timeout times 10. The misclassification penalty is the difference between the performance of the selected algorithm and the performance of the optimal algorithm. We report the mean and standard deviation of these values in Tables 1.3, 1.4, and 1.4.\nWe also measure the PAR10 score normalized gap closed between the sequential single best solver and the sequential virtual best solver. In Tables 1.3, 1.4, and 1.4, we report the mean and standard deviation of the normalized gap closed across the 10 folds of data used to train the performance models. In contrast to the reported runtime, MCP and PAR10 scores, in these tables, we do not report the mean and standard deviation in the distribution of all instances. Instead, we use folds because, based on the normalized gap closed formula \\(\\frac{\\text{sbs} - \\text{approach}}{\\text{sbs} - \\text{vbs}}\\), we aimed to avoid zero denominators in cases where the single best solver is the actual best solver for an instance. The plots 1.3, show the PAR10 score normalized gap closed over the entire distribution of instances.\n\n\n5.4.3 Baselines\nWe compare the performance of our approach to several baseline methods, in particular the sequential virtual best solver (VBS), which is the optimal algorithm from the portfolio per problem instance (with a cumulative misclassification penalty of zero) and the sequential single best solver (SBS), which is the algorithm from the portfolio with the best average performance across all problem instances. The VBS for parallel runs is the best solver for each instance, but including the overhead for \\(n\\) parallel runs. The parallel SBS is computed similarly, with the best solvers on average instead of the best on each instance. We run multiple solvers in parallel to measure the actual runtime of the best solver in this case, rather than assuming the sequential runtime.\nWe further compare to per-instance algorithm selection that simply runs the top \\(n\\) predicted algorithms in parallel without considering the overlap of the distributions of the performance predictions, with the same performance models we use for our approach. In the notation we introduced above, we set \\(p_{\\cap}=0\\) and cap the number of runs at the number of available processors. We use a simple scheduling method as a further baseline, where algorithms are scheduled according to their predicted rank and allocated a time slice equal to the predicted performance plus the standard deviation. This allows to run more than one algorithm per processor. This approach prioritizes the best-predicted algorithms but also potentially allows other algorithms to run.\nASPEED (Hoos et al. 2015) provides a general schedule for all instances in a given scenario, rather than a schedule for each instance individually. Therefore, we do not include ASPEED in our experimental evaluation – static schedules across large sets of problem instances do not achieve competitive performance, as shown in (Lindauer, Bergdoll, and Hutter 2016). The Flexfolio paper (Lindauer, Bergdoll, and Hutter 2016) shows experiments for Instance-Specific ASPEED and TSunny, but the available source code does not contain these algorithm selection methods and we are unable to compare to them.\nFinally, we compare our approach to 3S as implemented in Flexfolio (Lindauer, Bergdoll, and Hutter 2016), as the original 3S implementation is unavailable. In this implementation, the number of neighbors for the kNN models was set to 32, and ASPEED (Hoos et al. 2015) is used to schedule the chosen solvers instead of the original integer programming scheduler.\nWe normalize all performances across scenarios by the performances of the VBS and SBS and report the fraction of the gap between them that was closed by a particular approach. On this normalized scale, 0 corresponds to the performance of the SBS and 1 to the performance of the VBS. All code and data are available at https://github.com/uwyo-mallet/auto-parallel-portfolio-selection.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automatic Parallel Portfolio Selection</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4/AutomaticParallelPortfolioSelection.html#results",
    "href": "chapters/chapter4/AutomaticParallelPortfolioSelection.html#results",
    "title": "5  Automatic Parallel Portfolio Selection",
    "section": "5.5 Results",
    "text": "5.5 Results\n\n5.5.1 Tuning of \\(p_{\\cap}\\)\n\n\n\n\n\n\n\nSensitivity of portfolio performance to (p_{}). The top-left plot refers to the RFJ model—Regression Random Forest model using MLR with the Jackknife uncertainty estimation method. The top-right plot refers to the RI model—Regression Ranger model with the Infinitesimal Jackknife uncertainty estimation method. The bottom plot refers to the RJ model—Regression Ranger model with the Jackknife uncertainty estimation method. The plot illustrates the mean, Q1 (25th percentile), Q2 (50th percentile), and Q3 (75th percentile) runtime performance of each scenario for various values of (p_{}) as defined in Equation [eq:7]. Note the log scale for the normalized gap closed.\n\n\nThe tuning of \\(p_{\\cap}\\) shows that the optimal value depends on the scenario. For the IPC2018 scenario, the ideal \\(p_{\\cap}\\) value is 0.59, for the MAXSAT19-UCMS scenario 0.55, for SAT11-INDU 0.63, for SAT16-MAIN 0.33, and for SAT18-EXP 0.81 for the random forest model trained using the MLR and Jackknife uncertainty estimation method (RFJ). The optimal \\(p_{\\cap}\\) values for the Ranger models, one with Jackknife (RJ) and the other with Infinitesimal Jackknife (RI), are provided in Table 1.2.\n\n\nOptimum value of \\(p_{\\cap}\\) for each benchmark and model.\n\n\nScenario\nRandomForest_Jackknife\nRanger_Jackknife\nRanger_Inifinitesimal\n\n\n\n\n\nIPC2018\n0.59\n0.27\n0.44\n\n\n\nMAXSAT19-UCMS\n0.55\n0.14\n0.03\n\n\n\nSAT11-INDU\n0.63\n0.31\n0.01\n\n\n\nSAT16-MAIN\n0.33\n0.33\n0\n\n\n\nSAT18-EXP\n0.81\n0.58\n0.55\n\n\n\nGeneric best\n0.82\n0.31\n0.17\n\n\n\n\n\nFigures 1.2 shows the normalized gap closed for the mean, 25th percentile, 50th percentile, and 75th percentiles for each scenario depending on \\(p_{\\cap}\\). While the optimal values are very different across different scenario and each algorithm selector, the differences in terms of gap closed are relatively small as long as \\(p_{\\cap}\\) is not too large. The best average value for \\(p_{\\cap}\\) across all scenarios for RFJ model, RI model, and RJ model are 0.82, 0.17, 0.31 respectively which yields performance improvements over the baselines in most cases (see Table 1.3). For the overall best performance, we recommend to tune \\(p_{\\cap}\\) for the particular scenario. However, using the generic best values of 0.82, 0.17, and 0.31 for RFJ, RI, and RJ, respectively, provides a reasonable starting point that yields good performance across the range of scenarios considered here.\nThe optimal value of \\(p_{\\cap}\\) allows us to draw conclusions with respect to the predictive accuracy of the performance models we are using. A small value would suggest that the predictions of the performance models are not very accurate, as we have to include even solvers whose predicted runtime distribution has a small overlap with the runtime distribution of the best predicted solver to include solvers that are actually good. If the optimal value of \\(p_{\\cap}\\) was 0, we would have to include all solvers, even the ones whose predicted distribution has no overlap with the best predicted solver – in other words, the predicted runtime distribution of the actual best solver has no overlap with the predicted runtime distribution of the best predicted solver.\nHere, for the RFJ model, the optimal values for \\(p_{\\cap}\\) are relatively large in most cases, and even the smallest values are far greater than 0. For the RJ model, the values are lower than those for the RFJ model, except for SAT16-MAIN, where the values are equal. For the RI model, except for IPC2018, the values are even lower than those for the RJ model. This indicates that the predictions of the performance models for RFJ are quite good – while the best predicted solver is not always the actual best solver for a given problem instance, the predicted runtime distribution of the actual best solver has a large overlap with the predicted runtime distribution of the predicted best solver.\nBased on the low values of the RI model, it appears that this model performs worse than the other two. This claim is also evident in Table 1.3 where, for IPC2018, MAXSAT19-UCMS, and SAT11-INDU, the performance of the \\(AS\\) (RI) model is worse than the other two in at least two of the performance measurements. For SAT16-MAIN, the \\(AS\\) (RI) model performs worst only in terms of PAR10, indicating more timeouts; however, on average, it provides better runtime predictions, so the runtime and MCP measures are better than those of the RJ model. For SAT18-EXP, as shown in Table 1.2, the RI model has a slightly lower \\(p_{\\cap}\\) value than the RJ model; however, the difference is minimal, and the RI model performs better than the RJ model according to Table 1.3. Overall, according to Table 1.3, the RFJ model outperforms the other two models in at least two performance metrics in 4 out of 5 scenarios when performing single algorithm selection.\n\n\n5.5.2 Algorithm Selection Results\n\n\n\nSummary of results. The plot shows the degree to which the gap between the SBS and VBS PAR10 scores is closed by each method. For the VBS and SBS, we choose the top (n) solvers, where (n) is the number of processors, for a given problem instance and across all instances, respectively. (AS_0) chooses the top (n) solvers predicted by algorithm selection, without regard for any overlap in their predicted runtime distributions. (AS_{p_{}}) represents the proposed formulation, with the number of processors restricted to at most the specific value indicated on the x axis – depending on the overlap of the predicted runtime distributions, fewer solvers than the maximum may be chosen. The (p_{}) values for IPC2018, MAXSAT19-UCMS, SAT11-INDU, SAT18-EXP, and SAT16-MAIN are 0.59, 0.55, 0.63, 0.81, and 0.33 and respectively. Time Splitting is the baseline approach that allocates time proportional to the predicted runtime and standard deviation for each solver, scheduling more than one solver to be run per processor.\n\n\n\n\n\n\n\n\n\nViolin plot of the distribution of the number of selected solvers to run in parallel across all problem instances for each scenario for the respective optimal (p_{}) and the maximum level of parallelism (seven processors for MAXSAT19-UCMS and 10 for all other scenarios). The diamond denotes the mean value. The top-left plot refers to the RFJ model, the top-right plot to the RI model, and the bottom plot to the RJ model.\n\n\nTo evaluate the effectiveness of our approach, we carried out a series of experiments using the optimum and the average best value for \\(p_{\\cap}\\) for each scenario using the RFJ model where we varied the number of processors used for parallel execution from one to ten for the SAT18-EXP, SAT16-MAIN, SAT11-INDU, and IPC2018 scenarios. For the MAXSAT19-UCMS scenario, we used a maximum of seven processors as there are only seven algorithms. For RFJ model Figure 1.3 shows the PAR10 score results in terms of the normalized performance gap between the sequential single best solver and sequential virtual best solver for all scenarios and numbers of processors.\nThe figure demonstrates the promise of the approach we propose here. In three out of five scenarios, we achieve the overall top performance with the maximum number of processors (even better than the parallel virtual best solver!) and for the remaining two scenarios only the parallel virtual best solver is better. We are able to achieve better performance than the parallel virtual best solver when running in parallel because our approach does not necessarily use all available processors, unlike the baseline approaches that we compare to. While the performance of the virtual best solver suffers for a large number of parallel runs, our approach keeps the overhead of running many things in parallel low and is thus better overall. We emphasize that the results we show here are actual measured values for running in parallel, rather than assuming overhead-free parallelization based on sequential runtimes, as is commonly done in the literature. Our results demonstrate that this common assumption is unrealistic except for a small number of parallel runs.\nEven for a small number of processors, our approach yields better performance than others. Initially, the performance is similar to \\(AS_0\\) (running the top \\(n\\) solvers), but our approach quickly becomes better as the number of available processors increases. This is expected, as for a single processor the two methods run exactly the same solver, but for a larger number of processors our method may not run as many as \\(AS_0\\), thus decreasing overhead and overall solving performance.\nFor the IPC2018 scenario, we achieve the best overall results, improving performance substantially over all other approaches for 10 processors. The 3S approach is never close to the performance of our method and consistently yields worse results. The greedy time-splitting method also underperformed, often allocating time slices smaller than required to solve the instance and thus wasting resources. For more than seven parallel runs, the parallel virtual best solver, i.e. choosing the actual best solvers for each instance to run in parallel, starts to perform worse than our method, which does not use as many processors and incurs lower overhead.\nThe results for the other scenarios are qualitatively similar. While for a small number of processors, other methods perform similar to ours, the gap between them widens as the number of parallel runs increases. 3S consistently shows worse performance, whereas running the top \\(n\\) solvers based on algorithm selection (without considering the predicted performance distributions) is usually competitive and in some cases gives the same performance as our method. The baseline of allocating a time to run proportional to the predicted runtime and standard deviation for each solver is not competitive, consistently showing bad performance – this baseline is worse than simply running the top \\(n\\) single best solvers in parallel on three scenarios for large numbers of parallel runs. For the IPC2018 and MAXSAT19-UCMS scenarios, the performance of some methods becomes worse than the single best solver for large numbers of processors, showing the limitations of these approaches. For the SAT2016-MAIN scenario, our approach is performing so close to the naïve parallel algorithm selection (top \\(n\\) solvers based on algorithm selection) because the standard error of the predictions was large and this resulted in large parallel portfolios for the majority of instances.\nTable 1.3 shows more detailed results. The normalized gap closed represented the mean and standard deviation of the normalized gap closed across the 10 folds, whereas the  1.3 shows the mean of the normalized gap closed across all instances at once. We see that our method results in substantial savings in terms of all three measures across all scenarios – the proposed approach is always the best overall, regardless of the performance measure. Note that we never beat the sequential VBS, which represents the upper bound on the performance of any algorithm selection system – we cannot do better than only running the actual best solver. In many cases, the actual performance we achieve is close to the sequential VBS though. The results also show that using the “generic” best value for \\(p_{\\cap}\\) of 0.82 still gives substantial performance improvements over other approaches – usually it gives the second best performance. The only exception to this are the MAXSAT19-UCMS and SAT2016-MAIN scenarios, where running the top \\(n\\) solvers predicted by algorithm selection does better. The gap is relatively small though, and we still beat most of the other baselines.\n\n\n\nDetailed results. Mean and standard deviation of values for runtime, MCP, and PAR10 across all problem instances in a scenario for the sequential virtual best solver, sequential single best solver, and single top predicted algorithm in the initial three rows. The second set of rows for each scenario shows the results for the maximum number of processors (10 for IPC2018, and 7 for MAXSAT19-UCMS) for our approach and the baselines we compare to. All numbers were rounded to integers. The best value for each scenario and measure is shown in bold (excepting the sequential VBS, which is by definition always the best), the second best in italics. The normalized gap closed represents the mean and standard deviation of the normalized gap closed across the folds.\n\n\nScenario\nApproach\nRuntime [s]\nMCP\nPAR10\nNormalizedGap\n\n\n\n\n\n1 Processor\n\n\n\n\n\n\n\nVBS\n508\\(\\pm\\)697\n0\n3478\\(\\pm\\)6903\n1\n\n\n\nAS (RFJ)\n607\\(\\pm\\)751\n99\\(\\pm\\)301\n4657\\(\\pm\\)7725\n-0.44\\(\\pm\\)2.84\n\n\n\nAS (RI)\n608\\(\\pm\\)751\n100\\(\\pm\\)293\n4456\\(\\pm\\)7583\n-0.35\\(\\pm\\)2.85\n\n\n\nAS (RJ)\n604\\(\\pm\\)752\n96\\(\\pm\\)293\n4519\\(\\pm\\)7633\n-0.39\\(\\pm\\)2.84\n\n\n\nSBS\n734\\(\\pm\\)770\n226\\(\\pm\\)414\n5459\\(\\pm\\)8072\n0\n\n\n\n10 Processors\n\n\n\n\n\n\n\n3S\n645\\(\\pm\\)770\n137\\(\\pm\\)471\n5235\\(\\pm\\)8047\n-0.76\\(\\pm\\)2.7\n\n\n\nTime Splitting (RFJ)\n637\\(\\pm\\)797\n129\\(\\pm\\)348\n5565\\(\\pm\\)8241\n-0.84\\(\\pm\\)2.5\n\n\n\nTime Splitting (RI)\n641\\(\\pm\\)799\n133\\(\\pm\\)361\n5636\\(\\pm\\)8274\n-0.99\\(\\pm\\)2.52\n\n\n\nTime Splitting (RJ)\n636\\(\\pm\\)794\n128\\(\\pm\\)353\n5496\\(\\pm\\)8206\n-0.92\\(\\pm\\)2.51\n\n\n\n\\(AS_0\\) (RFJ)\n612\\(\\pm\\)779\n104\\(\\pm\\)307\n5134\\(\\pm\\)8027\n-0.66\\(\\pm\\)2.56\n\n\n\n\\(AS_0\\) (RI)\n616\\(\\pm\\)783\n107\\(\\pm\\)312\n5206\\(\\pm\\)8065\n-0.69\\(\\pm\\)2.54\n\n\n\n\\(AS_0\\) (RJ)\n616\\(\\pm\\)783\n107\\(\\pm\\)312\n5206\\(\\pm\\)8065\n-0.69\\(\\pm\\)2.54\n\n\n\n\\(AS_{p_{\\cap} = 0.59}\\) (RFJ)\n569\\(\\pm\\)745\n61\\(\\pm\\)223\n4484\\(\\pm\\)7651\n-0.18\\(\\pm\\)2.74\n\n\n\n\\(AS_{p_{\\cap} = 0.44}\\) (RI)\n557\\(\\pm\\)728\n49\\(\\pm\\)190\n4135\\(\\pm\\)7403\n-0.19\\(\\pm\\)2.89\n\n\n\n\\(AS_{p_{\\cap} = 0.27}\\) (RJ)\n570\\(\\pm\\)744\n62\\(\\pm\\)229\n4350\\(\\pm\\)7552\n-0.21\\(\\pm\\)2.72\n\n\n\n\\(AS_{p_{\\cap} = 0.82}\\) (RFJ)\n579\\(\\pm\\)742\n70\\(\\pm\\)233\n4359\\(\\pm\\)7548\n-0.26\\(\\pm\\)2.88\n\n\n\n\\(AS_{p_{\\cap} = 0.17}\\) (RI)\n570\\(\\pm\\)739\n62\\(\\pm\\)230\n4283\\(\\pm\\)7501\n-0.24\\(\\pm\\)2.87\n\n\n\n\\(AS_{p_{\\cap} = 0.31}\\) (RJ)\n570\\(\\pm\\)743\n62\\(\\pm\\)229\n4350\\(\\pm\\)7552\n-0.21\\(\\pm\\)2.72\n\n\n\n1 Processor\n\n\n\n\n\n\n\nVBS\n858\\(\\pm\\)1476\n0\n7768\\(\\pm\\)14717\n1\n\n\n\nAS (RFJ)\n1037\\(\\pm\\)1555\n179\\(\\pm\\)641\n9363\\(\\pm\\)15684\n0.55\\(\\pm\\)0.28\n\n\n\nAS (RI)\n1076\\(\\pm\\)1575\n218\\(\\pm\\)729\n9686\\(\\pm\\)15850\n0.45\\(\\pm\\)0.34\n\n\n\nAS (RJ)\n1044\\(\\pm\\)1565\n186\\(\\pm\\)666\n9540\\(\\pm\\)15793\n0.49\\(\\pm\\)0.23\n\n\n\nSBS\n1190\\(\\pm\\)1657\n332\\(\\pm\\)940\n11386\\(\\pm\\)16696\n0\n\n\n\n7 Processors\n\n\n\n\n\n\n\n3S\n953\\(\\pm\\)1480\n95\\(\\pm\\)437\n8317\\(\\pm\\)15031\n0.83\\(\\pm\\)0.16\n\n\n\nTime Splitting (RFJ)\n908\\(\\pm\\)1523\n51\\(\\pm\\)308\n8668\\(\\pm\\)15353\n0.75\\(\\pm\\)0.16\n\n\n\nTime Splitting (RI)\n919\\(\\pm\\)1535\n61\\(\\pm\\)356\n8849\\(\\pm\\)15470\n0.7\\(\\pm\\)0.2\n\n\n\nTime Splitting (RJ)\n917\\(\\pm\\)1531\n59\\(\\pm\\)352\n8790\\(\\pm\\)15431\n0.71\\(\\pm\\)0.2\n\n\n\n\\(AS_0\\) (RFJ)\n894\\(\\pm\\)1506\n37\\(\\pm\\)247\n8258\\(\\pm\\)15062\n0.85\\(\\pm\\)0.16\n\n\n\n\\(AS_0\\) (RI)\n894\\(\\pm\\)1506\n37\\(\\pm\\)247\n8258\\(\\pm\\)15062\n0.85\\(\\pm\\)0.16\n\n\n\n\\(AS_0\\) (RJ)\n894\\(\\pm\\)1506\n37\\(\\pm\\)247\n8258\\(\\pm\\)15062\n0.85\\(\\pm\\)0.16\n\n\n\n\\(AS_{p_{\\cap} = {0.55}}\\) (RFJ)\n891\\(\\pm\\)1496\n33\\(\\pm\\)215\n8141\\(\\pm\\)14975\n0.88\\(\\pm\\)0.17\n\n\n\n\\(AS_{p_{\\cap} = 0.03}\\) (RI)\n894\\(\\pm\\)1506\n37\\(\\pm\\)247\n8258\\(\\pm\\)15062\n0.85\\(\\pm\\)0.16\n\n\n\n\\(AS_{p_{\\cap} = 0.14}\\) (RJ)\n921\\(\\pm\\)1521\n63\\(\\pm\\)369\n8568\\(\\pm\\)15263\n0.76\\(\\pm\\)0.24\n\n\n\n\\(AS_{p_{\\cap} = 0.82}\\) (RFJ)\n928\\(\\pm\\)1513\n70\\(\\pm\\)364\n8461\\(\\pm\\)15175\n0.81\\(\\pm\\)0.18\n\n\n\n\\(AS_{p_{\\cap} = 0.17}\\) (RI)\n901\\(\\pm\\)1502\n43\\(\\pm\\)275\n8208\\(\\pm\\)15015\n0.88\\(\\pm\\)0.16\n\n\n\n\\(AS_{p_{\\cap} = 0.31}\\) (RJ)\n931\\(\\pm\\)1525\n73\\(\\pm\\)402\n8578\\(\\pm\\)15259\n0.78\\(\\pm\\)0.21\n\n\n\n\n\n\n\n\nDetailed results. Mean and standard deviation of values for runtime, MCP, and PAR10 across all problem instances in a scenario for the sequential virtual best solver, sequential single best solver, and single top predicted algorithm in the initial three rows. The second set of rows for each scenario shows the results for the maximum number of processors (10 for SAT16-MAIN and SAT11-INDU) for our approach and the baselines we compare to. All numbers were rounded to integers. The best value for each scenario and measure is shown in bold (excepting the sequential VBS, which is by definition always the best), the second best in italics. The normalized gap closed represents the mean and standard deviation of the normalized gap closed across the folds.\n\n\nScenario\nApproach\nRuntime [s]\nMCP\nPAR10\nNormalizedGap\n\n\n\n\n\n1 Processor\n\n\n\n\n\n\n\nVBS\n1140\\(\\pm\\)1836\n0\n8040\\(\\pm\\)17905\n1\n\n\n\nAS (RFJ)\n1535\\(\\pm\\)2058\n395\\(\\pm\\)1037\n11735\\(\\pm\\)20768\n0.16\\(\\pm\\)0.79\n\n\n\nAS (RI)\n1610\\(\\pm\\)2108\n470\\(\\pm\\)1145\n12710\\(\\pm\\)21389\n-0.06\\(\\pm\\)0.9\n\n\n\nAS (RJ)\n1565\\(\\pm\\)2049\n425\\(\\pm\\)1017\n11315\\(\\pm\\)20402\n0.34\\(\\pm\\)0.49\n\n\n\nSBS\n1818\\(\\pm\\)2168\n678\\(\\pm\\)1340\n14268\\(\\pm\\)22154\n0\n\n\n\n10 Processors\n\n\n\n\n\n\n\n3S\n1298\\(\\pm\\)1898\n158\\(\\pm\\)546\n9098\\(\\pm\\)18780\n0.78\\(\\pm\\)0.3\n\n\n\nTime Splitting (RFJ)\n1335\\(\\pm\\)2009\n225\\(\\pm\\)708\n10635\\(\\pm\\)20138\n0.49\\(\\pm\\)0.61\n\n\n\nTime Splitting (RI)\n1429\\(\\pm\\)2108\n318\\(\\pm\\)875\n12379\\(\\pm\\)21378\n0.19\\(\\pm\\)0.53\n\n\n\nTime Splitting (RJ)\n1334\\(\\pm\\)1998\n224\\(\\pm\\)689\n10634\\(\\pm\\)20138\n0.55\\(\\pm\\)0.43\n\n\n\n\\(AS_0\\) (RFJ)\n1272\\(\\pm\\)1927\n161\\(\\pm\\)548\n8922\\(\\pm\\)18645\n0.89\\(\\pm\\)0.12\n\n\n\n\\(AS_0\\) (RI)\n1238\\(\\pm\\)1892\n127\\(\\pm\\)385\n8588\\(\\pm\\)18350\n0.92\\(\\pm\\)0.1\n\n\n\n\\(AS_0\\) (RJ)\n1262\\(\\pm\\)1910\n151\\(\\pm\\)480\n8612\\(\\pm\\)18342\n0.92\\(\\pm\\)0.1\n\n\n\n\\(AS_{p_{\\cap} = 0.63}\\) (RFJ)\n1241\\(\\pm\\)1901\n131\\(\\pm\\)451\n8591\\(\\pm\\)18349\n0.92\\(\\pm\\)0.1\n\n\n\n\\(AS_{p_{\\cap} = 0.01}\\) (RI)\n1236\\(\\pm\\)1890\n121\\(\\pm\\)379\n8586\\(\\pm\\)18351\n0.92\\(\\pm\\)0.1\n\n\n\n\\(AS_{p_{\\cap} = 0.31}\\) (RJ)\n1289\\(\\pm\\)1934\n178\\(\\pm\\)595\n9089\\(\\pm\\)18787\n0.78\\(\\pm\\)0.28\n\n\n\n\\(AS_{p_{\\cap} = 0.82}\\) (RFJ)\n1247\\(\\pm\\)1900\n123\\(\\pm\\)431\n8747\\(\\pm\\)18501\n0.83\\(\\pm\\)0.28\n\n\n\n\\(AS_{p_{\\cap} = 0.17}\\) (RI)\n1259\\(\\pm\\)1912\n139\\(\\pm\\)477\n8909\\(\\pm\\)18649\n0.74\\(\\pm\\)0.36\n\n\n\n\\(AS_{p_{\\cap} = 0.31}\\) (RJ)\n1289\\(\\pm\\)1934\n178\\(\\pm\\)595\n9089\\(\\pm\\)18787\n0.78\\(\\pm\\)0.28\n\n\n\n1 Processor\n\n\n\n\n\n\n\nVBS\n1867\\(\\pm\\)2193\n0\n15005\\(\\pm\\)22530\n1\n\n\n\nAS (RFJ)\n2315\\(\\pm\\)2273\n448\\(\\pm\\)1109\n19066\\(\\pm\\)23883\n0.33\\(\\pm\\)0.56\n\n\n\nAS (RI)\n2383\\(\\pm\\)2294\n516\\(\\pm\\)1151\n19956\\(\\pm\\)24111\n0.05\\(\\pm\\)0.66\n\n\n\nAS (RJ)\n2400\\(\\pm\\)2269\n533\\(\\pm\\)1177\n19316\\(\\pm\\)23880\n0.3\\(\\pm\\)0.24\n\n\n\nSBS\n2560\\(\\pm\\)2294\n693\\(\\pm\\)1415\n21940\\(\\pm\\)24464\n0\n\n\n\n10 Processors\n\n\n\n\n\n\n\n3S\n2093\\(\\pm\\)2228\n226\\(\\pm\\)547\n16874\\(\\pm\\)23228\n0.59\\(\\pm\\)0.59\n\n\n\nTime Splitting (RFJ)\n2101\\(\\pm\\)2247\n234\\(\\pm\\)732\n16717\\(\\pm\\)23149\n0.78\\(\\pm\\)0.46\n\n\n\nTime Splitting (RI)\n2089\\(\\pm\\)2256\n222\\(\\pm\\)642\n17691\\(\\pm\\)23593\n0.37\\(\\pm\\)0.85\n\n\n\nTime Splitting (RJ)\n2098\\(\\pm\\)2254\n231\\(\\pm\\)674\n17372\\(\\pm\\)23447\n0.56\\(\\pm\\)0.39\n\n\n\n\\(AS_0\\) (RFJ)\n2065\\(\\pm\\)2221\n198\\(\\pm\\)652\n16189\\(\\pm\\)22931\n0.7\\(\\pm\\)0.39\n\n\n\n\\(AS_0\\) (RI)\n2016\\(\\pm\\)2225\n150\\(\\pm\\)503\n16469\\(\\pm\\)23122\n0.68\\(\\pm\\)0.6\n\n\n\n\\(AS_0\\) (RJ)\n2048\\(\\pm\\)2228\n181\\(\\pm\\)597\n16336\\(\\pm\\)23023\n0.68\\(\\pm\\)0.59\n\n\n\n\\(AS_{p_{\\cap} = 0.33}\\) (RFJ)\n2065\\(\\pm\\)2221\n198\\(\\pm\\)652\n16189\\(\\pm\\)22931\n0.7\\(\\pm\\)0.39\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RI)\n2016\\(\\pm\\)2225\n150\\(\\pm\\)503\n16469\\(\\pm\\)23122\n0.68\\(\\pm\\)0.6\n\n\n\n\\(AS_{p_{\\cap} = 0.33}\\) (RJ)\n2088\\(\\pm\\)2239\n222\\(\\pm\\)704\n16705\\(\\pm\\)23156\n0.64\\(\\pm\\)0.37\n\n\n\n\\(AS_{p_{\\cap} = 0.82}\\) (RFJ)\n2094\\(\\pm\\)2222\n228\\(\\pm\\)730\n16383\\(\\pm\\)22993\n0.69\\(\\pm\\)0.41\n\n\n\n\\(AS_{p_{\\cap} = 0.17}\\) (RI)\n2041\\(\\pm\\)2230\n174\\(\\pm\\)591\n16822\\(\\pm\\)23261\n0.57\\(\\pm\\)0.63\n\n\n\n\\(AS_{p_{\\cap} = 0.31}\\) (RJ)\n2096\\(\\pm\\)2240\n229\\(\\pm\\)713\n16877\\(\\pm\\)23227\n0.63\\(\\pm\\)0.36\n\n\n\n\n\n\n\n\nDetailed results. Mean and standard deviation of values for runtime, MCP, and PAR10 across all problem instances in a scenario for the sequential virtual best solver, sequential single best solver, and single top predicted algorithm in the initial three rows. The second set of rows for each scenario shows the results for the maximum number of processors (10 for SAT18-EXP) for our approach and the baselines we compare to. All numbers were rounded to integers. The best value for each scenario and measure is shown in bold (excepting the sequential VBS, which is by definition always the best), the second best in italics. The normalized gap closed represents the mean and standard deviation of the normalized gap closed across the folds.\n\n\nScenario\nApproach\nRuntime [s]\nMCP\nPAR10\nNormalizedGap\n\n\n\n\n\n1 Processor\n\n\n\n\n\n\n\nVBS\n1146\\(\\pm\\)1945\n0\n9687\\(\\pm\\)19547\n1\n\n\n\nAS (RFJ)\n1615\\(\\pm\\)2138\n468\\(\\pm\\)1192\n13470\\(\\pm\\)21889\n0.64\\(\\pm\\)0.18\n\n\n\nAS (RI)\n1648\\(\\pm\\)2151\n502\\(\\pm\\)1256\n13758\\(\\pm\\)22034\n0.59\\(\\pm\\)0.18\n\n\n\nAS (RJ)\n1690\\(\\pm\\)2170\n543\\(\\pm\\)1302\n14183\\(\\pm\\)22247\n0.57\\(\\pm\\)0.16\n\n\n\nSBS\n2400\\(\\pm\\)2249\n1254\\(\\pm\\)1832\n20629\\(\\pm\\)24280\n0\n\n\n\n10 Processors\n\n\n\n\n\n\n\n3S\n1625\\(\\pm\\)2228\n479\\(\\pm\\)1265\n15010\\(\\pm\\)22802\n0.5\\(\\pm\\)0.23\n\n\n\nTime Splitting (RFJ)\n1714\\(\\pm\\)2292\n571\\(\\pm\\)1384\n15992\\(\\pm\\)23222\n0.42\\(\\pm\\)0.23\n\n\n\nTime Splitting (RI)\n1640\\(\\pm\\)2266\n497\\(\\pm\\)1280\n15408\\(\\pm\\)23003\n0.46\\(\\pm\\)0.24\n\n\n\nTime Splitting (RJ)\n1745\\(\\pm\\)2308\n602\\(\\pm\\)1434\n16151\\(\\pm\\)23267\n0.4\\(\\pm\\)0.24\n\n\n\n\\(AS_0\\) (RFJ)\n1702\\(\\pm\\)2301\n559\\(\\pm\\)1389\n16235\\(\\pm\\)23355\n0.39\\(\\pm\\)0.27\n\n\n\n\\(AS_0\\) (RI)\n1654\\(\\pm\\)2285\n511\\(\\pm\\)1324\n15804\\(\\pm\\)23194\n0.42\\(\\pm\\)0.29\n\n\n\n\\(AS_0\\) (RJ)\n1678\\(\\pm\\)2288\n535\\(\\pm\\)1351\n15956\\(\\pm\\)23243\n0.4\\(\\pm\\)0.3\n\n\n\n\\(AS_{p_{\\cap} = 0.81}\\) (RFJ)\n1518\\(\\pm\\)2172\n372\\(\\pm\\)1124\n13884\\(\\pm\\)22265\n0.62\\(\\pm\\)0.22\n\n\n\n\\(AS_{p_{\\cap} = 0.55}\\) (RI)\n1541\\(\\pm\\)2191\n397\\(\\pm\\)1177\n14034\\(\\pm\\)22332\n0.6\\(\\pm\\)0.21\n\n\n\n\\(AS_{p_{\\cap} = 0.58}\\) (RJ)\n1622\\(\\pm\\)2237\n477\\(\\pm\\)1268\n15008\\(\\pm\\)22805\n0.5\\(\\pm\\)0.25\n\n\n\n\\(AS_{p_{\\cap} = 0.82}\\) (RFJ)\n1532\\(\\pm\\)2178\n386\\(\\pm\\)1146\n14025\\(\\pm\\)22336\n0.6\\(\\pm\\)0.23\n\n\n\n\\(AS_{p_{\\cap} = 0.17}\\) (RI)\n1555\\(\\pm\\)2221\n410\\(\\pm\\)1191\n14558\\(\\pm\\)22628\n0.57\\(\\pm\\)0.23\n\n\n\n\\(AS_{p_{\\cap} = 0.31}\\) (RJ)\n1649\\(\\pm\\)2265\n505\\(\\pm\\)1319\n15544\\(\\pm\\)23064\n0.46\\(\\pm\\)0.26\n\n\n\n\n\n\n\n5.5.3 Number of Selected Solvers\nAs mentioned above, allowing our approach to use up to a certain number of processors does not mean that this exact number of parallel runs will be done. In practice, it is often much lower than that, as we see when comparing the performance of our approach to \\(AS_0\\), which runs the top \\(n\\) predicted solvers in parallel. Figure 1.4 shows the distribution of the number of selected solvers for each scenario and each algorithm selector. For RFJ, the mean number of solvers chosen for IPC2018 is around 6.5, for MAXSAT19-UCMS around 6 (out of 7), for SAT11-INDU around 9, for SAT16-MAIN around 10, and for SAT18-EXP around 5.5. For RI, the mean number of solvers chosen for IPC2018 is around 4.5, for MAXSAT19-UCMS around 7 (out of 7), for SAT11-INDU around 9.5, for SAT16-MAIN around 10, and for SAT18-EXP around 6.5. Similarly, for RJ, the mean number of solvers chosen for IPC2018 is around 6, for MAXSAT19-UCMS around 6 (out of 7), for SAT11-INDU around 8, for SAT16-MAIN around 8, and for SAT18-EXP around 6.\nFor RFJ, we see that the largest difference to the maximum number of parallel runs occurs for the two scenarios where we observe the largest performance improvements of our approach, IPC2018 and SAT18-EXP. Similarly, the scenario with the highest number of solvers chosen on average (SAT16-MAIN) is where we see the smallest performance improvement. For RI and RJ the same comparison also exists. This clearly shows again that the advantage of our approach is that it does not simply use as many parallel processors as are available, which increases overhead, but intelligently chooses how many of the available processors to use for best performance. In at least some cases, more is less, and we show how to leverage this.\nFigure 1.4 also shows that our approach uses the full range of available parallel runs in most cases, from running only a single solver to as many parallel runs as there are processors. Our approach is not simply a one-size-fits all that usually uses a similar number of runs, but varies the size of the selected parallel portfolio dynamically, based on the instance to be solved.\n\n\n5.5.4 Ranger vs RandomForest Results\nFigure 1.5 presents a comparison between the random forest implementation and the ranger implementations. When comparing the naive algorithm selection methods \\(AS (RFJ)\\), \\(AS (RJ)\\) and \\(AS (RI)\\), which select the best predicted algorithm, based on Tables 1.3, 1.4 and 1.5, the RFJ model emerges as a more promising algorithm selector in all scenarios except IPC2018, where the RJ model performs slightly better in terms of runtime and MCP. For SAT11-INDU, the \\(AS (RFJ)\\) method is superior in terms of runtime and MCP, but performs slightly worse than the RI model in terms of PAR10.\nWhen we compare \\(AS_{0} (RFJ)\\), \\(AS_{0} (RJ)\\), and \\(AS_{0} (RI)\\), which select the top predicted algorithms to run in parallel, the performance of these methods is very competitive. In some cases, the RI model performs better, such as in SAT18-EXP and SAT11-INDU, across all metrics, and is superior only in terms of runtime and MCP in SAT16-MAIN. In other cases, the RFJ model performs better, as seen in IPC2018. For MAXSAT19-UCMS, all algorithm selectors perform equally, as with \\(p_{\\cap} = 0\\), all 7 available solvers are selected.\nWhen comparing the methods using a tuned value of \\(p_{\\cap}\\), denoted as \\(AS_{p_{\\cap}}\\), the RI model outperformed the others in most cases. In IPC2018 and SAT11-INDU, it was superior in terms of runtime, MCP, and PAR10. For SAT16-MAIN, it outperformed the other two in terms of MCP and runtime, although the PAR10 of RFJ was slightly better. For MAXSAT19-UCMS and SAT18-EXP, the RFJ model performed better than the others across all performance metrics. It is worth mentioning that the RJ model could not beat other models in the \\(AS_{p_{\\cap}}\\) method.\nThese comparisons is based on the runtime, MCP, and PAR10 scores listed in Tables 1.3, 1.4, and 1.5. As shown in Figure 1.5, a similar comparison exists, with the values representing the PAR10 score normalized gap closed between SBS and VBS. In this context, \\(AS_{p_{\\cap}} (RI)\\) is superior in three out of five scenarios, while \\(AS_{p_{\\cap}} (RFJ)\\) performs best in the remaining two scenarios. Although using RI for single algorithm selection performed the worst, it appears to be a better model for applying the portfolio selection approach.\n\n\n\nResults Overview. The plot illustrates the extent to which each method narrows the gap between the PAR10 scores of the Single Best Solver (SBS) and the Virtual Best Solver (VBS). For VBS and SBS, the top (n) solvers are selected, where (n) matches the number of processors available for each problem instance and across all instances, respectively. (AS_0) selects the top (n) solvers as predicted by algorithm selection, disregarding any overlap in their predicted runtime distributions. (AS_{p_{}}) follows the approach proposed in , with the number of processors capped at the specific value on the x-axis — fewer solvers than this maximum may be selected based on the overlap in runtime predictions. The RFJ model is trained with the randomForest and Jackknife method, RI uses the Ranger model with the Infinitesimal Jackknife method, and RJ applies the Ranger model with the Jackknife method. The optimal (p_{}) values for each scenario and each model are listed in Table 1.2.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automatic Parallel Portfolio Selection</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4/AutomaticParallelPortfolioSelection.html#conclusions-and-future-work",
    "href": "chapters/chapter4/AutomaticParallelPortfolioSelection.html#conclusions-and-future-work",
    "title": "5  Automatic Parallel Portfolio Selection",
    "section": "5.6 Conclusions and Future Work",
    "text": "5.6 Conclusions and Future Work\nIn this study, we proposed a general method for selecting solvers from a portfolio of solvers and scheduling them in parallel, taking into account the predicted runtime distribution to intelligently choose not only which solvers to run, but also how many. This is in contrast to most other approaches in the literature, which either choose a constant number or use all available processors. Further, we measured the actual runtime when running more than one algorithm in parallel, rather than assuming the sequential runtime. We demonstrated substantial performance improvements across a wide range of scenarios, handily beating baseline methods and other approaches from the literature. The proposed method establishes a new state of the art in parallel algorithm selection and is simple to apply in practice – we are only using information that is readily available in common algorithm selection methods, and while for the best performance the parameter \\(p_{\\cap}\\) of our method should be tuned, a reasonable default already shows good performance. This parameter allows our method to be tailored to specific application domains and scenarios. We also compared three different algorithm performance models, specifically three variations of the regression random forest, and applied the method to evaluate their effectiveness.\nWhile we do show substantial performance improvements, there is room for further advances. We have focused our investigation on state-of-the-art random forest performance models, the jackknife and infinitesimal jackknife method for estimating uncertainties. It is possible that other types of models may perform better in this context if the uncertainty estimates of their predictions are better. It is also possible to combine different types of performance models for different algorithms, allowing much more flexibility and potentially greater performance improvements. While our baseline method that allocates resources to each algorithm did not perform well, investigating more sophisticated approaches for this would also be interesting.\nFurthermore, the optimal values of \\(p_{\\cap}\\) varied significantly between different scenarios and performance models, requiring tuning for each. This can increase the complexity of the algorithm selection process, as well as the computational effort and time required for method configuration, since each scenario needs specific adjustments to achieve optimal performance.\n\n\n\n\n\n\nAigner, Martin, Armin Biere, Christoph M Kirsch, Aina Niemetz, and Mathias Preiner. 2013. “Analysis of Portfolio-Style Parallel SAT Solving on Current Multi-Core Architectures.” POS@ SAT 29: 28–40.\n\n\nAmadini, Roberto, Maurizio Gabbrielli, and Jacopo Mauro. 2014. “SUNNY: A Lazy Portfolio Approach for Constraint Solving.” Theory Pract. Log. Program. 14 (4-5): 509–24. https://doi.org/10.1017/S1471068414000179.\n\n\n———. 2015. “A Multicore Tool for Constraint Solving.” In Proceedings of the 24th International Conference on Artificial Intelligence, 232–38.\n\n\nBischl, Bernd, Pascal Kerschke, Lars Kotthoff, Marius Lindauer, Yuri Malitsky, Alexandre Fréchette, Holger Hoos, et al. 2016. “ASlib: A Benchmark Library for Algorithm Selection.” Artificial Intelligence 237: 41–58.\n\n\nBischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. Jones. 2016. “mlr: Machine Learning in R.” Journal of Machine Learning Research 17 (170): 1–5. https://jmlr.org/papers/v17/15-066.html.\n\n\nBischl, Bernd, Raphael Sonabend, Lars Kotthoff, and Michel Lang, eds. 2024. Applied Machine Learning Using mlr3 in R. CRC Press. https://mlr3book.mlr-org.com.\n\n\nBreiman, Leo. 2001. “Random Forests.” Machine Learning 45: 5–32.\n\n\nFawcett, Chris, Mauro Vallati, Frank Hutter, Jörg Hoffmann, Holger Hoos, and Kevin Leyton-Brown. 2014. “Improved Features for Runtime Prediction of Domain-Independent Planners.” Proceedings of the International Conference on Automated Planning and Scheduling 24 (1): 355–59. https://doi.org/10.1609/icaps.v24i1.13680.\n\n\nGomes, Carla, and Bart Selman. 2001. “Algorithm Portfolios.” Artificial Intelligence 126: 43–62.\n\n\nHoos, Holger H., Roland Kaminski, Marius Thomas Lindauer, and Torsten Schaub. 2015. “aspeed: Solver scheduling via answer set programming.” TPLP 15 (1): 117–42.\n\n\nHu, Mengqi, Teresa Wu, and Jeffery D. Weir. 2012. “An Intelligent Augmentation of Particle Swarm Optimization with Multiple Adaptive Methods.” Information Sciences 213: 68–83. https://doi.org/https://doi.org/10.1016/j.ins.2012.05.020.\n\n\nHuberman, Bernardo A., Rajan M. Lukose, and Tad Hogg. 1997. “An economics approach to hard computational problems.” Science 275 (5296): 51–54. https://doi.org/10.1126/science.275.5296.51.\n\n\nHutter, Frank, Lin Xu, Holger H. Hoos, and Kevin Leyton-Brown. 2014. “Algorithm Runtime Prediction: Methods & Evaluation.” Artificial Intelligence 206: 79–111. https://doi.org/https://doi.org/10.1016/j.artint.2013.10.003.\n\n\nKadioglu, Serdar, Yuri Malitsky, Ashish Sabharwal, Horst Samulowitz, and Meinolf Sellmann. 2011. “Algorithm selection and scheduling.” Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 6876 LNCS: 454–69. https://doi.org/10.1007/978-3-642-23786-7_35.\n\n\nKashgarani, Haniye, and Lars Kotthoff. 2021. “Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution.” In AAAI Workshop on Meta-Learning and MetaDL Challenge, 140:58–64. Proceedings of Machine Learning Research. PMLR. https://proceedings.mlr.press/v140/kashgarani21a.html.\n\n\n———. 2023. “Automatic Parallel Portfolio Selection.” In ECAI 2023, 1215–22. IOS Press.\n\n\nKerschke, Pascal, Holger H. Hoos, Frank Neumann, and Heike Trautmann. 2019. “Automated Algorithm Selection: Survey and Perspectives.” Evolutionary Computation 27 (1): 3–45. https://doi.org/10.1162/evco_a_00242.\n\n\nKerschke, Pascal, Lars Kotthoff, Jakob Bossek, Holger H. Hoos, and Heike Trautmann. 2018. “Leveraging TSP Solver Complementarity through Machine Learning.” Evolutionary Computation 26 (4): 597–620. https://doi.org/10.1162/evco_a_00215.\n\n\nKotthoff, Lars. 2013. “LLAMA: Leveraging Learning to Automatically Manage Algorithms.” CoRR abs/1306.1031. http://arxiv.org/abs/1306.1031.\n\n\n———. 2014. “Algorithm selection for combinatorial search problems: A survey.” AI Magazine 35 (3): 48–69.\n\n\nLindauer, Marius, Rolf-David Bergdoll, and Frank Hutter. 2016. “An Empirical Study of Per-Instance Algorithm Scheduling.” In Proceedings of the Tenth International Conference on Learning and Intelligent Optimization, LION’16, in: Lecture Notes in Computer Science, 253–59. Springer; Springer.\n\n\nLindauer, Marius, Holger H Hoos, Frank Hutter, and Torsten Schaub. 2015. “Autofolio: An Automatically Configured Algorithm Selector.” Journal of Artificial Intelligence Research 53: 745–78.\n\n\nLindauer, Marius, Holger Hoos, and Frank Hutter. 2015. “From sequential algorithm selection to parallel portfolio selection.” In International Conference on Learning and Intelligent Optimization, 1–16. Springer.\n\n\nLindauer, Marius, Holger Hoos, Kevin Leyton-Brown, and Torsten Schaub. 2017. “Automatic Construction of Parallel Portfolios via Algorithm Configuration.” Artificial Intelligence 244: 272–90. https://doi.org/https://doi.org/10.1016/j.artint.2016.05.004.\n\n\nLindauer, Marius, Jan N. van Rijn, and Lars Kotthoff. 2017. “Open Algorithm Selection Challenge 2017: Setup and Scenarios.” In Proceedings of the Open Algorithm Selection Challenge, 79:1–7. PMLR. https://proceedings.mlr.press/v79/lindauer17a.html.\n\n\nMalitsky, Yuri, Ashish Sabharwal, Horst Samulowitz, and Meinolf Sellmann. 2012. “Parallel SAT Solver Selection and Scheduling.” In Proceedings of the 18th International Conference on Principles and Practice of Constraint Programming - Volume 7514, 512–26. Springer-Verlag.\n\n\nMaratea, Marco, Luca Pulina, and Francesco Ricca. 2014. “A Multi-Engine Approach to Answer-Set Programming.” Theory and Practice of Logic Programming 14 (6): 841–68.\n\n\nMaturana, Jorge, Álvaro Fialho, Frédéric Saubion, Marc Schoenauer, Frédéric Lardeux, and Michèle Sebag. 2012. “Adaptive Operator Selection and Management in Evolutionary Algorithms.” In Autonomous Search, 161–89. Springer. https://doi.org/10.1007/978-3-642-21434-9_7.\n\n\nO’Mahony, Eoin, Emmanuel Hebrard, Alan Holland, Conor Nugent, and Barry O’Sullivan. 2008. “Using Case-Based Reasoning in an Algorithm Portfolio for Constraint Solving.” In Irish Conference on Artificial Intelligence and Cognitive Science, 210–16. Proceedings of the 19th Irish Conference on Artificial Intelligence; Cognitive Science.\n\n\nPulatov, Damir, Marie Anastacio, Lars Kotthoff, and Holger Hoos. 2022. “Opening the Black Box: Automated Software Analysis for Algorithm Selection.” In Proceedings of the First International Conference on Automated Machine Learning, 188:6/1–18. PMLR. https://proceedings.mlr.press/v188/pulatov22a.html.\n\n\nRice, John R. 1976. “The Algorithm Selection Problem.” Advances in Computers. https://doi.org/10.1016/S0065-2458(08)60520-3.\n\n\nRoussel, Olivier. 2012. “Description of Ppfolio (2011).” Proc. SAT Challenge, 46.\n\n\nTorağay, Oğuz, and Shaheen Pouya. 2023. “A Monte Carlo simulation approach to the gap-time relationship in solving scheduling problem.” Journal of Turkish Operations Management 7 (1): 1579–90. https://doi.org/10.56554/jtom.1286288.\n\n\nWager, Stefan, Trevor Hastie, and Bradley Efron. 2014. “Confidence intervals for random forests: The jackknife and the infinitesimal jackknife.” The Journal of Machine Learning Research 15 (1): 1625–51.\n\n\nWright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” Journal of Statistical Software 77 (1): 1–17. https://doi.org/10.18637/jss.v077.i01.\n\n\nXu, Lin, Holger H. Hoos, and Kevin Leyton-Brown. 2010. “Hydra: Automatically Configuring Algorithms for Portfolio-Based Selection.” In Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, 210–16. AAAI’10 1. AAAI Press.\n\n\nXu, Lin, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2008. “SATzilla: Portfolio-Based Algorithm Selection for SAT.” J. Artif. Int. Res. 32 (1): 565–606.\n\n\nXu, Lin, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. 2011. “Hydra-MIP: Automated algorithm configuration and selection for mixed integer programming.” In Proceedings of the 18th RCRA Workshop, 16–30.\n\n\nYuen, Shiu Yin, Chi Kin Chow, and Xin Zhang. 2013. “Which algorithm should I choose at any point of the search: An evolutionary portfolio approach.” In GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference, 567–74. https://doi.org/10.1145/2463372.2463435.\n\n\nYuen, Shiu Yin, Yang Lou, and Xin Zhang. 2019. “Selecting Evolutionary Algorithms for Black Box Design Optimization Problems.” Soft Computing 23 (15): 6511–31.\n\n\nYun, Xi, and Susan L. Epstein. 2012. “Learning Algorithm Portfolios for Parallel Execution.” In Revised Selected Papers of the 6th International Conference on Learning and Intelligent Optimization - Volume 7219, 323–38. LION 6. Paris, France: Springer-Verlag.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Automatic Parallel Portfolio Selection</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5/RevisitingParallelPortfolioSelectionwithKLDivergence.html",
    "href": "chapters/chapter5/RevisitingParallelPortfolioSelectionwithKLDivergence.html",
    "title": "6  Revisiting Parallel Portfolio Selection with KL Divergence",
    "section": "",
    "text": "6.1 Introduction\nAlgorithms designed to solve combinatorial problems often exhibit complementary performance across different problem instances. Therefore, using a portfolio of algorithms frequently demonstrates superior performance compared to selecting the single best solver (SBS) averaged across all instances (Huberman, Lukose, and Hogg 1997; Gomes and Selman 2001). Portfolios can either be run in parallel, or a single algorithm can be selected on an instance-by-instance basis by training performance models using machine learning algorithms. However, both methods have drawbacks.\nAlgorithm selection has proven to be effective in solving different problems such as SAT, constraint programming, and mixed integer programming, as demonstrated by systems such as SATzilla, Hydra, and AutoFolio (Xu et al. 2008, 2011; Lindauer et al. 2015; O’Mahony et al. 2008). In single algorithm selection, if machine learning models are not well generalized, they might not select the correct best algorithm for a given instance. Although executing the whole portfolio of algorithms seems to avoid this issue, the more solvers that perform computations in parallel, the more time-out computations we will encounter (Kashgarani and Kotthoff 2021; Lindauer et al. 2017). However, the proposed parallel portfolio approaches often simulate parallel execution based on sequential data, which conceals the significant overhead and performance drop that occurs when many algorithms run parallel.\nBased on the results presented in the third chapter, even with a small number of solvers, selecting a single algorithm using the imperfect regression random forest ML model can outperform parallel portfolios (Kashgarani and Kotthoff 2021). In Chapter 4, we proposed a hybrid approach that leverages both algorithm selection and parallel execution. We introduced a middle-path strategy that identifies the most promising subset of algorithms to run simultaneously on a single non-distributed computer (Kashgarani and Kotthoff 2023). This innovative method demonstrated improved performance by utilizing three regression random forest algorithm selectors with different implementations and uncertainty estimation methods.\nUsing the method proposed in the previous chapter, it is possible to select an instance-based subportfolio of solvers to run in parallel, avoiding the drawbacks of algorithm selection and reducing the overhead associated with running too many solvers simultaneously. This method can achieve optimal or near-optimal performance, provided the virtual best solver is included in the selected subset of algorithms. We used the estimated uncertainty of the predictions while considering the impact of overhead from running the portfolio in parallel.\nHowever, this strategy still has some limitations. Specifically, the threshold value \\(p_{\\cap}\\)–which is defined as the threshold for the joint probability between the prediction distributions of the minimum predicted algorithm and other algorithms, and serving as a measure of the likelihood that an algorithm is predicted to perform very closely to the minimum predicted algorithm—could not be generalized across all scenarios and algorithm performance models, as the tuned values varied significantly. Here, we aim to provide an alternative formulation for subportfolio selection that overcomes this limitation.\nIn this chapter, we revisit the method of selecting the optimal parallel subportfolio of algorithms to run on a single computing machine. Similar to the main proposed method, we incorporate the uncertainty of the performance predictions. Here, rather than using the threshold \\(p_\\cap\\) in Equation [eq:7] as an estimate of the probability that the algorithm is as good as the best predicted algorithm, we investigated the use of the Kullback–Leibler (KL) divergence method, which measures the difference between the probability distributions of the predicted algorithms. This method provides an understanding of the differences between algorithm predictions, in contrast to the joint probability approach, which focused on the similarity of predictions. This enables a redefined selection criterion based on the divergence from the best-predicted performance distribution.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Revisiting Parallel Portfolio Selection with KL Divergence</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5/RevisitingParallelPortfolioSelectionwithKLDivergence.html#revisit-parallel-portfolio-selection",
    "href": "chapters/chapter5/RevisitingParallelPortfolioSelectionwithKLDivergence.html#revisit-parallel-portfolio-selection",
    "title": "6  Revisiting Parallel Portfolio Selection with KL Divergence",
    "section": "6.2 Revisit Parallel Portfolio Selection",
    "text": "6.2 Revisit Parallel Portfolio Selection\nWe aim to select a subset of solvers $ P_i S $ for a given instance \\(i \\in I\\), prioritizing algorithms predicted to perform best (\\(A \\in S\\) and \\(A \\in P_i\\)) based on their predicted performance \\(\\hat{m}\\). For each instance, a total ranking of algorithms in the portfolio \\(S\\) is established using their predicted performance:\n\\[A &lt; B \\quad \\text{if} \\quad \\hat{m}(A, i) &lt; \\hat{m}(B, i); A, B \\in S\\]\nFrom this ranking, the rank \\(r_{A, i}\\) is assigned to each algorithm \\(A\\), representing the number of algorithms predicted to outperform \\(A\\) on instance \\(i\\). A portfolio of size \\(n\\) is then defined by the top \\(n\\) ranked algorithms:\n\\[P_i = \\{A \\in S \\: | \\: r_{A,i} \\leq n\\}\\]\nThis method allows the selection of a subset of solvers for parallel execution, balancing the likelihood of including the best-performing solver with the overhead of running multiple solvers. However, the critical challenge in parallel portfolios is determining the appropriate portfolio size \\(n\\) for each problem instance. To address this balance, we incorporate the predicted performance distribution of algorithms and their associated uncertainty.\nSimilar to the proposed method in Chapter 4, instead of considering only a point prediction, we consider the predicted distribution of performance metric values, characterized by its mean and standard deviation. Formally, we denote the standard deviation of the prediction \\(\\hat{m}(A, i)\\) as \\(\\sigma_{A, i}\\) for each solver \\(A\\) and instance \\(i\\). We assume that the predictions of our performance models follow a normal distribution, i.e. the predicted value is the mean of that distribution, and allow us to characterize it completely together with the standard deviation. In the previous approach, we assess the likelihood that two algorithms perform equally well by calculating the overlap area between their prediction distributions. If two algorithms are predicted to perform very similarly, then the overlap area between the distributions will be very large. Here, we replace this method by considering the Kullback–Leibler (KL) divergence between the two univariate Gaussian distributions. KL divergence captures the divergence in shape and spread between the distributions.\nWe are in particular interested in the predicted performance distribution of the best-predicted algorithm \\(A_{1,i}\\) (no algorithms are predicted to perform better than it), and how the predictions for the other algorithms compare to it. Formally, for the best predicted solver \\(A_{1,i}\\) on instance \\(i\\) the distribution of predictions is $ (A_{1,i}, i) ({A{1,i},i}, ^2_{A_{1,i},i}) $ with probability density function $ f_{A_{1,i},i}$ and cumulative distribution function $ F_{A_{1,i},i}$. The performance distributions for other algorithms are defined similarly.\nThe Kullback–Leibler (KL) divergence is a statistical metric used to quantify the difference between two probability distributions (Kullback and Leibler 1951). According to the formulation in (Bishop and Nasrabadi 2006), given two distributions \\(p\\) and \\(q\\) with probability density functions \\(p(x)\\) and \\(q(x)\\), the KL divergence is calculated as:\n\\[\\label{eq:5.4}\n    KL(p \\| q) = - \\int p(x) \\log q(x) \\, dx + \\int p(x) \\log p(x) \\, dx\\]\nIn our context, we are interested in comparing the predicted performance distributions of two algorithms, \\(A_{x}\\) and \\(A_{y}\\), on a specific instance \\(i\\). Let \\(f_{A_{x},i}\\) and \\(f_{A_{y},i}\\) denote the probability density functions of the predicted performance of algorithms \\(A_{x}\\) and \\(A_{y}\\) on instance \\(i\\), respectively. By substituting \\(p(x)\\) with \\(f_{A_{x},i}\\) and \\(q(x)\\) with \\(f_{A_{y},i}\\), we adapt the KL divergence to quantify the difference in predicted performance between the two algorithms. Thus, the KL divergence between the performance distributions of \\(A_{x}\\) and \\(A_{y}\\) in instance \\(i\\) is computed as follows:\n\\[\\label{eq:5.5} KL(f_{A_{x},i} \\| f_{A_{y},i}) = - \\int f_{A_{x},i}(x) \\log f_{A_{y},i}(x) dx + \\int f_{A_{x},i}(x) \\log f_{A_{x},i}(x) dx\\]\nThis formulation indicates to what extent the probability distributions differ. Since the two distributions are univariate Gaussians, the exact formula for KL divergence is as follows (we omit the index \\(i\\) for the sake of brevity here):\n\\[\\label{eq:5.6}\n    KL(f_{A_x} \\| f_{A_y}) = \\log \\frac{\\sigma_{A_y}}{\\sigma_{A_x}} + \\frac{\\sigma_{A_x}^2 + (\\mu_{A_x} - \\mu_{A_y})^2}{2 \\sigma_{A_y}^2} - \\frac{1}{2}\\]\nWe define \\(kl \\in [0, \\infty)\\) as a threshold for the computed KL divergence to include a given algorithm:\n\\[\\label{eq:5.7}\nP_i = \\{A \\:| \\:  KL(f_{A_{1,i},i} \\| f_{A_{x,i},i}) \\leq \\\\kl\\:\\}\\]\n\\(kl\\) is 0 for the best predicted algorithm. In contrast to \\(p_{\\cap}\\), which could only be in the range of [0,1], the value of \\(kl\\) can be greater than 1. A very large value of \\(kl\\) corresponds to algorithms whose distributions diverge the most from that of the best predicted algorithm, that is, algorithms with performance predictions that are markedly different from those of the best predicted algorithm.\nWe can adjust the size of the parallel portfolio by modifying the \\(kl\\) threshold. When \\(kl\\) is set to 0, only the best predicted algorithm and those expected to perform identically are included. Setting \\(kl\\) to a very large positive value allows all algorithms to be included. Finding the optimal \\(kl\\) is necessary to determine how many solvers to include in the portfolio. This flexibility enables us to tailor the approach to specific algorithm selection scenarios, allowing the selection of algorithms to run in parallel and accommodating any potential inaccuracies in performance predictions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Revisiting Parallel Portfolio Selection with KL Divergence</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5/RevisitingParallelPortfolioSelectionwithKLDivergence.html#experimental-setup",
    "href": "chapters/chapter5/RevisitingParallelPortfolioSelectionwithKLDivergence.html#experimental-setup",
    "title": "6  Revisiting Parallel Portfolio Selection with KL Divergence",
    "section": "6.3 Experimental Setup",
    "text": "6.3 Experimental Setup\n\n6.3.1 Data Collection\nWe used the same five scenarios as in the previous chapter (Kashgarani and Kotthoff 2023), now included in the ASlib benchmark repository (Bischl, Kerschke, et al. 2016): MAXSAT19-UCMS, SAT11-INDU, SAT18-EXP, SAT16-MAIN, and IPC2018. These datasets include algorithm performance data from single and parallel runs, with parallel run measurements conducted on individual machines as described in (Kashgarani and Kotthoff 2023). Feature extraction was performed using the SATZilla feature extraction code for MAXSAT19-UCMS, SAT11-INDU, SAT16-MAIN, and SAT18-EXP, producing 54 features, while IPC2018 features were extracted using the code from (Fawcett et al. 2014), resulting in 305 features.\n\n\n6.3.2 Training\nWe used the same random forest regression models from Chapter 4. The random forest regression models are trained in three ways: one using the randomForest package in R and two using the Ranger package, to predict algorithm performance on specific instances. Random forests are generally recognized for their strong performance in algorithm selection and performance prediction (Bischl, Kerschke, et al. 2016; Hutter et al. 2014). Given the existence of two distinct implementations, we trained the models using both randomForest and Ranger implementations.\nOur regression random forest models are built using the MLR package with the randomForest package as dependency, and the Ranger models are trained using the MLR3 and Ranger implementations. These models predict the runtime for each solver as the mean of the underlying distribution and estimate the standard deviation. The initial random forest model and one of the Ranger models use the Jackknife method (Wager, Hastie, and Efron 2014; Bischl, Lang, et al. 2016). The Jackknife method estimates the standard deviation of the mean predictions in all observations used to train the random forest. This technique involves training the random forest model on \\(n-1\\) observations, leaving one out each time to make a prediction, and repeating this for each observation. The mean prediction for each tree is calculated by averaging its predictions on the left-out data points. The Jackknife method assumes that predictions follow a normal distribution, with the standard deviation indicating the uncertainty of the overall prediction. The infinitesimal jackknife method assesses the impact of each observation by slightly down-weighting it, unlike the traditional jackknife, which removes one observation at a time.\nOur setup closely follows the approach in (Bischl, Kerschke, et al. 2016) for all three models: we excluded instance features with constant values and imputed missing feature values by using the mean of all nonmissing values for each feature. The random forest hyperparameters were tuned through random search with 250 iterations, where \\(ntree\\) was varied from 10 to 200 and \\(mtry\\) from 1 to 30, using nested cross-validation with three inner folds and 10 outer folds (Bischl, Kerschke, et al. 2016).\n\n\n6.3.3 Tuning \\(kl\\) and \\(p_{\\cap}\\)\n\n\nOptimum value of \\(kl\\) for each benchmark and model.\n\n\nScenario\nRandomForest_Jackknife\nRanger_Jackknife\nRanger_Inifinitesimal\n\n\n\n\n\nIPC2018\n0.71\n2.35\n2.39\n\n\n\nMAXSAT19-UCMS\n0.63\n2.7\n2.7\n\n\n\nSAT11-INDU\n0.55\n1.62\n1.94\n\n\n\nSAT16-MAIN\n2.66\n2.06\n2.96\n\n\n\nSAT18-EXP\n0.12\n1.81\n1.35\n\n\n\nGeneric best\n0.41\n2.65\n2.82\n\n\n\n\n\nThe tuned \\(p_{\\cap}\\) value for each benchmark and each random forest model is listed in Table [tab:pcap] and we are using the same values in this chapter. These values were individually optimized for each scenario to ensure that the selected portfolio provided the best balance between performance and computational efficiency. For tuning \\(kl\\), we perform a grid search to determine the optimal value in Equation [eq:5.7] for each scenario. The search is carried out over the interval \\([0, 3)\\) with a resolution \\(0.01\\), resulting in 300 possible values.\nThe tuned \\(p_{\\cap}\\) value for each benchmark and each random forest model is listed in Table [tab:pcap], and we use the same values in this chapter. These values were individually optimized for each scenario to ensure that the selected portfolio provided the best balance between performance and computational efficiency. For tuning \\(kl\\), we perform a grid search to determine the optimal value in Equation [eq:5.7] for each scenario. The search is carried out over the interval \\([0, 3)\\) with a resolution of 0.01, resulting in 300 possible values.\n\n\n6.3.4 Baselines\nFor all the comparisons mentioned, we evaluate the performance of our approaches against several baseline methods. Specifically, we compare to the sequential virtual best solver (VBS), which picks the best solver for each problem instance with a cumulative misclassification penalty of zero, and to the sequential single best solver (SBS), which is the solver with the best average performance across all instances and a cumulative misclassification penalty of one. For parallel runs, the VBS is the best solver for each instance but includes the overhead for \\(n\\) parallel runs. The parallel SBS is determined similarly, using the solvers with the best average performance instead of the best for each instance. We executed multiple solvers in parallel to capture the real run-time of the best solver in this setup, instead of assuming that it would run sequentially.\nWe have three algorithm selectors: RFJ (random forest with Jackknife), RJ (Ranger with Jackknife), and RI (Ranger with infinitesimal Jackknife). Each algorithm selector has five approaches for comparison. The first involves selecting algorithms on a per-instance basis, running the top \\(n\\) predicted algorithms in parallel without accounting for any uncertainty the subportfolio selection approaches. Using the notation introduced in the previous chapter, we assign \\(p_{\\cap}=0\\) and limit the number of runs to match the available processors. Also, when we assign \\(kl = \\infty\\), we are doing the same thing and limiting the number of runs to match the available processors. The second approach uses the associated tuned \\(p_{\\cap}\\) values mentioned in Table [tab:pcap]. The third approach uses the average \\(p_{\\cap}\\) value, which was the best generic value across all scenarios. Other approach uses the associated tuned \\(kl\\) values mentioned in Table 1.1 for each scenario and performance model. The last approach uses the generic best \\(kl\\) value across all scenarios for each performance model.\nWe evaluate the proposed method by measuring the penalized average runtime with a factor of 10 (PAR10), the misclassification penalty (MCP), and the runtime. The PAR10 metric equals the actual runtime if the algorithm successfully solves the instance within the timeout; otherwise, it is calculated as the timeout multiplied by 10. The MCP represents the difference between the performance of the selected algorithm and that of the optimal algorithm. The mean and standard deviation of these values are presented in Tables 1.2,1.3, and1.4. We normalize PAR10 values across scenarios using the performances of the VBS and SBS, reporting the proportion of the performance gap each approach bridges. On this normalized scale, 0 denotes the performance of the SBS, while 1 denotes the performance of the VBS.\nIn the tables, we report the mean and standard deviation of the normalized gap closed across the 10 folds of data used to train the performance models. In contrast to the reported runtime, MCP, and PAR10 scores, where we present the mean and standard deviation across the distribution of all instances. We use folds here to avoid zero denominators in cases where the single best solver is the actual best solver for an instance, based on the normalized gap closed formula \\(\\frac{\\text{sbs} - \\text{approach}}{\\text{sbs} - \\text{vbs}}\\). The plots [fig:all_results], show the PAR10 score normalized gap closed over the entire distribution of instances.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Revisiting Parallel Portfolio Selection with KL Divergence</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5/RevisitingParallelPortfolioSelectionwithKLDivergence.html#results",
    "href": "chapters/chapter5/RevisitingParallelPortfolioSelectionwithKLDivergence.html#results",
    "title": "6  Revisiting Parallel Portfolio Selection with KL Divergence",
    "section": "6.4 Results",
    "text": "6.4 Results\n\n6.4.1 Tuning of \\(kl\\) and \\(p_{\\cap}\\)\nTuning \\(p_{\\cap}\\) and \\(kl\\) reveals that the optimal values vary by scenario. Tables [tab:pcap] and 1.1 present the optimal values for each scenario and algorithm selector. We presented Figure 1.1, which shows the normalized gap closed for the mean, 25th percentile, 50th percentile, and 75th percentile across each scenario based on \\(kl\\) value. Although optimal values vary significantly between scenarios and performance models, normalized gaps closed remain relatively small as long as \\(kl\\) is not too low. For the most optimal performance, we recommend tuning \\(kl\\) for each specific scenario.\nThe optimal value of \\(kl\\), similar to \\(p_{\\cap}\\), provides insight into the predictive accuracy of the performance models. A large \\(kl\\) value suggests that the models’ predictions may lack accuracy, as it requires including solvers whose predicted runtime distributions differ significantly from that of the best predicted solver in order to capture solvers that perform well. If the optimal value of \\(kl\\) were 0, we would include only solvers that are exactly similar to the best-predicted solver. A very large optimal \\(kl\\) requires us to include all solvers, even those whose predicted distributions differ entirely from the best predicted solver.\nHere, the optimal values for \\(kl\\) are relatively small, typically up to 2.5 in most cases. The further the \\(kl\\) value deviates from 0, the lower the accuracy. Table 1.1 also shows that the RFJ model, which uses randomForest with a Jackknife uncertainty estimate, has a significantly lower optimal \\(kl\\) in most scenarios, suggesting that this model should outperform the other two when selecting single solver in 4 out of 5 scenarios in terms of prediction accuracy. This is confirmed by Tables 1.2, 1.3 and 1.4 which is denoted as \\(AS (RFJ)\\). The optimal \\(kl\\) values are closer for all scenarios when we have a single model. This seems to contrast with \\(p_{\\cap}\\), where different scenarios had significantly varying values per model. Here, the values are more consistent, suggesting that tuning this parameter per model should yield good performance.\n\n\n6.4.2 \\(AS_{p_{\\cap}}\\) vs. \\(AS_{kl}\\) Comparison\nTo evaluate the effectiveness of any of the approaches, we carried out a series of experiments using the optimum best value for \\(p_{\\cap}\\) and \\(kl\\) and \\(p_{\\cap} = 0\\) for each scenario, where we varied the number of processors used for parallel execution from one to ten for the scenarios SAT18-EXP, SAT16-MAIN, SAT11-INDU and IPC2018. For the MAXSAT19-UCMS scenario, we used a maximum of seven processors, as there are only seven algorithms. Figure 1.2 shows the PAR10 score results in terms of the normalized performance gap closed between the sequential single best solver and the sequential virtual best solver for all scenarios and number of processors. In addition, Tables 1.2, 1.3 and 1.4 show the mean and standard deviation values for runtime, MCP and PAR10 when limiting the maximum number of parallel runs to 10 or SAT18-EXP, SAT16-MAIN, SAT11-INDU and IPC2018, and to 7 for MAXSAT19-UCMS. In addition, we reported the mean and standard deviation of the normalized gap closed across folds in these tables. This differs from the plots, as the plots report values in terms of the mean of the problem distribution rather than across folds.\nHere, we discuss the results of the comparison of \\(AS_{p_{\\cap}}\\) with the \\(AS_{kl}\\) methods. For this comparison, we excluded the results of the RJ model in Figure 1.2 because, as mentioned in the previous section comparing ranger and randomForest, the RJ model did not outperform the other two models in subportfolio selection, while RI and RFJ were competitive in portfolio selection. This is also evident in Tables 1.2, 1.3, and 1.4 that the RJ model did not surpass the other two when using \\(AS_{p_{\\cap}}\\) and \\(AS_{kl}\\). Therefore, we discuss the results for the RI and RFJ models when selecting subportfolios using Equation [eq:7], denoted as \\(AS_{p_{\\cap}}\\), and Equation [eq:5.7], denoted as \\(AS_{kl}\\).\nWhen comparing \\(AS_{p_{\\cap}}\\) and \\(AS_{kl}\\) with optimal threshold values, based on Tables 1.2, 1.3, and 1.4, \\(AS_{kl}\\) outperforms \\(AS_{p_{\\cap}}\\) in three of four performance metrics for IPC2018 and the RFJ model. For the RI model, \\(AS_{p_{\\cap}}\\) is superior in three of the four metrics, while for the RJ model, \\(AS_{kl}\\) performs worse in all performance metrics. Figure 1.2 further illustrates that \\(AS_{p_{\\cap}}\\) consistently outperforms \\(AS_{kl}\\) for both the RFJ and RI models when the cores are limited to different values. For the MAXSAT19-UCMS and SAT11-INDU scenarios, \\(AS_{p_{\\cap}}\\) performs better than \\(AS_{kl}\\) in all performance metrics for the RJ, RFJ, and RI models, which is also reflected in the closed mean normalized gap in Figure 1.2. In the SAT16-MAIN scenario, \\(AS_{p_{\\cap}}\\) is overall superior, except for the RFJ model, where both methods perform equally, and the RJ model where the normalized gap closed for folds is slightly worse for \\(AS_{p_{\\cap}}\\). In the SAT18-EXP scenario, \\(AS_{p_{\\cap}}\\) generally outperforms \\(AS_{kl}\\), except for the RFJ model where the two are highly competitive. In three of four performance metrics, \\(AS_{kl}\\) is better, while \\(AS_{p_{\\cap}}\\) excels in the remaining metric.\nWhen comparing the RFJ and RI models for portfolio selection using the \\(AS_{kl}\\) method, the RFJ model performed consistently best in all scenarios. This contrasts with the \\(AS_{p_{\\cap}}\\) approach, where the RI model outperformed the RFJ model in two out of five scenarios. When comparing \\(AS_{p_{\\cap}}\\) and \\(AS_{kl}\\) using the generic best values across the tuned scenarios for each model, \\(AS_{p_{\\cap}}\\) outperforms \\(AS_{kl}\\) in two of the three models for IPC2018, MAXSAT19-UCMS and SAT16-MAIN. However, for the RFJ model in these scenarios, the trend is reversed, with \\(AS_{kl}\\) performing slightly worse. For SAT11-INDU and SAT18-EXP, \\(AS_{p_{\\cap}}\\) is the consistently better approach across all models.\nWhen comparing all the experimented methods in all scenarios with the number of parallel runs limited to 10, the \\(AS_{p_{\\cap}}\\) of the RI model delivered the best performance for IPC2018 and SAT11-INDU in terms of runtime, MCP, normalized gap, and PAR10. For these scenarios, \\(AS_{kl}\\) of the RI model was the second-best method. For MAXSAT19-UCMS, the \\(AS_{p_{\\cap}}\\) of the RFJ model achieved the best performance, followed by the \\(AS_{kl}\\) of the RFJ model as the second-best method. In SAT16-MAIN, \\(AS_{p_{\\cap}}\\) of the RI model, with \\(p_{\\cap}\\) set to zero and selecting the top 10 predicted algorithms, emerged as the best method. The second-best method in this scenario was the \\(AS_{p_{\\cap}}\\) of the RI model, using the generic best value for \\(p_{\\cap}\\). Finally, for SAT18-EXP, the \\(AS_{p_{\\cap}}\\) and \\(AS_{kl}\\) methods of the RFJ model performed very competitively, both achieving the best performance.\nAlthough the \\(AS_{p_\\cap}\\) method appears to be in general superior to \\(AS_{kl}\\), its performance is very close, making \\(AS_{kl}\\) the best alternative in the absence of \\(AS_{p_\\cap}\\). One significant advantage of \\(AS_{kl}\\) is that the tuned values of \\(kl\\) for different scenarios are highly consistent, and this suggests that tuning of \\(kl\\) globally across all scenarios can still produce good performance. This consistency reduces the cost of scenario-specific tuning. In contrast, the \\(p_{\\cap}\\) values vary significantly between different models, and it makes tuning more challenging and model-dependent. On the other hand, \\(kl\\) demonstrates greater consistency, with its best generic values for the RI and RJ models being close, which further highlights its practicality for streamlined optimization.\n\n\n\n\n\n\n\nSensitivity of Portfolio Performance to (kl). The top left plot corresponds to the RFJ model, and the top right plot corresponds to the RI model, and the bottom plot is for RJ model. The plot displays the mean, first quartile (Q1, 25th percentile), median (Q2, 50th percentile), and third quartile (Q3, 75th percentile) runtime performance for each scenario across different (kl) values, as defined in Equation [eq:5.7]. The y-axis uses a log scale to represent the normalized gap closed, highlighting variations in performance sensitivity relative to (kl) adjustments.\n\n\n\n\n\nResults Overview. The plot illustrates the extent to which each method narrows the gap between the PAR10 scores of the Single Best Solver (SBS) and the Virtual Best Solver (VBS). For VBS and SBS, the top (n) solvers are selected, where (n) matches the number of processors available for each problem instance and across all instances, respectively. (AS_{p_{}}) and (AS_{kl}) follow the approaches proposed in  and Equation [eq:5.7], respectively, with the number of processors capped at the specific value on the x-axis — fewer solvers than this maximum may be selected based on the overlap and divergence in runtime predictions. The RFJ model is trained with the randomForest and Jackknife method, RI uses the Ranger model with the Infinitesimal Jackknife method. The optimal (p_{}) and (kl) values for each scenario and each model are listed in Tables [tab:pcap] and 1.1.\n\n\n\n\n\nDetailed results. Mean and standard deviation of values for runtime, MCP, and PAR10 across all problem instances in a scenario for the sequential virtual best solver, sequential single best solver, and single top predicted algorithm in the initial three rows. The second set of rows for each scenario shows the results for the maximum number of processors (10 for IPC2018, and 7 for MAXSAT19-UCMS) for our approaches and the baselines we compare to. All numbers were rounded to integers. The best value for each scenario and measure is shown in bold (excepting the sequential VBS, which is by definition always the best), the second best in italics. The normalized gap closed represents the mean and standard deviation of the normalized gap closed across the folds.\n\n\nScenario\nApproach\nRuntime [s]\nMCP\nPAR10\nNormalizedGap\n\n\n\n\n\n1 Processor\n\n\n\n\n\n\n\nVBS\n508\\(\\pm\\)697\n0\n3478\\(\\pm\\)6903\n1\n\n\n\nAS (RFJ)\n607\\(\\pm\\)751\n99\\(\\pm\\)301\n4657\\(\\pm\\)7725\n-0.44\\(\\pm\\)2.84\n\n\n\nAS (RI)\n608\\(\\pm\\)751\n100\\(\\pm\\)293\n4456\\(\\pm\\)7583\n-0.35\\(\\pm\\)2.85\n\n\n\nAS (RJ)\n604\\(\\pm\\)752\n96\\(\\pm\\)293\n4519\\(\\pm\\)7633\n-0.39\\(\\pm\\)2.84\n\n\n\nSBS\n734\\(\\pm\\)770\n226\\(\\pm\\)414\n5459\\(\\pm\\)8072\n0\n\n\n\n10 Processors\n\n\n\n\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RFJ)\n612\\(\\pm\\)779\n104\\(\\pm\\)307\n5134\\(\\pm\\)8027\n-0.66\\(\\pm\\)2.56\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RI)\n616\\(\\pm\\)783\n107\\(\\pm\\)312\n5206\\(\\pm\\)8065\n-0.69\\(\\pm\\)2.54\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RJ)\n616\\(\\pm\\)783\n107\\(\\pm\\)312\n5206\\(\\pm\\)8065\n-0.69\\(\\pm\\)2.54\n\n\n\n\\(AS_{p_{\\cap} = 0.59}\\) (RFJ)\n569\\(\\pm\\)745\n61\\(\\pm\\)223\n4484\\(\\pm\\)7651\n-0.18\\(\\pm\\)2.74\n\n\n\n\\(AS_{p_{\\cap} = 0.44}\\) (RI)\n557\\(\\pm\\)728\n49\\(\\pm\\)190\n4135\\(\\pm\\)7403\n-0.19\\(\\pm\\)2.89\n\n\n\n\\(AS_{p_{\\cap} = 0.27}\\) (RJ)\n570\\(\\pm\\)744\n62\\(\\pm\\)229\n4350\\(\\pm\\)7552\n-0.21\\(\\pm\\)2.72\n\n\n\n\\(AS_{p_{\\cap} = 0.82}\\) (RFJ)\n579\\(\\pm\\)742\n70\\(\\pm\\)233\n4359\\(\\pm\\)7548\n-0.26\\(\\pm\\)2.88\n\n\n\n\\(AS_{p_{\\cap} = 0.17}\\) (RI)\n570\\(\\pm\\)739\n62\\(\\pm\\)230\n4283\\(\\pm\\)7501\n-0.24\\(\\pm\\)2.87\n\n\n\n\\(AS_{p_{\\cap} = 0.31}\\) (RJ)\n570\\(\\pm\\)743\n62\\(\\pm\\)229\n4350\\(\\pm\\)7552\n-0.21\\(\\pm\\)2.72\n\n\n\n\\(AS_{kl = 0.71} (RFJ)\\)\n560\\(\\pm\\)735\n52\\(\\pm\\)193\n4272\\(\\pm\\)7506\n-0.19\\(\\pm\\)2.71\n\n\n\n\\(AS_{kl = 2.39} (RI)\\)\n570\\(\\pm\\)740\n62\\(\\pm\\)224\n4350\\(\\pm\\)7552\n-0.3\\(\\pm\\)2.86\n\n\n\n\\(AS_{kl = 2.35} (RJ)\\)\n577\\(\\pm\\)750\n70\\(\\pm\\)243\n4492\\(\\pm\\)7647\n-0.31\\(\\pm\\)2.67\n\n\n\n\\(AS_{kl = 0.41} (RFJ)\\)\n571\\(\\pm\\)738\n63\\(\\pm\\)220\n4351\\(\\pm\\)7551\n-0.23\\(\\pm\\)2.73\n\n\n\n\\(AS_{kl = 2.82} (RI)\\)\n575\\(\\pm\\)746\n67\\(\\pm\\)244\n4423\\(\\pm\\)7599\n-0.32\\(\\pm\\)2.85\n\n\n\n\\(AS_{kl = 2.65} (RJ)\\)\n577\\(\\pm\\)750\n70\\(\\pm\\)243\n4492\\(\\pm\\)7647\n-0.31\\(\\pm\\)2.67\n\n\n\n1 Processor\n\n\n\n\n\n\n\nVBS\n858\\(\\pm\\)1476\n0\n7768\\(\\pm\\)14717\n1\n\n\n\nAS (RFJ)\n1037\\(\\pm\\)1555\n179\\(\\pm\\)641\n9363\\(\\pm\\)15684\n0.55\\(\\pm\\)0.28\n\n\n\nAS (RI)\n1076\\(\\pm\\)1575\n218\\(\\pm\\)729\n9686\\(\\pm\\)15850\n0.45\\(\\pm\\)0.34\n\n\n\nAS (RJ)\n1044\\(\\pm\\)1565\n186\\(\\pm\\)666\n9540\\(\\pm\\)15793\n0.49\\(\\pm\\)0.23\n\n\n\nSBS\n1190\\(\\pm\\)1657\n332\\(\\pm\\)940\n11386\\(\\pm\\)16696\n0\n\n\n\n7 Processors\n\n\n\n\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RFJ)\n894\\(\\pm\\)1506\n37\\(\\pm\\)247\n8258\\(\\pm\\)15062\n0.85\\(\\pm\\)0.16\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RI)\n894\\(\\pm\\)1506\n37\\(\\pm\\)247\n8258\\(\\pm\\)15062\n0.85\\(\\pm\\)0.16\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RJ)\n894\\(\\pm\\)1506\n37\\(\\pm\\)247\n8258\\(\\pm\\)15062\n0.85\\(\\pm\\)0.16\n\n\n\n\\(AS_{p_{\\cap} = {0.55}}\\) (RFJ)\n891\\(\\pm\\)1496\n33\\(\\pm\\)215\n8141\\(\\pm\\)14975\n0.88\\(\\pm\\)0.17\n\n\n\n\\(AS_{p_{\\cap} = 0.03}\\) (RI)\n894\\(\\pm\\)1506\n37\\(\\pm\\)247\n8258\\(\\pm\\)15062\n0.85\\(\\pm\\)0.16\n\n\n\n\\(AS_{p_{\\cap} = 0.14}\\) (RJ)\n921\\(\\pm\\)1521\n63\\(\\pm\\)369\n8568\\(\\pm\\)15263\n0.76\\(\\pm\\)0.24\n\n\n\n\\(AS_{p_{\\cap} = 0.82}\\) (RFJ)\n928\\(\\pm\\)1513\n70\\(\\pm\\)364\n8461\\(\\pm\\)15175\n0.81\\(\\pm\\)0.18\n\n\n\n\\(AS_{p_{\\cap} = 0.17}\\) (RI)\n901\\(\\pm\\)1502\n43\\(\\pm\\)275\n8208\\(\\pm\\)15015\n0.88\\(\\pm\\)0.16\n\n\n\n\\(AS_{p_{\\cap} = 0.31}\\) (RJ)\n931\\(\\pm\\)1525\n73\\(\\pm\\)402\n8578\\(\\pm\\)15259\n0.78\\(\\pm\\)0.21\n\n\n\n\\(AS_{kl = 0.63} (RFJ)\\)\n892\\(\\pm\\)1495\n35\\(\\pm\\)216\n8143\\(\\pm\\)14974\n0.88\\(\\pm\\)0.17\n\n\n\n\\(AS_{kl = 2.7} (RI)\\)\n920\\(\\pm\\)1517\n63\\(\\pm\\)349\n8454\\(\\pm\\)15179\n0.82\\(\\pm\\)0.17\n\n\n\n\\(AS_{kl = 2.7} (RJ)\\)\n931\\(\\pm\\)1530\n74\\(\\pm\\)409\n8691\\(\\pm\\)15342\n0.74\\(\\pm\\)0.25\n\n\n\n\\(AS_{kl = 0.41} (RFJ)\\)\n915\\(\\pm\\)1513\n57\\(\\pm\\)335\n8448\\(\\pm\\)15182\n0.8\\(\\pm\\)0.19\n\n\n\n\\(AS_{kl = 2.82} (RI)\\)\n921\\(\\pm\\)1517\n63\\(\\pm\\)349\n8454\\(\\pm\\)15179\n0.82\\(\\pm\\)0.17\n\n\n\n\\(AS_{kl = 2.65} (RJ)\\)\n931\\(\\pm\\)1530\n74\\(\\pm\\)409\n8691\\(\\pm\\)15342\n0.74\\(\\pm\\)0.25\n\n\n\n\n\n\n\n\nDetailed results. Mean and standard deviation of values for runtime, MCP, and PAR10 across all problem instances in a scenario for the sequential virtual best solver, sequential single best solver, and single top predicted algorithm in the initial three rows. The second set of rows for each scenario shows the results for the maximum number of processors (10 for SAT16-MAIN and SAT11-INDU) for our approaches and the baselines we compare to. All numbers were rounded to integers. The best value for each scenario and measure is shown in bold (excepting the sequential VBS, which is by definition always the best), the second best in italics. The normalized gap closed represents the mean and standard deviation of the normalized gap closed across the folds.\n\n\nScenario\nApproach\nRuntime [s]\nMCP\nPAR10\nNormalizedGap\n\n\n\n\n\n1 Processor\n\n\n\n\n\n\n\nVBS\n1140\\(\\pm\\)1836\n0\n8040\\(\\pm\\)17905\n1\n\n\n\nAS (RFJ)\n1535\\(\\pm\\)2058\n395\\(\\pm\\)1037\n11735\\(\\pm\\)20768\n0.16\\(\\pm\\)0.79\n\n\n\nAS (RI)\n1610\\(\\pm\\)2108\n470\\(\\pm\\)1145\n12710\\(\\pm\\)21389\n-0.06\\(\\pm\\)0.9\n\n\n\nAS (RJ)\n1565\\(\\pm\\)2049\n425\\(\\pm\\)1017\n11315\\(\\pm\\)20402\n0.34\\(\\pm\\)0.49\n\n\n\nSBS\n1818\\(\\pm\\)2168\n678\\(\\pm\\)1340\n14268\\(\\pm\\)22154\n0\n\n\n\n10 Processors\n\n\n\n\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RFJ)\n1272\\(\\pm\\)1927\n161\\(\\pm\\)548\n8922\\(\\pm\\)18645\n0.89\\(\\pm\\)0.12\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RI)\n1238\\(\\pm\\)1892\n127\\(\\pm\\)385\n8588\\(\\pm\\)18350\n0.92\\(\\pm\\)0.1\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RJ)\n1262\\(\\pm\\)1910\n151\\(\\pm\\)480\n8612\\(\\pm\\)18342\n0.92\\(\\pm\\)0.1\n\n\n\n\\(AS_{p_{\\cap} = 0.63}\\) (RFJ)\n1241\\(\\pm\\)1901\n131\\(\\pm\\)451\n8591\\(\\pm\\)18349\n0.92\\(\\pm\\)0.1\n\n\n\n\\(AS_{p_{\\cap} = 0.01}\\) (RI)\n1236\\(\\pm\\)1890\n121\\(\\pm\\)379\n8586\\(\\pm\\)18351\n0.92\\(\\pm\\)0.1\n\n\n\n\\(AS_{p_{\\cap} = 0.31}\\) (RJ)\n1289\\(\\pm\\)1934\n178\\(\\pm\\)595\n9089\\(\\pm\\)18787\n0.78\\(\\pm\\)0.28\n\n\n\n\\(AS_{p_{\\cap} = 0.82}\\) (RFJ)\n1247\\(\\pm\\)1900\n123\\(\\pm\\)431\n8747\\(\\pm\\)18501\n0.83\\(\\pm\\)0.28\n\n\n\n\\(AS_{p_{\\cap} = 0.17}\\) (RI)\n1259\\(\\pm\\)1912\n139\\(\\pm\\)477\n8909\\(\\pm\\)18649\n0.74\\(\\pm\\)0.36\n\n\n\n\\(AS_{p_{\\cap} = 0.31}\\) (RJ)\n1289\\(\\pm\\)1934\n178\\(\\pm\\)595\n9089\\(\\pm\\)18787\n0.78\\(\\pm\\)0.28\n\n\n\n\\(AS_{kl = 0.55} (RFJ)\\)\n1243\\(\\pm\\)1902\n132\\(\\pm\\)450\n8593\\(\\pm\\)18349\n0.92\\(\\pm\\)0.1\n\n\n\n\\(AS_{kl = 1.62} (RI)\\)\n1283\\(\\pm\\)1931\n158\\(\\pm\\)539\n9233\\(\\pm\\)18936\n0.68\\(\\pm\\)0.34\n\n\n\n\\(AS_{kl = 1.94} (RJ)\\)\n1307\\(\\pm\\)1950\n194\\(\\pm\\)622\n9407\\(\\pm\\)19072\n0.73\\(\\pm\\)0.26\n\n\n\n\\(AS_{kl = 0.41} (RFJ)\\)\n1250\\(\\pm\\)1910\n137\\(\\pm\\)465\n8750\\(\\pm\\)18501\n0.89\\(\\pm\\)0.12\n\n\n\n\\(AS_{kl = 2.82} (RI)\\)\n1288\\(\\pm\\)1939\n166\\(\\pm\\)550\n9238\\(\\pm\\)18934\n0.68\\(\\pm\\)0.34\n\n\n\n\\(AS_{kl = 2.65} (RJ)\\)\n1310\\(\\pm\\)1953\n198\\(\\pm\\)626\n9410\\(\\pm\\)19071\n0.73\\(\\pm\\)0.26\n\n\n\n1 Processor\n\n\n\n\n\n\n2-6\nVBS\n1867\\(\\pm\\)2193\n0\n15005\\(\\pm\\)22530\n1\n\n\n\nAS (RFJ)\n2315\\(\\pm\\)2273\n448\\(\\pm\\)1109\n19066\\(\\pm\\)23883\n0.33\\(\\pm\\)0.56\n\n\n\nAS (RI)\n2383\\(\\pm\\)2294\n516\\(\\pm\\)1151\n19956\\(\\pm\\)24111\n0.05\\(\\pm\\)0.66\n\n\n\nAS (RJ)\n2400\\(\\pm\\)2269\n533\\(\\pm\\)1177\n19316\\(\\pm\\)23880\n0.3\\(\\pm\\)0.24\n\n\n\nSBS\n2560\\(\\pm\\)2294\n693\\(\\pm\\)1415\n21940\\(\\pm\\)24464\n0\n\n\n2-6\n10 Processors\n\n\n\n\n\n\n2-6\n\\(AS_{p_{\\cap} = 0}\\) (RFJ)\n2065\\(\\pm\\)2221\n198\\(\\pm\\)652\n16189\\(\\pm\\)22931\n0.7\\(\\pm\\)0.39\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RI)\n2016\\(\\pm\\)2225\n150\\(\\pm\\)503\n16469\\(\\pm\\)23122\n0.68\\(\\pm\\)0.6\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RJ)\n2048\\(\\pm\\)2228\n181\\(\\pm\\)597\n16336\\(\\pm\\)23023\n0.68\\(\\pm\\)0.59\n\n\n\n\\(AS_{p_{\\cap} = 0.33}\\) (RFJ)\n2065\\(\\pm\\)2221\n198\\(\\pm\\)652\n16189\\(\\pm\\)22931\n0.7\\(\\pm\\)0.39\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RI)\n2016\\(\\pm\\)2225\n150\\(\\pm\\)503\n16469\\(\\pm\\)23122\n0.68\\(\\pm\\)0.6\n\n\n\n\\(AS_{p_{\\cap} = 0.33}\\) (RJ)\n2088\\(\\pm\\)2239\n222\\(\\pm\\)704\n16705\\(\\pm\\)23156\n0.64\\(\\pm\\)0.37\n\n\n\n\\(AS_{p_{\\cap} = 0.82}\\) (RFJ)\n2094\\(\\pm\\)2222\n228\\(\\pm\\)730\n16383\\(\\pm\\)22993\n0.69\\(\\pm\\)0.41\n\n\n\n\\(AS_{p_{\\cap} = 0.17}\\) (RI)\n2041\\(\\pm\\)2230\n174\\(\\pm\\)591\n16822\\(\\pm\\)23261\n0.57\\(\\pm\\)0.63\n\n\n\n\\(AS_{p_{\\cap} = 0.31}\\) (RJ)\n2096\\(\\pm\\)2240\n229\\(\\pm\\)713\n16877\\(\\pm\\)23227\n0.63\\(\\pm\\)0.36\n\n\n\n\\(AS_{kl = 2.66} (RFJ)\\)\n2065\\(\\pm\\)2221\n198\\(\\pm\\)652\n16189\\(\\pm\\)22931\n0.7\\(\\pm\\)0.39\n\n\n\n\\(AS_{kl = 2.96} (RI)\\)\n2070\\(\\pm\\)2236\n204\\(\\pm\\)647\n17016\\(\\pm\\)23318\n0.64\\(\\pm\\)0.59\n\n\n\n\\(AS_{kl = 2.06} (RJ)\\)\n2104\\(\\pm\\)2245\n237\\(\\pm\\)736\n17049\\(\\pm\\)23297\n0.69\\(\\pm\\)0.35\n\n\n\n\\(AS_{kl = 0.41} (RFJ)\\)\n2089\\(\\pm\\)2223\n223\\(\\pm\\)698\n16213\\(\\pm\\)22916\n0.71\\(\\pm\\)0.4\n\n\n\n\\(AS_{kl = 2.82} (RI)\\)\n2093\\(\\pm\\)2238\n227\\(\\pm\\)704\n17203\\(\\pm\\)23376\n0.59\\(\\pm\\)0.39\n\n\n\n\\(AS_{kl = 2.65} (RJ)\\)\n2104\\(\\pm\\)2245\n237\\(\\pm\\)735\n17049\\(\\pm\\)23297\n0.62\\(\\pm\\)0.36\n\n\n\n\n\n\n\n\nDetailed results. Mean and standard deviation of values for runtime, MCP, and PAR10 across all problem instances in a scenario for the sequential virtual best solver, sequential single best solver, and single top predicted algorithm in the initial three rows. The second set of rows for each scenario shows the results for the maximum number of processors (10 for SAT18-EXP) for our approaches and the baselines we compare to. All numbers were rounded to integers. The best value for each scenario and measure is shown in bold (excepting the sequential VBS, which is by definition always the best), the second best in italics. The normalized gap closed represents the mean and standard deviation of the normalized gap closed across the folds.\n\n\nScenario\nApproach\nRuntime [s]\nMCP\nPAR10\nNormalizedGap\n\n\n\n\n\n1 Processor\n\n\n\n\n\n\n\nVBS\n1146\\(\\pm\\)1945\n0\n9687\\(\\pm\\)19547\n1\n\n\n\nAS (RFJ)\n1615\\(\\pm\\)2138\n468\\(\\pm\\)1192\n13470\\(\\pm\\)21889\n0.64\\(\\pm\\)0.18\n\n\n\nAS (RI)\n1648\\(\\pm\\)2151\n502\\(\\pm\\)1256\n13758\\(\\pm\\)22034\n0.59\\(\\pm\\)0.18\n\n\n\nAS (RJ)\n1690\\(\\pm\\)2170\n543\\(\\pm\\)1302\n14183\\(\\pm\\)22247\n0.57\\(\\pm\\)0.16\n\n\n\nSBS\n2400\\(\\pm\\)2249\n1254\\(\\pm\\)1832\n20629\\(\\pm\\)24280\n0\n\n\n\n10 Processors\n\n\n\n\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RFJ)\n1702\\(\\pm\\)2301\n559\\(\\pm\\)1389\n16235\\(\\pm\\)23355\n0.39\\(\\pm\\)0.27\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RI)\n1654\\(\\pm\\)2285\n511\\(\\pm\\)1324\n15804\\(\\pm\\)23194\n0.42\\(\\pm\\)0.29\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RJ)\n1678\\(\\pm\\)2288\n535\\(\\pm\\)1351\n15956\\(\\pm\\)23243\n0.4\\(\\pm\\)0.3\n\n\n\n\\(AS_{p_{\\cap} = 0.81}\\) (RFJ)\n1518\\(\\pm\\)2172\n372\\(\\pm\\)1124\n13884\\(\\pm\\)22265\n0.62\\(\\pm\\)0.22\n\n\n\n\\(AS_{p_{\\cap} = 0.55}\\) (RI)\n1541\\(\\pm\\)2191\n397\\(\\pm\\)1177\n14034\\(\\pm\\)22332\n0.6\\(\\pm\\)0.21\n\n\n\n\\(AS_{p_{\\cap} = 0.58}\\) (RJ)\n1622\\(\\pm\\)2237\n477\\(\\pm\\)1268\n15008\\(\\pm\\)22805\n0.5\\(\\pm\\)0.25\n\n\n\n\\(AS_{p_{\\cap} = 0.82}\\) (RFJ)\n1532\\(\\pm\\)2178\n386\\(\\pm\\)1146\n14025\\(\\pm\\)22336\n0.6\\(\\pm\\)0.23\n\n\n\n\\(AS_{p_{\\cap} = 0.17}\\) (RI)\n1555\\(\\pm\\)2221\n410\\(\\pm\\)1191\n14558\\(\\pm\\)22628\n0.57\\(\\pm\\)0.23\n\n\n\n\\(AS_{p_{\\cap} = 0.31}\\) (RJ)\n1649\\(\\pm\\)2265\n505\\(\\pm\\)1319\n15544\\(\\pm\\)23064\n0.46\\(\\pm\\)0.26\n\n\n\n\\(AS_{kl = 0.12} (RFJ)\\)\n1518\\(\\pm\\)2169\n373\\(\\pm\\)1129\n13756\\(\\pm\\)22187\n0.62\\(\\pm\\)0.23\n\n\n\n\\(AS_{kl = 1.81} (RI)\\)\n1567\\(\\pm\\)2213\n422\\(\\pm\\)1211\n14442\\(\\pm\\)22547\n0.58\\(\\pm\\)0.23\n\n\n\n\\(AS_{kl = 1.35} (RJ)\\)\n1656\\(\\pm\\)2268\n512\\(\\pm\\)1316\n15551\\(\\pm\\)23060\n0.45\\(\\pm\\)0.23\n\n\n\n\\(AS_{kl = 0.41} (RFJ)\\)\n1585\\(\\pm\\)2230\n440\\(\\pm\\)1236\n14843\\(\\pm\\)22755\n0.53\\(\\pm\\)0.28\n\n\n\n\\(AS_{kl = 2.82} (RI)\\)\n1569\\(\\pm\\)2227\n424\\(\\pm\\)1219\n14699\\(\\pm\\)22693\n0.55\\(\\pm\\)0.22\n\n\n\n\\(AS_{kl = 2.65} (RJ)\\)\n1669\\(\\pm\\)2274\n525\\(\\pm\\)1335\n15691\\(\\pm\\)23119\n0.43\\(\\pm\\)0.22\n\n\n\n\n\n\n\n\n\n\n\n\nViolin plot of the distribution of the number of selected solvers to run in parallel across all problem instances for each scenario for the respective optimal (kl) and the maximum level of parallelism (seven processors for MAXSAT19-UCMS and 10 for all other scenarios). The diamond denotes the mean value. The top-left plot refers to the RFJ model, the top-right plot to the RI model, and the bottom plot to the RJ model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Revisiting Parallel Portfolio Selection with KL Divergence</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5/RevisitingParallelPortfolioSelectionwithKLDivergence.html#conclusions-and-future-work",
    "href": "chapters/chapter5/RevisitingParallelPortfolioSelectionwithKLDivergence.html#conclusions-and-future-work",
    "title": "6  Revisiting Parallel Portfolio Selection with KL Divergence",
    "section": "6.5 Conclusions and Future Work",
    "text": "6.5 Conclusions and Future Work\nIn this study, we proposed a variation of the method introduced in Chapter 4 and expanded our experiments to incorporate these adaptations. We developed an alternative general approach for selecting solvers from a portfolio and scheduling them in parallel. This method leverages the predicted runtime distribution to make informed decisions about which solvers and how many to run in parallel. Specifically, in contrast to the method introduced in Chapter 4, where the joint probability of the prediction distribution is used as a measure of the likelihood that an algorithm performs, as well as the best-predicted solver, the new approach utilizes the KL divergence formula. This allows us to evaluate how much an algorithm’s prediction diverges from the best-predicted solver, excluding solvers whose predictions differ the most. Moreover, similar to previous chapter, we measured the actual runtime when operating multiple algorithms in parallel, instead of relying on assumed sequential runtimes.\nOur results showed that while the previous method outperforms the new approach, in the absence of the old method, the new approach proves to be superior. Additionally, tuning the threshold for the joint probability in the method of the previous chapter varies significantly between different benchmarks and performance models. In contrast, the tuned threshold for divergence in the new approach is more consistent and this consistency can reduce the computational effort required for tuning.\nFor future work, we plan to explore replacing the performance models with models trained on parallel data instead of sequential data. Currently, the training data does not reflect the actual runtimes when algorithms are executed in parallel, and running algorithms in parallel can introduce overhead. We aim to evaluate whether this replacement improves portfolio selection performance.\n\n\n\n\n\n\nBischl, Bernd, Pascal Kerschke, Lars Kotthoff, Marius Lindauer, Yuri Malitsky, Alexandre Fréchette, Holger Hoos, et al. 2016. “ASlib: A Benchmark Library for Algorithm Selection.” Artificial Intelligence 237: 41–58.\n\n\nBischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. Jones. 2016. “mlr: Machine Learning in R.” Journal of Machine Learning Research 17 (170): 1–5. https://jmlr.org/papers/v17/15-066.html.\n\n\nBishop, Christopher M, and Nasser M Nasrabadi. 2006. Pattern Recognition and Machine Learning. Vol. 4. 4. Springer.\n\n\nFawcett, Chris, Mauro Vallati, Frank Hutter, Jörg Hoffmann, Holger Hoos, and Kevin Leyton-Brown. 2014. “Improved Features for Runtime Prediction of Domain-Independent Planners.” Proceedings of the International Conference on Automated Planning and Scheduling 24 (1): 355–59. https://doi.org/10.1609/icaps.v24i1.13680.\n\n\nGomes, Carla, and Bart Selman. 2001. “Algorithm Portfolios.” Artificial Intelligence 126: 43–62.\n\n\nHuberman, Bernardo A., Rajan M. Lukose, and Tad Hogg. 1997. “An economics approach to hard computational problems.” Science 275 (5296): 51–54. https://doi.org/10.1126/science.275.5296.51.\n\n\nHutter, Frank, Lin Xu, Holger H. Hoos, and Kevin Leyton-Brown. 2014. “Algorithm Runtime Prediction: Methods & Evaluation.” Artificial Intelligence 206: 79–111. https://doi.org/https://doi.org/10.1016/j.artint.2013.10.003.\n\n\nKashgarani, Haniye, and Lars Kotthoff. 2021. “Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution.” In AAAI Workshop on Meta-Learning and MetaDL Challenge, 140:58–64. Proceedings of Machine Learning Research. PMLR. https://proceedings.mlr.press/v140/kashgarani21a.html.\n\n\n———. 2023. “Automatic Parallel Portfolio Selection.” In ECAI 2023, 1215–22. IOS Press.\n\n\nKullback, S., and R. A. Leibler. 1951. “On Information and Sufficiency.” The Annals of Mathematical Statistics 22 (1): 79–86. http://www.jstor.org/stable/2236703.\n\n\nLindauer, Marius, Holger H Hoos, Frank Hutter, and Torsten Schaub. 2015. “Autofolio: An Automatically Configured Algorithm Selector.” Journal of Artificial Intelligence Research 53: 745–78.\n\n\nLindauer, Marius, Holger Hoos, Kevin Leyton-Brown, and Torsten Schaub. 2017. “Automatic Construction of Parallel Portfolios via Algorithm Configuration.” Artificial Intelligence 244: 272–90. https://doi.org/https://doi.org/10.1016/j.artint.2016.05.004.\n\n\nO’Mahony, Eoin, Emmanuel Hebrard, Alan Holland, Conor Nugent, and Barry O’Sullivan. 2008. “Using Case-Based Reasoning in an Algorithm Portfolio for Constraint Solving.” In Irish Conference on Artificial Intelligence and Cognitive Science, 210–16. Proceedings of the 19th Irish Conference on Artificial Intelligence; Cognitive Science.\n\n\nWager, Stefan, Trevor Hastie, and Bradley Efron. 2014. “Confidence intervals for random forests: The jackknife and the infinitesimal jackknife.” The Journal of Machine Learning Research 15 (1): 1625–51.\n\n\nXu, Lin, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2008. “SATzilla: Portfolio-Based Algorithm Selection for SAT.” J. Artif. Int. Res. 32 (1): 565–606.\n\n\nXu, Lin, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. 2011. “Hydra-MIP: Automated algorithm configuration and selection for mixed integer programming.” In Proceedings of the 18th RCRA Workshop, 16–30.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Revisiting Parallel Portfolio Selection with KL Divergence</span>"
    ]
  },
  {
    "objectID": "chapters/chapter6/ParallelPortfolioSelectionwithParallelDataTraining.html",
    "href": "chapters/chapter6/ParallelPortfolioSelectionwithParallelDataTraining.html",
    "title": "7  Parallel Portfolio Selection with Parallel Data Training",
    "section": "",
    "text": "7.1 Introduction\nCombinatorial problem solvers often exhibit complementary performance when solving given problem instances. The use of algorithms portfolios has proven to be more effective than choosing the best single solver (SBS) averaged across all instances (Huberman, Lukose, and Hogg 1997; Gomes and Selman 2001). Different portfolio approaches and designs are presented in the literature. Entire portfolios can be run in parallel or a single algorithm can be selected instance by instance using algorithm selection techniques.\nThis selection-based approach is often addressed by training performance models using machine learning algorithms and instance features (Kotthoff 2014; Kerschke et al. 2019). One drawback of this approach is that since no machine learning model is perfect, the selections might be incorrect, and performance models might not always be able to select the best algorithm for a given instance. This means that selecting the wrong algorithm can waste time and resources.\nSince the hardware architecture of modern computing units can handle multiple tasks in parallel, another portfolio approach is to execute the entire portfolio simultaneously. This approach avoids the drawback of algorithm selection, as the actual best solver will always be included in the portfolio and executed alongside other algorithms. However, running too many solvers in parallel can lead to resource wastage by overloading computing processors. Additionally, this method can introduce significant overhead, as solvers and cores compete for shared resources when too many solvers are executed simultaneously. The more algorithms executed in parallel, the greater the risk of time-out computations and overhead. The preliminary results presented in Chapter 3 (Kashgarani and Kotthoff 2021) showed that, while the selection of a single algorithm is suboptimal and imperfect, it performs better than running too many solvers in parallel.\nIn Chapter 4, we proposed a hybrid approach that combines the benefits of algorithm selection and parallel execution. We further extend this method to a variation that performs competitively with the original approach in Chapter 5. These \"middle-path\" methods aim to select the most promising subset of algorithms to run in parallel on a single non-distributed computing node (Kashgarani and Kotthoff 2023). These promising methods optimized performance by utilizing regression-based random forest algorithm selectors, incorporating the estimated uncertainty of predictions, and accounting for the overhead associated with parallel portfolio execution. However, these approaches were tested using performance models trained on sequential data, which do not account for the overhead associated with parallel execution.\nIn this chapter, instead of relying on regression random forest models trained on sequential data, we train performance models using actual parallel data. We then compare algorithm selectors trained on sequential data with those trained on parallel data, applying the method proposed in Chapter 4 for parallel portfolio selection. By evaluating the effectiveness of these two types of algorithm selector in portfolio selection, we investigate whether collecting parallel data provides a significant advantage or whether sequential data are equally effective.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parallel Portfolio Selection with Parallel Data Training</span>"
    ]
  },
  {
    "objectID": "chapters/chapter6/ParallelPortfolioSelectionwithParallelDataTraining.html#training-performance-modeling-with-parallel-data",
    "href": "chapters/chapter6/ParallelPortfolioSelectionwithParallelDataTraining.html#training-performance-modeling-with-parallel-data",
    "title": "7  Parallel Portfolio Selection with Parallel Data Training",
    "section": "7.2 Training Performance Modeling with Parallel Data",
    "text": "7.2 Training Performance Modeling with Parallel Data\nIn the paper Chapter 4 and 5(Kashgarani and Kotthoff 2023), we used data collected from running a single solver exclusively on a computing node to train the regression-based random forest model. This data does not account for parallelization or the overhead associated with running multiple solvers concurrently. Although this approach has proven effective, since we have already collected parallel data, we now aim to investigate the efficiency of training the performance model using these parallel data as well. Specifically, we focus on training performance models on parallel data. These parallel data are gathered by running solvers in parallel alongside other solvers, with varying numbers of solvers executed simultaneously on a single, exclusive computing node ensuring that no other tasks interfere with the recorded runtimes.\nIn this approach, for scenarios such as SAT11-NDU, SAT18-EXP, SAT16-MAIN, and IPC2018, where data are available for up to 10 parallel runs, we trained 10 distinct performance models, each tailored to a specific level of parallelization. Similarly, for MAXSAT2019, which includes data for up to 7 parallel runs, we trained 7 separate models, each trained for a given parallel run level. This means that the performance model, trained on data collected from running \\(n\\) parallel runs, is used to predict the performance of a solver when running \\(n\\) parallel runs.\n\n7.2.1 Parallel Portfolio Selection\nAlgorithm performance for combinatorial problems varies widely between instances, and portfolio approaches that use machine learning-based algorithm selection aim to optimize performance by selecting the best algorithms to execute while minimizing overhead from concurrent execution. Algorithm selection has been effective in domains like SAT and mixed integer programming. However, selecting a single algorithm can sometimes lead to suboptimal outcomes due to inaccurate predictions. This issue can be addressed by selecting a small subset of algorithms to run in parallel, which mitigates prediction inaccuracies while reducing overhead and resource demands.\nWe follow the methods and equations outlined in Chapter 4 as described in (Kashgarani and Kotthoff 2023). We do not include the subportfolio selection method using KL divergence, as it consistently performed worse than the method proposed in Chapter 4. The selection approach in Chapter 4 aims to select a subportfolio of solvers to run in parallel for a given problem instance by leveraging predicted performance rankings. Solvers are ranked according to their predicted performance metric, forming a total order, and a subset of size \\(n\\) is selected from the top-ranked solvers. The key challenge is determining the optimal portfolio size \\(n\\), as including too many algorithms increases the chance of including the best solver but also introduces significant overhead from running solvers in parallel. To address this, the method incorporates a probabilistic approach using the predicted performance distribution of each solver.\nRather than relying on point predictions, the method uses the predicted performance distributions and extends typical algorithm selection practices by incorporating uncertainty of performance predictions. We assume that predictions follow a normal distribution characterized by their mean and standard deviation. The overlap between these distributions is used to compute the likelihood that a solver performs comparably to the best-predicted solver. A threshold parameter, \\(p_{\\cap}\\), controls the size of the parallel portfolio by including solvers whose distributions overlap sufficiently with that of the best solver. This approach balances the trade-off between including solvers that might achieve optimal performance and limiting computational overhead, providing a flexible and robust framework for parallel portfolio selection.\nAs in previous chapters, we used regression random forests as the algorithm selector, identified as the best performing approach in the ASLib study (Bischl, Kerschke, et al. 2016). Furthermore, we transitioned from using a model trained exclusively on sequential data to one trained on sequential and parallel data to assess their comparative effectiveness. Through this approach and our experiments, we sought to optimize the utilization of parallel computational resources for solving combinatorial problems while minimizing the overhead associated with parallel execution.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parallel Portfolio Selection with Parallel Data Training</span>"
    ]
  },
  {
    "objectID": "chapters/chapter6/ParallelPortfolioSelectionwithParallelDataTraining.html#experimental-setup",
    "href": "chapters/chapter6/ParallelPortfolioSelectionwithParallelDataTraining.html#experimental-setup",
    "title": "7  Parallel Portfolio Selection with Parallel Data Training",
    "section": "7.3 Experimental Setup",
    "text": "7.3 Experimental Setup\n\n7.3.1 Data Collection\nWe used the same five scenarios from previous chapters (Kashgarani and Kotthoff 2023), all now included in the ASlib benchmark repository (Bischl, Kerschke, et al. 2016): MAXSAT19-UCMS, SAT11-INDU, SAT18-EXP, SAT16-MAIN, and IPC2018. Although the ASlib data repository only provides performance data for single runs, we incorporated parallel run measurements from (Kashgarani and Kotthoff 2023), performed on standalone machines. For MAXSAT19-UCMS, SAT11-INDU, SAT16-MAIN, and SAT18-EXP, 54 features are computed using the SATZilla feature extraction code (Xu et al. 2008). For IPC2018, feature extraction tool by (Fawcett et al. 2014) generated 305 features. The scenarios, data and features of our experiments align with those used in (Kashgarani and Kotthoff 2023), as summarized in Table 1.1.\nThe data were collected on compute nodes with 32 processors, a 40 MB cache (Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz), and 128 GB of memory, running Red Hat Linux version 8.6. We adhered to the ASlib time limits: 5000 CPU seconds for SAT18-EXP, SAT11-INDU, and SAT16-MAIN; 3600 seconds for MAXSAT19-UCMS; and 1800 seconds for IPC2018. Each algorithm was individually executed over 2 to 10 parallel runs during data collection.\n\n\nNumber of Algorithms, Instances, and Features Across All Scenarios.\n\n\nScenario\nAlgorithms\nInstances\nInstance Features\n\n\n\n\n\nIPC2018\n15\n240\n305\n\n\n\nMAXSAT19-UCMS\n7\n572\n54\n\n\n\nSAT11-INDU\n14\n300\n54\n\n\n\nSAT16-MAIN\n25\n274\n54\n\n\n\nSAT18-EXP\n37\n353\n54\n\n\n\n\n\n\n\n7.3.2 Training and Tuning\nWe constructed random forest regression models using the Ranger implementation from the MLR3 package (Wright and Ziegler 2017) in R to predict the algorithm performance in specific instances. Although we previously presented results from various random forest implementations in R, this chapter focuses exclusively on Ranger models to compare the performance of models trained on sequential data with those trained on parallel data. Our setup closely follows the methodology outlined in Chapter 4 and in (Bischl, Kerschke, et al. 2016): we excluded instance features with constant values and imputed missing feature values using the mean of all nonmissing values for each feature. The random forest hyperparameters were optimized using random search with 250 iterations, varying \\(ntree\\) between 10 and 200 and mtry between 1 and 30. Nested cross-validation with three inner folds and ten outer folds was used for model evaluation (Bischl, Kerschke, et al. 2016).\nFor training the models with parallel data, we trained 10 models for the SAT11-INDU, SAT16-MAIN, SAT18-EXP, and IPC2018 scenarios, and 7 models for the MAXSAT19-UCMS scenario, as only 7 solvers are available for this benchmark. Each model was trained using the runtimes associated with its specific parallel configuration. For instance, we trained one performance model using the runtimes of algorithms running in parallel with 2 other solvers, another model using the data collected when running 3 solvers in parallel, and so forth.\nOur random forest regression models, built with the MLR3 and Ranger packages, predict solver runtime as the mean of the underlying distribution and estimate the standard deviation using the Jackknife and Infinitesimal Jackknife methods (Wager, Hastie, and Efron 2014; Bischl, Lang, et al. 2016). The Jackknife method calculates the standard deviation of the mean predictions in all training observations. In this approach, the random forest model is trained on \\(n-1\\) observations, leaving one out for prediction, and this process is repeated for each observation. The mean prediction for each tree is calculated by averaging its predictions for the left-out observations. The Jackknife method assumes a normal distribution of the predictions, where the standard deviation quantifies the uncertainty of the overall prediction. The Infinitesimal Jackknife method, rather than completely removing observations, assigns them a small weight to account for their influence on the model.\nThe tuned \\(p_{\\cap}\\) values of Equation [eq:7] for each benchmark and model are provided in Table [tab:pcap]. These values were individually optimized for each scenario to ensure that the selected portfolio achieved an optimal balance between performance and computational efficiency.\nWe first compare the predictions of the performance models trained on sequential data with those trained on data collected under different levels of parallelism. The analysis was performed using the McNemar test to assess whether there are significant differences in predictive performance between the sequential and parallel models in various scenarios. For each scenario, models trained on sequential data were compared with models trained on parallel data, involving varying numbers of parallel runs, ranging from 2 to the maximum number of solvers available for each scenario. We used contingency tables to capture the outcomes: instances where both models made correct or incorrect predictions and instances where one model succeeded while the other failed. The McNemar test provided p-values and test statistics for each comparison. The resulting \\(p-values\\) indicate significant differences between the predictions of sequential and parallel models across all with significant level \\(\\alpha = 0.05\\) configurations. For example, in the MAXSAT2019 scenario, the \\(p-value\\) for models trained with 2 parallel runs was \\(2.51e-8\\), highlighting substantial differences in their predictions compared to sequential models. Similar trends were observed in other scenarios, such as IPC2018 and SAT18-EXP, where p-values remained very low at different levels of parallelism. These results demonstrate that the predictions of sequential models are notably distinct from those of parallel models, with significant differences observed in most scenarios and configurations.\nAlthough the McNemar tests showed that the predictions are significantly different, we further investigated the Critical Difference (CD). CD analysis reveals that while the predictions of algorithm selectors trained in sequential and parallel data may vary, their ranking among the models on the five benchmarks is not significantly different at \\(\\alpha = 0.05\\). Using a critical difference value of 6.06 (calculated for 10 models and 5 datasets), the CD diagram in Figure 1.1 confirms that the variations in average ranks fall within the nonsignificant range. For instance, the average ranks of the models show minor differences across parallel configurations (e.g., the Sequential model has an average rank of 4.11 compared to the \"10 parallel runs\" model with 6.12), but these differences do not exceed the CD threshold. The rankings of the average ranks matrix in Table 1.3 highlight consistent algorithm selection performance trends in datasets such as MAXSAT2019, IPC2018, SAT11-INDU, SAT16-MAIN and SAT18-EXP. Although individual rankings may differ slightly between scenarios, the overall rankings do not exhibit significant divergence. This suggests that algorithm selectors, whether trained in sequential or parallel data, produce models that rank algorithms similarly in terms of performance when evaluated across multiple datasets.\n\n\n\nP-values for McNemar tests comparing models trained on parallel data with the model trained on sequential data. The McNemar tests were conducted by selecting the top single solver based on the performance model and constructing the corresponding 2x2 contingency matrix.\n\n\nModel\nMAXSAT19-UCMS\nIPC2018\nSAT11-INDU\nSAT16-MAIN\nSAT18-EXP\n\n\n\n\n2 parallel runs\n2.51e-08\n3.39e-02\n4.53e-03\n4.54e-04\n6.17e-04\n\n\n3 parallel runs\n2.19e-06\n4.89e-03\n2.85e-02\n1.42e-03\n2.29e-04\n\n\n4 parallel runs\n3.97e-04\n9.51e-04\n1.05e-03\n1.26e-03\n2.97e-04\n\n\n5 parallel runs\n3.15e-05\n1.39e-03\n5.66e-03\n1.56e-03\n7.66e-04\n\n\n6 parallel runs\n8.25e-04\n5.85e-03\n4.01e-03\n9.37e-04\n3.83e-04\n\n\n7 parallel runs\n3.80e-03\n8.97e-03\n3.68e-02\n3.82e-03\n6.14e-06\n\n\n8 parallel runs\n–\n4.65e-04\n1.13e-03\n1.76e-04\n6.85e-06\n\n\n9 parallel runs\n–\n4.07e-04\n4.67e-02\n1.09e-03\n1.28e-04\n\n\n10 parallel runs\n–\n2.16e-04\n5.07e-03\n1.09e-03\n2.28e-05\n\n\n\n\n\n\nRanking Matrix for Critical Difference\n\n\n\nSeq.\n2p\n3p\n4p\n5p\n6p\n7p\n8p\n9p\n10p\n\n\n\n\nMAXSAT19-UCMS\n3.42\n3.65\n4.01\n3.92\n4.21\n4.42\n4.38\n–\n–\n–\n\n\nIPC2018\n3.88\n4.59\n4.94\n5.19\n5.28\n5.96\n5.69\n6.19\n6.58\n6.69\n\n\nSAT11-INDU\n4.18\n4.58\n4.67\n5.25\n5.28\n5.83\n5.88\n6.34\n6.32\n6.68\n\n\nSAT16-MAIN\n5.13\n4.98\n4.85\n5.07\n5.42\n5.53\n5.61\n5.77\n6.51\n6.13\n\n\nSAT18-EXP\n3.94\n4.80\n5.11\n5.61\n5.55\n6.01\n6.17\n6.44\n6.40\n4.99\n\n\nAverage\n4.11\n4.52\n4.71\n5.01\n5.15\n5.55\n5.54\n6.19\n6.45\n6.12\n\n\n\n\n\n\n\nCritical Difference Diagram. This figure shows the average ranking of each performance model trained at different levels of parallelism, based on the actual performance of the selected algorithms. The CD of 6.06 means that the Critical Difference (CD) threshold for determining statistically significant differences in rankings is 6.06. If the difference in average rankings between two models is greater than 6.06, the rankings are considered statistically significantly different at a significance level of ().\n\n\n\n\n7.3.3 Baselines\nAlthough the algorithm selection rankings did not show significant differences in algorithm selection in these five scenarios, we evaluated the performance of the dynamic parallel portfolio approach against several baseline methods using sequential and parallel data-trained models. Specifically, we compared the results to the sequential Virtual Best Solver (VBS), which selects the best solver for each problem instance with a cumulative misclassification penalty of zero, and the sequential Single Best Solver (SBS), which is the solver with the best average performance across all instances and has a cumulative misclassification penalty of one. For parallel runs, the VBS selects the best solver for each instance but accounts for the overhead of \\(n\\) parallel runs. The parallel SBS is determined similarly, based on the solver with the best average performance across all instances instead of the best solver for each instance. To ensure a realistic evaluation, we executed multiple solvers in parallel to measure the actual runtime of the best solver in this setup, rather than assuming it would run sequentially.\nWe used two methods to train algorithm selectors: one trained using Ranger with the Jackknife method and another using Ranger with the Infinitesimal Jackknife method to estimate the uncertainty of predictions. Initially, we trained models on sequential data and then trained performance models on parallel performance data (\\(AS_{\\parallel}\\)). In the \\(AS_{\\parallel}\\) method, to predict performance for \\(n\\) parallel runs, we used the predictions of the performance model trained on data collected from \\(n\\) parallel runs.\nWe evaluated the proposed approach using three metrics: penalized average runtime with a factor of 10 (PAR10), misclassification penalty (MCP), and runtime. The PAR10 metric corresponds to the actual runtime if the algorithm successfully solves the instance within the timeout; otherwise, it is calculated as the timeout multiplied by 10. The MCP represents the difference between the performance of the selected algorithm and that of the optimal algorithm. To ensure comparability across scenarios, all performance metrics are normalized relative to the Virtual Best Solver (VBS) and Single Best Solver (SBS). The results, shown in Figure 1.2, depict the proportion of the performance gap closed by each approach. On this normalized scale, a value of 0 corresponds to the performance of the SBS, while a value of 1 corresponds to the performance of the VBS.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parallel Portfolio Selection with Parallel Data Training</span>"
    ]
  },
  {
    "objectID": "chapters/chapter6/ParallelPortfolioSelectionwithParallelDataTraining.html#results",
    "href": "chapters/chapter6/ParallelPortfolioSelectionwithParallelDataTraining.html#results",
    "title": "7  Parallel Portfolio Selection with Parallel Data Training",
    "section": "7.4 Results",
    "text": "7.4 Results\nWe experimented with training Ranger models using parallel data to compare their performance with models trained on sequential data. Figures [fig:rangervsrf] present the PAR10 score results in terms of the normalized performance gap between the sequential Single Best Solver (SBS) and the sequential Virtual Best Solver (VBS) across all scenarios and processor counts. These figures specifically compare Ranger models trained with the Infinitesimal Jackknife method. Additionally, Tables 1.4, 1.5 and 1.6 provide the exact runtime, Misclassification Penalty (MCP), and PAR10 scores for the methods, limiting the maximum number of parallel runs to 10 for SAT18-EXP, SAT16-MAIN, SAT11-INDU, and IPC2018, and to 7 for MAXSAT19-UCMS.\n\n\n\nResults Overview. The plot illustrates the extent to which each method narrows the gap between the PAR10 scores of the Single Best Solver (SBS) and the Virtual Best Solver (VBS). For VBS and SBS, the top (n) solvers are selected, where (n) matches the number of processors available for each problem instance and across all instances, respectively. (AS_0) selects the top (n) solvers as predicted by algorithm selection, disregarding any overlap in their predicted runtime distributions. (AS_{p_{}}) follows the approach proposed in , with the number of processors capped at the specific value on the x-axis — fewer solvers than this maximum may be selected based on the overlap in runtime predictions. The algorithm selection here is Ranger model with the Infinitesimal Jackknife method for predicting uncertainty. The () symbol is referring to the performance model trained on parallel data.\n\n\n\n\n\nDetailed results. Mean and standard deviation of values for runtime, MCP, and PAR10 across all problem instances in a scenario for the sequential virtual best solver, sequential single best solver, and single top predicted algorithm in the initial three rows. The second set of rows for each scenario shows the results for the maximum number of processors (10 for IPC2018, and 7 for MAXSAT19-UCMS) for our approaches and the baselines we compare to. All numbers were rounded to integers. The best value for each scenario and measure is shown in bold (excepting the sequential VBS, which is by definition always the best), the second best in italics. The normalized gap closed represents the mean and standard deviation of the normalized gap closed across the folds.\n\n\nScenario\nApproach\nRuntime [s]\nMCP\nPAR10\nNormalizedGap\n\n\n\n\n\n1 Processor\n\n\n\n\n\n\n\nVBS\n508\\(\\pm\\)697\n0\n3478\\(\\pm\\)6903\n1\n\n\n\nAS (RI)\n608\\(\\pm\\)751\n100\\(\\pm\\)293\n4456\\(\\pm\\)7583\n-0.35\\(\\pm\\)2.85\n\n\n\nAS (RJ)\n604\\(\\pm\\)752\n96\\(\\pm\\)293\n4519\\(\\pm\\)7633\n-0.39\\(\\pm\\)2.84\n\n\n\nSBS\n734\\(\\pm\\)770\n226\\(\\pm\\)414\n5459\\(\\pm\\)8072\n0\n\n\n\n10 Processors\n\n\n\n\n\n\n\n\\(AS_0\\) (RI)\n616\\(\\pm\\)783\n107\\(\\pm\\)312\n5206\\(\\pm\\)8065\n-0.69\\(\\pm\\)2.54\n\n\n\n\\(AS_0\\) (RJ)\n616\\(\\pm\\)783\n107\\(\\pm\\)312\n5206\\(\\pm\\)8065\n-0.69\\(\\pm\\)2.54\n\n\n\n\\(AS_{p_{\\cap} = 0.44}\\) (RI)\n557\\(\\pm\\)728\n49\\(\\pm\\)190\n4135\\(\\pm\\)7403\n-0.19\\(\\pm\\)2.89\n\n\n\n\\(AS_{p_{\\cap} = 0.27}\\) (RJ)\n570\\(\\pm\\)744\n62\\(\\pm\\)229\n4350\\(\\pm\\)7552\n-0.21\\(\\pm\\)2.72\n\n\n\n\\(AS_{\\parallel 0} (RI)\\)\n615\\(\\pm\\)782\n107\\(\\pm\\)311\n5205\\(\\pm\\)8065\n-0.69\\(\\pm\\)2.55\n\n\n\n\\(AS_{\\parallel 0} (RJ)\\)\n619\\(\\pm\\)786\n112\\(\\pm\\)322\n5277\\(\\pm\\)8102\n-0.71\\(\\pm\\)2.54\n\n\n\n\\(AS_{\\parallel p_{\\cap} = 0.44} (RI)\\)\n581\\(\\pm\\)743\n73\\(\\pm\\)252\n4428\\(\\pm\\)7596\n-0.34\\(\\pm\\)2.67\n\n\n\n\\(AS_{\\parallel p_{\\cap} = 0.27} (RJ)\\)\n608\\(\\pm\\)771\n100\\(\\pm\\)304\n4996\\(\\pm\\)7946\n-1.5\\(\\pm\\)5.65\n\n\n\n1 Processor\n\n\n\n\n\n\n\nVBS\n858\\(\\pm\\)1476\n0\n7768\\(\\pm\\)14717\n1\n\n\n\nAS (RI)\n1076\\(\\pm\\)1575\n218\\(\\pm\\)729\n9686\\(\\pm\\)15850\n0.45\\(\\pm\\)0.34\n\n\n\nAS (RJ)\n1044\\(\\pm\\)1565\n186\\(\\pm\\)666\n9540\\(\\pm\\)15793\n0.49\\(\\pm\\)0.23\n\n\n\nSBS\n1190\\(\\pm\\)1657\n332\\(\\pm\\)940\n11386\\(\\pm\\)16696\n0\n\n\n2-6\n7 Processors\n\n\n\n\n\n\n2-6\n\\(AS_0\\) (RI)\n894\\(\\pm\\)1506\n37\\(\\pm\\)247\n8258\\(\\pm\\)15062\n0.85\\(\\pm\\)0.16\n\n\n\n\\(AS_0\\) (RJ)\n894\\(\\pm\\)1506\n37\\(\\pm\\)247\n8258\\(\\pm\\)15062\n0.85\\(\\pm\\)0.16\n\n\n\n\\(AS_{p_{\\cap} = 0.03}\\) (RI)\n894\\(\\pm\\)1506\n37\\(\\pm\\)247\n8258\\(\\pm\\)15062\n0.85\\(\\pm\\)0.16\n\n\n\n\\(AS_{p_{\\cap} = 0.14}\\) (RJ)\n921\\(\\pm\\)1521\n63\\(\\pm\\)369\n8568\\(\\pm\\)15263\n0.76\\(\\pm\\)0.24\n\n\n\n\\(AS_{\\parallel 0} (RI)\\)\n894\\(\\pm\\)1506\n37\\(\\pm\\)247\n8258\\(\\pm\\)15062\n0.85\\(\\pm\\)0.16\n\n\n\n\\(AS_{\\parallel 0} (RJ)\\)\n894\\(\\pm\\)1506\n37\\(\\pm\\)247\n8258\\(\\pm\\)15062\n0.85\\(\\pm\\)0.16\n\n\n\n\\(AS_{\\parallel p_{\\cap} = 0.03} (RI)\\)\n894\\(\\pm\\)1506\n37\\(\\pm\\)247\n8258\\(\\pm\\)15062\n0.85\\(\\pm\\)0.16\n\n\n\n\\(AS_{\\parallel p_{\\cap} = 0.14} (RJ)\\)\n939\\(\\pm\\)1534\n82\\(\\pm\\)442\n8756\\(\\pm\\)15379\n0.71\\(\\pm\\)0.31\n\n\n\n\n\n\n\n\nDetailed results. Mean and standard deviation of values for runtime, MCP, and PAR10 across all problem instances in a scenario for the sequential virtual best solver, sequential single best solver, and single top predicted algorithm in the initial three rows. The second set of rows for each scenario shows the results for the maximum number of processors (10 for SAT16-MAIN and SAT11-INDU) for our approaches and the baselines we compare to. All numbers were rounded to integers. The best value for each scenario and measure is shown in bold (excepting the sequential VBS, which is by definition always the best), the second best in italics. The normalized gap closed represents the mean and standard deviation of the normalized gap closed across the folds.\n\n\nScenario\nApproach\nRuntime [s]\nMCP\nPAR10\nNormalizedGap\n\n\n\n\n\n1 Processor\n\n\n\n\n\n\n\nVBS\n1140\\(\\pm\\)1836\n0\n8040\\(\\pm\\)17905\n1\n\n\n\nAS (RI)\n1610\\(\\pm\\)2108\n470\\(\\pm\\)1145\n12710\\(\\pm\\)21389\n-0.06\\(\\pm\\)0.9\n\n\n\nAS (RJ)\n1565\\(\\pm\\)2049\n425\\(\\pm\\)1017\n11315\\(\\pm\\)20402\n0.34\\(\\pm\\)0.49\n\n\n\nSBS\n1818\\(\\pm\\)2168\n678\\(\\pm\\)1340\n14268\\(\\pm\\)22154\n0\n\n\n\n10 Processors\n\n\n\n\n\n\n\n\\(AS_0\\) (RI)\n1238\\(\\pm\\)1892\n127\\(\\pm\\)385\n8588\\(\\pm\\)18350\n0.92\\(\\pm\\)0.1\n\n\n\n\\(AS_0\\) (RJ)\n1262\\(\\pm\\)1910\n151\\(\\pm\\)480\n8612\\(\\pm\\)18342\n0.92\\(\\pm\\)0.1\n\n\n\n\\(AS_{p_{\\cap} = 0.01}\\) (RI)\n1236\\(\\pm\\)1890\n121\\(\\pm\\)379\n8586\\(\\pm\\)18351\n0.92\\(\\pm\\)0.1\n\n\n\n\\(AS_{p_{\\cap} = 0.31}\\) (RJ)\n1289\\(\\pm\\)1934\n178\\(\\pm\\)595\n9089\\(\\pm\\)18787\n0.78\\(\\pm\\)0.28\n\n\n\n\\(AS_{\\parallel 0} (RI)\\)\n1276\\(\\pm\\)1926\n166\\(\\pm\\)546\n8926\\(\\pm\\)18643\n0.89\\(\\pm\\)0.13\n\n\n\n\\(AS_{\\parallel 0} (RJ)\\)\n1286\\(\\pm\\)1939\n175\\(\\pm\\)564\n8936\\(\\pm\\)18640\n0.88\\(\\pm\\)0.12\n\n\n\n\\(AS_{\\parallel p_{\\cap} = 0.01} (RI)\\)\n1279\\(\\pm\\)1929\n162\\(\\pm\\)530\n9079\\(\\pm\\)18791\n0.78\\(\\pm\\)0.27\n\n\n\n\\(AS_{\\parallel p_{\\cap} = 0.31} (RJ)\\)\n1365\\(\\pm\\)2004\n247\\(\\pm\\)794\n10065\\(\\pm\\)19605\n0.64\\(\\pm\\)0.24\n\n\n\n1 Processor\n\n\n\n\n\n\n\nVBS\n1867\\(\\pm\\)2193\n0\n15005\\(\\pm\\)22530\n1\n\n\n\nAS (RI)\n2383\\(\\pm\\)2294\n516\\(\\pm\\)1151\n19956\\(\\pm\\)24111\n0.05\\(\\pm\\)0.66\n\n\n\nAS (RJ)\n2400\\(\\pm\\)2269\n533\\(\\pm\\)1177\n19316\\(\\pm\\)23880\n0.3\\(\\pm\\)0.24\n\n\n\nSBS\n2560\\(\\pm\\)2294\n693\\(\\pm\\)1415\n21940\\(\\pm\\)24464\n0\n\n\n2-6\n10 Processors\n\n\n\n\n\n\n2-6\n\\(AS_0\\) (RI)\n2016\\(\\pm\\)2225\n150\\(\\pm\\)503\n16469\\(\\pm\\)23122\n0.68\\(\\pm\\)0.6\n\n\n\n\\(AS_0\\) (RJ)\n2048\\(\\pm\\)2228\n181\\(\\pm\\)597\n16336\\(\\pm\\)23023\n0.68\\(\\pm\\)0.59\n\n\n\n\\(AS_{p_{\\cap} = 0}\\) (RI)\n2016\\(\\pm\\)2225\n150\\(\\pm\\)503\n16469\\(\\pm\\)23122\n0.68\\(\\pm\\)0.6\n\n\n\n\\(AS_{p_{\\cap} = 0.33}\\) (RJ)\n2088\\(\\pm\\)2239\n222\\(\\pm\\)704\n16705\\(\\pm\\)23156\n0.64\\(\\pm\\)0.37\n\n\n\n\\(AS_{\\parallel 0} (RI)\\)\n2053\\(\\pm\\)2246\n186\\(\\pm\\)593\n16505\\(\\pm\\)23101\n0.79\\(\\pm\\)0.49\n\n\n\n\\(AS_{\\parallel 0} (RJ)\\)\n2040\\(\\pm\\)2223\n174\\(\\pm\\)579\n16000\\(\\pm\\)22864\n0.8\\(\\pm\\)0.34\n\n\n\n\\(AS_{\\parallel p_{\\cap} = 0} (RI)\\)\n2053\\(\\pm\\)2246\n186\\(\\pm\\)593\n16505\\(\\pm\\)23101\n0.79\\(\\pm\\)0.49\n\n\n\n\\(AS_{\\parallel p_{\\cap} = 0.33} (RJ)\\)\n2197\\(\\pm\\)2271\n330\\(\\pm\\)924\n17635\\(\\pm\\)23454\n0.6\\(\\pm\\)0.43\n\n\n\n\n\n\n\n\nDetailed results. Mean and standard deviation of values for runtime, MCP, and PAR10 across all problem instances in a scenario for the sequential virtual best solver, sequential single best solver, and single top predicted algorithm in the initial three rows. The second set of rows for each scenario shows the results for the maximum number of processors (10 for SAT18-EXP) for our approaches and the baselines we compare to. All numbers were rounded to integers. The best value for each scenario and measure is shown in bold (excepting the sequential VBS, which is by definition always the best), the second best in italics. The normalized gap closed represents the mean and standard deviation of the normalized gap closed across the folds.\n\n\nScenario\nApproach\nRuntime [s]\nMCP\nPAR10\nNormalizedGap\n\n\n\n\n\n1 Processor\n\n\n\n\n\n\n\nVBS\n1146\\(\\pm\\)1945\n0\n9687\\(\\pm\\)19547\n1\n\n\n\nAS (RI)\n1648\\(\\pm\\)2151\n502\\(\\pm\\)1256\n13758\\(\\pm\\)22034\n0.59\\(\\pm\\)0.18\n\n\n\nAS (RJ)\n1690\\(\\pm\\)2170\n543\\(\\pm\\)1302\n14183\\(\\pm\\)22247\n0.57\\(\\pm\\)0.16\n\n\n\nSBS\n2400\\(\\pm\\)2249\n1254\\(\\pm\\)1832\n20629\\(\\pm\\)24280\n0\n\n\n\n10 Processors\n\n\n\n\n\n\n\n\\(AS_0\\) (RI)\n1654\\(\\pm\\)2285\n511\\(\\pm\\)1324\n15804\\(\\pm\\)23194\n0.42\\(\\pm\\)0.29\n\n\n\n\\(AS_0\\) (RJ)\n1678\\(\\pm\\)2288\n535\\(\\pm\\)1351\n15956\\(\\pm\\)23243\n0.4\\(\\pm\\)0.3\n\n\n\n\\(AS_{p_{\\cap} = 0.55}\\) (RI)\n1541\\(\\pm\\)2191\n397\\(\\pm\\)1177\n14034\\(\\pm\\)22332\n0.6\\(\\pm\\)0.21\n\n\n\n\\(AS_{p_{\\cap} = 0.58}\\) (RJ)\n1622\\(\\pm\\)2237\n477\\(\\pm\\)1268\n15008\\(\\pm\\)22805\n0.5\\(\\pm\\)0.25\n\n\n\n\\(AS_{\\parallel 0} (RI)\\)\n1709\\(\\pm\\)2303\n566\\(\\pm\\)1397\n16242\\(\\pm\\)23351\n0.4\\(\\pm\\)0.21\n\n\n\n\\(AS_{\\parallel 0} (RJ)\\)\n1676\\(\\pm\\)2294\n533\\(\\pm\\)1358\n15954\\(\\pm\\)23245\n0.42\\(\\pm\\)0.22\n\n\n\n\\(AS_{\\parallel p_{\\cap} = 0.55} (RI)\\)\n1624\\(\\pm\\)2217\n478\\(\\pm\\)1218\n14754\\(\\pm\\)22660\n0.54\\(\\pm\\)0.2\n\n\n\n\\(AS_{\\parallel p_{\\cap} = 0.58} (RJ)\\)\n1708\\(\\pm\\)2269\n563\\(\\pm\\)1363\n15730\\(\\pm\\)23095\n0.43\\(\\pm\\)0.25\n\n\n\n\n\n\n\n\n\n\n\nViolin plot of the distribution of the number of selected solvers to run in parallel across all problem instances for each scenario for the respective optimal (p_{}) and the maximum level of parallelism (seven processors for MAXSAT19-UCMS and 10 for all other scenarios). The diamond denotes the mean value. The top-left plot refers to the RFJ model, the top-right plot to the RI model, and the bottom plot to the RJ model.\n\n\nWhen comparing models trained on sequential data with those trained on parallel data for portfolio selection, the model trained on sequential data consistently outperforms the model trained on parallel data. This is evident in Tables [tab:summary5-ipc-max], [tab:summary5-sat11-sat16] and [tab:summary5-sat18], which show that both the \\(AS_0\\) and \\(AS_{p_{\\cap}}\\) methods are superior to \\(AS_{\\parallel 0}\\) and \\(AS_{\\parallel p_{\\cap}}\\), respectively. This holds for both the RJ and RI models in all scenarios, except when comparing \\(AS_0\\) and \\(AS_{\\parallel 0}\\) for SAT16-MAIN and SAT18-EXP, where the RJ model performs slightly better. In Figure 1.2, for the RI model with a number of parallel runs limited to 10, the \\(AS_{\\cap}\\) method, using a model trained on sequential data, emerges as the best approach to portfolio selection.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parallel Portfolio Selection with Parallel Data Training</span>"
    ]
  },
  {
    "objectID": "chapters/chapter6/ParallelPortfolioSelectionwithParallelDataTraining.html#conclusions",
    "href": "chapters/chapter6/ParallelPortfolioSelectionwithParallelDataTraining.html#conclusions",
    "title": "7  Parallel Portfolio Selection with Parallel Data Training",
    "section": "7.5 Conclusions",
    "text": "7.5 Conclusions\nIn this chapter, we explore the impact of training algorithm selectors using parallel data versus sequential data for parallel portfolio selection. Using both sequential and parallel performance models, our objective was to understand whether incorporating data collected under parallel execution conditions would offer a significant advantage in portfolio selection tasks. Our initial analysis reveals that while predictions of algorithm selectors trained on parallel data differ significantly from those trained on sequential data, their overall rankings across models on the benchmarks are not significantly different.\nFurthermore, when comparing the effectiveness of portfolio selection, models trained on sequential data consistently outperform those trained on parallel data in most scenarios. Specifically, the \\(AS_{p_{\\cap}}\\) method trained in sequential data demonstrated superior performance compared to its counterpart trained in parallel data \\(AS_{\\parallel p_{\\cap}}\\)).\nThese findings underscore the robustness of sequentially trained models for portfolio selection, even in parallel execution environments. Importantly, the results suggest that collecting parallel data may not be necessary for effective portfolio selection. Sequential data models are just as effective, if not superior, for predicting solver performance and guiding efficient portfolio selection. Although parallel data offers insight into runtime overheads, sequential data models remain more effective in predicting solver performance and ranking which resulted in efficient portfolio selection.\n\n\n\n\n\n\nBischl, Bernd, Pascal Kerschke, Lars Kotthoff, Marius Lindauer, Yuri Malitsky, Alexandre Fréchette, Holger Hoos, et al. 2016. “ASlib: A Benchmark Library for Algorithm Selection.” Artificial Intelligence 237: 41–58.\n\n\nBischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. Jones. 2016. “mlr: Machine Learning in R.” Journal of Machine Learning Research 17 (170): 1–5. https://jmlr.org/papers/v17/15-066.html.\n\n\nFawcett, Chris, Mauro Vallati, Frank Hutter, Jörg Hoffmann, Holger Hoos, and Kevin Leyton-Brown. 2014. “Improved Features for Runtime Prediction of Domain-Independent Planners.” Proceedings of the International Conference on Automated Planning and Scheduling 24 (1): 355–59. https://doi.org/10.1609/icaps.v24i1.13680.\n\n\nGomes, Carla, and Bart Selman. 2001. “Algorithm Portfolios.” Artificial Intelligence 126: 43–62.\n\n\nHuberman, Bernardo A., Rajan M. Lukose, and Tad Hogg. 1997. “An economics approach to hard computational problems.” Science 275 (5296): 51–54. https://doi.org/10.1126/science.275.5296.51.\n\n\nKashgarani, Haniye, and Lars Kotthoff. 2021. “Is Algorithm Selection Worth It? Comparing Selecting Single Algorithms and Parallel Execution.” In AAAI Workshop on Meta-Learning and MetaDL Challenge, 140:58–64. Proceedings of Machine Learning Research. PMLR. https://proceedings.mlr.press/v140/kashgarani21a.html.\n\n\n———. 2023. “Automatic Parallel Portfolio Selection.” In ECAI 2023, 1215–22. IOS Press.\n\n\nKerschke, Pascal, Holger H. Hoos, Frank Neumann, and Heike Trautmann. 2019. “Automated Algorithm Selection: Survey and Perspectives.” Evolutionary Computation 27 (1): 3–45. https://doi.org/10.1162/evco_a_00242.\n\n\nKotthoff, Lars. 2014. “Algorithm selection for combinatorial search problems: A survey.” AI Magazine 35 (3): 48–69.\n\n\nWager, Stefan, Trevor Hastie, and Bradley Efron. 2014. “Confidence intervals for random forests: The jackknife and the infinitesimal jackknife.” The Journal of Machine Learning Research 15 (1): 1625–51.\n\n\nWright, Marvin N., and Andreas Ziegler. 2017. “ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R.” Journal of Statistical Software 77 (1): 1–17. https://doi.org/10.18637/jss.v077.i01.\n\n\nXu, Lin, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. 2008. “SATzilla: Portfolio-Based Algorithm Selection for SAT.” J. Artif. Int. Res. 32 (1): 565–606.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parallel Portfolio Selection with Parallel Data Training</span>"
    ]
  },
  {
    "objectID": "chapters/chapter7/DiscussionandConclusion.html",
    "href": "chapters/chapter7/DiscussionandConclusion.html",
    "title": "8  Discussion and Conclusion",
    "section": "",
    "text": "8.1 Limitations\nThis dissertation investigated the challenges and advancements in the fields of algorithm selection and parallel portfolio design methods. Through six interconnected chapters, we address critical gaps in existing approaches by offering novel contributions and insights. This research led to the development of innovative methods for solving combinatorial problems more effectively. The findings underscore the importance of bridging theoretical assumptions with practical implementations, particularly in the context of running solvers in parallel.\nOne of the core contributions of this work is the empirical evaluation of the performance of parallel portfolios. Unlike most previous studies, which rely on theoretical or simulated performance, this research measured the actual observed performance of parallel portfolios. The analysis revealed that running multiple algorithms in parallel often incurs significant performance penalties due to resource contention, even when only a small number of algorithms are executed on the same machine. In contrast, algorithm selection methods that choose a single solver based on instance-specific predictions demonstrated better overall performance, despite inaccuracies in the predictions. This analysis highlighted the weaknesses of relying solely on parallel approaches or single-algorithm selection methods, paving the way for creating hybrid solutions.\nTo address these limitations, a novel hybrid approach was introduced that combined algorithm selection with parallel execution. This method dynamically determines both the solvers and the number of solvers to run in parallel, based on predicted run-time distributions. Unlike traditional methods that use all available processors or a fixed number of solvers, the proposed approach intelligently adapts to different problem instances. It demonstrated substantial performance improvements on five different benchmarks and outperformed baseline methods and existing approaches in the literature. This method is simple and practical, making it highly useful because it relies on readily available information in algorithm selection frameworks. In addition, the tunable parameter \\(p_{\\cap}\\) enables the method to be customized for specific application domains, while its default generic value already provides strong performance.\nThis dissertation also introduced variations of the hybrid method that focused on improving the metrics for solver selection. A key contribution was the development of a parallel portfolio selection approach based on the KL divergence formula, which measures how much an algorithm’s prediction diverges from the best-predicted solver. This method offers a more reliable alternative to the original approach, as the optimal values of \\(kl\\) were more consistent between different scenarios. In contrast, the adjustment of the joint probability threshold \\(p_{\\cap}\\) in the prediction distribution showed greater variability. While the original method outperformed the KL divergence-based variation in most cases, the latter proved beneficial in scenarios where the complexity of tuning \\(p_{\\cap}\\) presented challenges.\nThe final chapter of this work analyzed the impact of training algorithm selectors using parallel data versus sequential data. Interestingly, models trained on sequential data consistently outperformed those trained on parallel data in most scenarios. While predictions from parallel data captured runtime overheads, sequential data models provided more accurate rankings and superior performance in portfolio selection tasks. This finding suggests that collecting parallel data may not be necessary for effective portfolio selection, as sequential data models remain robust and efficient even in parallel execution environments.\nWhile this dissertation makes significant contributions, it acknowledges certain limitations. The methods were evaluated on specific problem scenarios and benchmarks, which might limit their applicability to other domains. Furthermore, reliance on particular performance models, such as random forest-based regressors, highlights the potential to explore alternative models or ensemble techniques that could offer improved uncertainty estimates. The variability in tuning thresholds across benchmarks also underscores the need for more generalized or automated parameter tuning methods, which could streamline the configuration process and enhance efficiency.\nAlthough advances proposed in this work and the literature mark considerable progress in the world of combinatorial optimization, the challenge is to turn them into practical solutions that work for a wide range of real-world needs. As noted in (Amadini, Gabbrielli, and Mauro 2015), the use of portfolio methods and algorithm selection faces major challenges, especially because standardized datasets are often missing in some domains of problems. Without a common format, it becomes practically difficult to apply portfolio methods. Additionally, some combinatorial problems, such as hierarchical planning problems often represented in HDDL format, lack tools to extract the features necessary to train performance models. The available tool for extracting planning problem features is designed only for PDDL formats. Most of the research in this field has also been done in academic settings. Although this dissertation and similar studies show great progress in improving combinatorial problem solving, there is an urgent need to apply these methods to real-world areas such as bioinformatics, operations research, and software dependency resolution. Bringing these techniques into practical use will help tackle real-world problems and demonstrate their effectiveness in diverse scenarios.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discussion and Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/chapter7/DiscussionandConclusion.html#future-directions",
    "href": "chapters/chapter7/DiscussionandConclusion.html#future-directions",
    "title": "8  Discussion and Conclusion",
    "section": "8.2 Future Directions",
    "text": "8.2 Future Directions\nThis research opens up several promising directions for future work. One key area is improving dynamic resource allocation methods for algorithm selection by incorporating adaptive techniques to optimize how solvers are executed. Furthermore, exploring alternative machine learning models, particularly those with better capabilities for estimating uncertainty, could further improve the performance of the solver selection. The results in Chapter 4 indicate that the RJ model excelled as a single algorithm selector, while the RI model performed better for portfolio selection. This difference is likely because the infinitesimal jackknife method used in the RI model provides more accurate uncertainty estimates than the Jackknife method used in the RJ model. These findings emphasize the need to focus on robust methods for estimating uncertainty to enhance both algorithm selection and portfolio design.\nAnother clear direction for future research is to include parallel solvers. Although this work focuses on sequential solvers treated as black boxes and includes them in parallel portfolios, adding parallel solvers introduces both challenges and opportunities. One of the main challenges is figuring out how to allocate computational resources, such as deciding how many cores to assign to each parallel solver. Exploring strategies to effectively manage these resources could lead to better and more efficient portfolio designs.\nAnother promising avenue for future research is applying the proposed approach to algorithm configuration and hyperparameter tuning. Currently, algorithms are typically configured for sequential settings, but it is important to study how these configured algorithms behave when executed in parallel. For example, does tuning an algorithm in a sequential environment result in reduced performance when it is run in parallel? Investigating this question could help refine tuning processes to achieve the best possible performance in parallel settings. Additionally, exploring how algorithm configurations perform in parallel environments could provide insights into optimizing performance and resource usage.\nFinally, a key goal is to extend the proposed methods to real-world challenges in domains such as bioinformatics, operations research, and software dependency resolution to validate their effectiveness and inspire further improvements. Future efforts could also focus on creating feature extraction tools for unsupported domains to expand the applicability of algorithm selection and portfolio optimization.\nIn conclusion, this dissertation has contributed to the advancement of algorithm selection and parallel portfolio optimization, addressing important challenges and proposing practical solutions. Although much work remains to be done, the findings presented here provide a foundation for future research and practical applications. It is my hope that this work will inspire further advancements and help tackle combinatorial problems in meaningful ways.\n\n\n\n\n\n\nAmadini, Roberto, Maurizio Gabbrielli, and Jacopo Mauro. 2015. “Why CP Portfolio Solvers Are (under)Utilized? Issues and Challenges.” In Logic-Based Program Synthesis and Transformation, edited by Moreno Falaschi, 349–64. Cham: Springer International Publishing.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Discussion and Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/appendixA.html",
    "href": "chapters/appendices/appendixA.html",
    "title": "9  Appendix",
    "section": "",
    "text": "10 Supporting Topics",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "chapters/appendices/appendixA.html#sub-portfolio-selection-based-on-p_cap",
    "href": "chapters/appendices/appendixA.html#sub-portfolio-selection-based-on-p_cap",
    "title": "9  Appendix",
    "section": "10.1 Sub-Portfolio Selection Based on \\(p_{\\cap}\\)",
    "text": "10.1 Sub-Portfolio Selection Based on \\(p_{\\cap}\\)\nBased on the discussion in Chapter Four, the optimal values of \\(p_{\\cap}\\) for IPC2018, MAXSAT19-UCMS, SAT11-INDU, SAT18-EXP, and SAT16-MAIN are 0.59, 0.55, 0.63, 0.81, and 0.33, respectively for regression random-forest performance model. In the following, some examples of the prediction distribution curves are plotted per scenario. Figure 1.1 shows two instances from the IPC2018 scenario, nurikabe_p03.pddl (left) and caldera_p03.pddl (right). Since the optimal value of \\(p_{\\cap}\\) for this scenario is 0.59, for each instance we choose the solvers whose overlap area with the minimum predicted solver is greater than 0.59. Based on this method, for nurikabe_p03.pddl, blind, Delfi1, FDMS2, symbolic.bidirectional, and DecStar will be chosen to run in parallel, while blind is the VBS and also the best predicted solver. For the caldera_p03.pddl instance, two solvers are selected: Metis1 and Metis2. In this instance, the Virtual Best Solver (VBS) is also the blind solver, but the solver with the minimum predicted time is Metis1.\nThe same situation exists with the MAXSAT19-UCMS scenario, where the optimal \\(p_{\\cap}\\) is 0.55. Figure 1.2 shows that based on the optimal \\(p_{\\cap}\\) for the wolfram72_9.wcnf instance (left), the UWrMaxSat.1.0, maxino2018 and MaxHS solvers should be selected. For this instance, UWrMaxSat.1.0 is the best predicted solver, and MaxHS, which is included in the portfolio, is the actual best solver. For the 1bpi_2knt_gwcnft.wcnf instance (right), all solvers except MaxHS are selected in the portfolio, and the VBS (Virtual Best Solver) QMaxSAT2018 is included in the subportfolio. The same situation exists with the SAT scenarios; however, due to the large number of solvers in those scenarios, the plots are omitted here.\n\n\n\n\n\n\nSub-portfolio selection for two example instances from IPC2018 based on the optimal ( p_{} ) values\n\n\n\n\n\n\n\n\nSub-portfolio selection for two example instances from MAXSAT19-UCMS based on the optimal p_{} values",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Appendix</span>"
    ]
  }
]